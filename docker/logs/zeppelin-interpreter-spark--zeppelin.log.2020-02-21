 WARN [2020-02-19 07:00:54,295] ({main} NativeCodeLoader.java[<clinit>]:62) - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
 INFO [2020-02-19 07:00:54,614] ({main} RemoteInterpreterServer.java[main]:261) - URL:jar:file:/zeppelin/interpreter/spark/spark-interpreter-0.8.2.jar!/org/apache/zeppelin/interpreter/remote/RemoteInterpreterServer.class
 INFO [2020-02-19 07:00:54,677] ({main} RemoteInterpreterServer.java[<init>]:162) - Launching ThriftServer at 172.25.0.3:39425
 INFO [2020-02-19 07:00:54,683] ({main} RemoteInterpreterServer.java[<init>]:166) - Starting remote interpreter server on port 39425
 INFO [2020-02-19 07:00:54,686] ({Thread-3} RemoteInterpreterServer.java[run]:203) - Starting remote interpreter server on port 39425
 INFO [2020-02-19 07:00:55,700] ({Thread-4} RemoteInterpreterUtils.java[registerInterpreter]:165) - callbackHost: 172.25.0.3, callbackPort: 33541, callbackInfo: CallbackInfo(host:172.25.0.3, port:39425)
 INFO [2020-02-19 07:00:56,114] ({pool-1-thread-1} RemoteInterpreterServer.java[createInterpreter]:311) - Instantiate interpreter org.apache.zeppelin.spark.SparkInterpreter
 INFO [2020-02-19 07:00:56,120] ({pool-1-thread-1} RemoteInterpreterServer.java[createInterpreter]:311) - Instantiate interpreter org.apache.zeppelin.spark.SparkSqlInterpreter
 INFO [2020-02-19 07:00:56,124] ({pool-1-thread-1} RemoteInterpreterServer.java[createInterpreter]:311) - Instantiate interpreter org.apache.zeppelin.spark.DepInterpreter
 INFO [2020-02-19 07:00:56,130] ({pool-1-thread-1} RemoteInterpreterServer.java[createInterpreter]:311) - Instantiate interpreter org.apache.zeppelin.spark.PySparkInterpreter
 INFO [2020-02-19 07:00:56,138] ({pool-1-thread-1} RemoteInterpreterServer.java[createInterpreter]:311) - Instantiate interpreter org.apache.zeppelin.spark.IPySparkInterpreter
 INFO [2020-02-19 07:00:56,141] ({pool-1-thread-1} RemoteInterpreterServer.java[createInterpreter]:311) - Instantiate interpreter org.apache.zeppelin.spark.SparkRInterpreter
 WARN [2020-02-19 07:00:56,309] ({pool-1-thread-1} ZeppelinConfiguration.java[create]:117) - Failed to load configuration, proceeding with a default
 INFO [2020-02-19 07:00:56,336] ({pool-1-thread-1} ZeppelinConfiguration.java[create]:129) - Server Host: 0.0.0.0
 INFO [2020-02-19 07:00:56,337] ({pool-1-thread-1} ZeppelinConfiguration.java[create]:131) - Server Port: 8080
 INFO [2020-02-19 07:00:56,340] ({pool-1-thread-1} ZeppelinConfiguration.java[create]:135) - Context Path: /
 INFO [2020-02-19 07:00:56,345] ({pool-1-thread-1} ZeppelinConfiguration.java[create]:136) - Zeppelin Version: 0.8.2
 INFO [2020-02-19 07:00:56,346] ({pool-1-thread-1} SchedulerFactory.java[<init>]:59) - Scheduler Thread Pool Size: 100
 INFO [2020-02-19 07:00:56,352] ({pool-2-thread-2} SchedulerFactory.java[jobStarted]:114) - Job 20200219-065917_1853343063 started by scheduler interpreter_672731064
 INFO [2020-02-19 07:01:01,573] ({pool-2-thread-2} OldSparkInterpreter.java[createSparkSession]:311) - ------ Create new SparkSession spark://spark-master:7077 -------
 INFO [2020-02-19 07:01:02,040] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Running Spark version 2.4.0
 INFO [2020-02-19 07:01:02,088] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Submitted application: Zeppelin
 INFO [2020-02-19 07:01:02,189] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Changing view acls to: root
 INFO [2020-02-19 07:01:02,190] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Changing modify acls to: root
 INFO [2020-02-19 07:01:02,190] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Changing view acls groups to: 
 INFO [2020-02-19 07:01:02,191] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Changing modify acls groups to: 
 INFO [2020-02-19 07:01:02,192] ({pool-2-thread-2} Logging.scala[logInfo]:54) - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
 INFO [2020-02-19 07:01:02,586] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Successfully started service 'sparkDriver' on port 34177.
 INFO [2020-02-19 07:01:02,643] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Registering MapOutputTracker
 INFO [2020-02-19 07:01:02,691] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Registering BlockManagerMaster
 INFO [2020-02-19 07:01:02,700] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
 INFO [2020-02-19 07:01:02,702] ({pool-2-thread-2} Logging.scala[logInfo]:54) - BlockManagerMasterEndpoint up
 INFO [2020-02-19 07:01:02,727] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Created local directory at /tmp/blockmgr-f3e4ab62-37e9-463b-9626-77bcfd426659
 INFO [2020-02-19 07:01:02,764] ({pool-2-thread-2} Logging.scala[logInfo]:54) - MemoryStore started with capacity 366.3 MB
 INFO [2020-02-19 07:01:02,794] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Registering OutputCommitCoordinator
 INFO [2020-02-19 07:01:02,919] ({pool-2-thread-2} Log.java[initialized]:192) - Logging initialized @11449ms
 INFO [2020-02-19 07:01:03,082] ({pool-2-thread-2} Server.java[doStart]:351) - jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
 INFO [2020-02-19 07:01:03,107] ({pool-2-thread-2} Server.java[doStart]:419) - Started @11637ms
 INFO [2020-02-19 07:01:03,163] ({pool-2-thread-2} AbstractConnector.java[doStart]:278) - Started ServerConnector@96b35f4{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
 INFO [2020-02-19 07:01:03,164] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Successfully started service 'SparkUI' on port 4040.
 INFO [2020-02-19 07:01:03,212] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@52bda8e2{/jobs,null,AVAILABLE,@Spark}
 INFO [2020-02-19 07:01:03,214] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@45fffe9e{/jobs/json,null,AVAILABLE,@Spark}
 INFO [2020-02-19 07:01:03,215] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@34fd266c{/jobs/job,null,AVAILABLE,@Spark}
 INFO [2020-02-19 07:01:03,218] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@20035e17{/jobs/job/json,null,AVAILABLE,@Spark}
 INFO [2020-02-19 07:01:03,221] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@1ab24f87{/stages,null,AVAILABLE,@Spark}
 INFO [2020-02-19 07:01:03,226] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@4721c4fb{/stages/json,null,AVAILABLE,@Spark}
 INFO [2020-02-19 07:01:03,229] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@7d0009d6{/stages/stage,null,AVAILABLE,@Spark}
 INFO [2020-02-19 07:01:03,235] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@640a198{/stages/stage/json,null,AVAILABLE,@Spark}
 INFO [2020-02-19 07:01:03,237] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@7b73563f{/stages/pool,null,AVAILABLE,@Spark}
 INFO [2020-02-19 07:01:03,243] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@7429a7fd{/stages/pool/json,null,AVAILABLE,@Spark}
 INFO [2020-02-19 07:01:03,245] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@468060a4{/storage,null,AVAILABLE,@Spark}
 INFO [2020-02-19 07:01:03,247] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@45cf558d{/storage/json,null,AVAILABLE,@Spark}
 INFO [2020-02-19 07:01:03,249] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@24ec21a5{/storage/rdd,null,AVAILABLE,@Spark}
 INFO [2020-02-19 07:01:03,251] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@1d437c8a{/storage/rdd/json,null,AVAILABLE,@Spark}
 INFO [2020-02-19 07:01:03,252] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@5803aac6{/environment,null,AVAILABLE,@Spark}
 INFO [2020-02-19 07:01:03,254] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@67dfff76{/environment/json,null,AVAILABLE,@Spark}
 INFO [2020-02-19 07:01:03,256] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@227b0b21{/executors,null,AVAILABLE,@Spark}
 INFO [2020-02-19 07:01:03,258] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@245a94c8{/executors/json,null,AVAILABLE,@Spark}
 INFO [2020-02-19 07:01:03,261] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@614a5f1f{/executors/threadDump,null,AVAILABLE,@Spark}
 INFO [2020-02-19 07:01:03,263] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@7976380f{/executors/threadDump/json,null,AVAILABLE,@Spark}
 INFO [2020-02-19 07:01:03,271] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@32e1603{/static,null,AVAILABLE,@Spark}
 INFO [2020-02-19 07:01:03,272] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@18b06989{/,null,AVAILABLE,@Spark}
 INFO [2020-02-19 07:01:03,274] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@6f48162f{/api,null,AVAILABLE,@Spark}
 INFO [2020-02-19 07:01:03,276] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@19476c41{/jobs/job/kill,null,AVAILABLE,@Spark}
 INFO [2020-02-19 07:01:03,279] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@77ca62e4{/stages/stage/kill,null,AVAILABLE,@Spark}
 INFO [2020-02-19 07:01:03,284] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Bound SparkUI to 0.0.0.0, and started at http://zeppelin:4040
 INFO [2020-02-19 07:01:03,313] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Added JAR file:/zeppelin/interpreter/spark/spark-interpreter-0.8.2.jar at spark://zeppelin:34177/jars/spark-interpreter-0.8.2.jar with timestamp 1582095663312
 WARN [2020-02-19 07:01:03,396] ({pool-2-thread-2} Logging.scala[logWarning]:66) - Fair Scheduler configuration file not found so jobs will be scheduled in FIFO order. To use fair scheduling, configure pools in fairscheduler.xml or set spark.scheduler.allocation.file to a file that contains the configuration.
 INFO [2020-02-19 07:01:03,406] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Created default pool: default, schedulingMode: FIFO, minShare: 0, weight: 1
 INFO [2020-02-19 07:01:03,472] ({appclient-register-master-threadpool-0} Logging.scala[logInfo]:54) - Connecting to master spark://spark-master:7077...
 INFO [2020-02-19 07:01:03,880] ({netty-rpc-connection-0} TransportClientFactory.java[createClient]:267) - Successfully created connection to spark-master/172.25.0.5:7077 after 33 ms (0 ms spent in bootstraps)
 INFO [2020-02-19 07:01:04,030] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Connected to Spark cluster with app ID app-20200219070104-0000
 INFO [2020-02-19 07:01:04,050] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45809.
 INFO [2020-02-19 07:01:04,056] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Server created on zeppelin:45809
 INFO [2020-02-19 07:01:04,062] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
 INFO [2020-02-19 07:01:04,136] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Executor added: app-20200219070104-0000/0 on worker-20200219065748-172.25.0.7-39297 (172.25.0.7:39297) with 4 core(s)
 INFO [2020-02-19 07:01:04,140] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Granted executor ID app-20200219070104-0000/0 on hostPort 172.25.0.7:39297 with 4 core(s), 2.0 GB RAM
 INFO [2020-02-19 07:01:04,142] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Executor added: app-20200219070104-0000/1 on worker-20200219065748-172.25.0.6-38847 (172.25.0.6:38847) with 4 core(s)
 INFO [2020-02-19 07:01:04,148] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Granted executor ID app-20200219070104-0000/1 on hostPort 172.25.0.6:38847 with 4 core(s), 2.0 GB RAM
 INFO [2020-02-19 07:01:04,156] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Registering BlockManager BlockManagerId(driver, zeppelin, 45809, None)
 INFO [2020-02-19 07:01:04,185] ({dispatcher-event-loop-3} Logging.scala[logInfo]:54) - Registering block manager zeppelin:45809 with 366.3 MB RAM, BlockManagerId(driver, zeppelin, 45809, None)
 INFO [2020-02-19 07:01:04,194] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Registered BlockManager BlockManagerId(driver, zeppelin, 45809, None)
 INFO [2020-02-19 07:01:04,201] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Initialized BlockManager: BlockManagerId(driver, zeppelin, 45809, None)
 INFO [2020-02-19 07:01:04,344] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Executor updated: app-20200219070104-0000/1 is now RUNNING
 INFO [2020-02-19 07:01:04,353] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Executor updated: app-20200219070104-0000/0 is now RUNNING
 INFO [2020-02-19 07:01:04,750] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@49b613a0{/metrics/json,null,AVAILABLE,@Spark}
 INFO [2020-02-19 07:01:04,834] ({pool-2-thread-2} Logging.scala[logInfo]:54) - SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
 INFO [2020-02-19 07:01:04,899] ({pool-2-thread-2} OldSparkInterpreter.java[createSparkSession]:347) - Created Spark session with Hive support
 INFO [2020-02-19 07:01:11,661] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.25.0.7:35064) with ID 0
 INFO [2020-02-19 07:01:11,733] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.25.0.6:47726) with ID 1
 INFO [2020-02-19 07:01:13,100] ({dispatcher-event-loop-3} Logging.scala[logInfo]:54) - Registering block manager 172.25.0.7:44975 with 912.3 MB RAM, BlockManagerId(0, 172.25.0.7, 44975, None)
 INFO [2020-02-19 07:01:13,126] ({dispatcher-event-loop-3} Logging.scala[logInfo]:54) - Registering block manager 172.25.0.6:45543 with 912.3 MB RAM, BlockManagerId(1, 172.25.0.6, 45543, None)
 INFO [2020-02-19 07:01:18,897] ({pool-2-thread-2} SparkShims.java[loadShims]:62) - Initializing shims for Spark 2.x
 INFO [2020-02-19 07:01:18,907] ({pool-2-thread-2} OldSparkInterpreter.java[populateSparkWebUrl]:931) - Sending metadata to Zeppelin server: {message=Spark UI enabled, url=http://zeppelin:4040}
 INFO [2020-02-19 07:01:20,507] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Block broadcast_0 stored as values in memory (estimated size 236.7 KB, free 366.1 MB)
 INFO [2020-02-19 07:01:20,614] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Block broadcast_0_piece0 stored as bytes in memory (estimated size 22.9 KB, free 366.0 MB)
 INFO [2020-02-19 07:01:20,619] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Added broadcast_0_piece0 in memory on zeppelin:45809 (size: 22.9 KB, free: 366.3 MB)
 INFO [2020-02-19 07:01:20,636] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Created broadcast 0 from textFile at <console>:25
 INFO [2020-02-19 07:01:20,730] ({pool-2-thread-2} SchedulerFactory.java[jobFinished]:120) - Job 20200219-065917_1853343063 finished by scheduler interpreter_672731064
 INFO [2020-02-19 07:02:07,110] ({pool-2-thread-2} SchedulerFactory.java[jobStarted]:114) - Job 20200219-065917_1853343063 started by scheduler interpreter_672731064
 INFO [2020-02-19 07:02:07,121] ({pool-2-thread-2} OldSparkInterpreter.java[populateSparkWebUrl]:931) - Sending metadata to Zeppelin server: {message=Spark UI enabled, url=http://zeppelin:4040}
 INFO [2020-02-19 07:02:07,474] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Block broadcast_1 stored as values in memory (estimated size 236.7 KB, free 365.8 MB)
 INFO [2020-02-19 07:02:07,490] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Block broadcast_1_piece0 stored as bytes in memory (estimated size 22.9 KB, free 365.8 MB)
 INFO [2020-02-19 07:02:07,492] ({dispatcher-event-loop-2} Logging.scala[logInfo]:54) - Added broadcast_1_piece0 in memory on zeppelin:45809 (size: 22.9 KB, free: 366.3 MB)
 INFO [2020-02-19 07:02:07,494] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Created broadcast 1 from textFile at <console>:25
 INFO [2020-02-19 07:02:07,862] ({pool-2-thread-2} FileInputFormat.java[listStatus]:249) - Total input paths to process : 1
 INFO [2020-02-19 07:02:07,924] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Starting job: first at <console>:26
 INFO [2020-02-19 07:02:07,947] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Got job 0 (first at <console>:26) with 1 output partitions
 INFO [2020-02-19 07:02:07,949] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Final stage: ResultStage 0 (first at <console>:26)
 INFO [2020-02-19 07:02:07,951] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Parents of final stage: List()
 INFO [2020-02-19 07:02:07,954] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Missing parents: List()
 INFO [2020-02-19 07:02:07,963] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting ResultStage 0 (/data/test.txt MapPartitionsRDD[3] at textFile at <console>:25), which has no missing parents
 INFO [2020-02-19 07:02:07,995] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_2 stored as values in memory (estimated size 3.4 KB, free 365.8 MB)
 INFO [2020-02-19 07:02:08,000] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.0 KB, free 365.8 MB)
 INFO [2020-02-19 07:02:08,004] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Added broadcast_2_piece0 in memory on zeppelin:45809 (size: 2.0 KB, free: 366.3 MB)
 INFO [2020-02-19 07:02:08,008] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Created broadcast 2 from broadcast at DAGScheduler.scala:1161
 INFO [2020-02-19 07:02:08,045] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting 1 missing tasks from ResultStage 0 (/data/test.txt MapPartitionsRDD[3] at textFile at <console>:25) (first 15 tasks are for partitions Vector(0))
 INFO [2020-02-19 07:02:08,048] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Adding task set 0.0 with 1 tasks
 INFO [2020-02-19 07:02:08,074] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Added task set TaskSet_0.0 tasks to pool default
 INFO [2020-02-19 07:02:08,098] ({dispatcher-event-loop-2} Logging.scala[logInfo]:54) - Starting task 0.0 in stage 0.0 (TID 0, 172.25.0.6, executor 1, partition 0, PROCESS_LOCAL, 7883 bytes)
 INFO [2020-02-19 07:02:08,777] ({dispatcher-event-loop-2} Logging.scala[logInfo]:54) - Added broadcast_2_piece0 in memory on 172.25.0.6:45543 (size: 2.0 KB, free: 912.3 MB)
 INFO [2020-02-19 07:02:09,125] ({dispatcher-event-loop-3} Logging.scala[logInfo]:54) - Added broadcast_1_piece0 in memory on 172.25.0.6:45543 (size: 22.9 KB, free: 912.3 MB)
 WARN [2020-02-19 07:02:09,625] ({task-result-getter-0} Logging.scala[logWarning]:66) - Lost task 0.0 in stage 0.0 (TID 0, 172.25.0.6, executor 1): java.io.FileNotFoundException: File file:/data/test.txt does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:611)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:824)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:601)
	at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:421)
	at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:142)
	at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:346)
	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:769)
	at org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:109)
	at org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.liftedTree1$1(HadoopRDD.scala:267)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:266)
	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:224)
	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:95)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

 INFO [2020-02-19 07:02:09,630] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Starting task 0.1 in stage 0.0 (TID 1, 172.25.0.7, executor 0, partition 0, PROCESS_LOCAL, 7883 bytes)
 INFO [2020-02-19 07:02:10,337] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Added broadcast_2_piece0 in memory on 172.25.0.7:44975 (size: 2.0 KB, free: 912.3 MB)
 INFO [2020-02-19 07:02:10,471] ({dispatcher-event-loop-2} Logging.scala[logInfo]:54) - Added broadcast_1_piece0 in memory on 172.25.0.7:44975 (size: 22.9 KB, free: 912.3 MB)
 INFO [2020-02-19 07:02:10,859] ({task-result-getter-1} Logging.scala[logInfo]:54) - Lost task 0.1 in stage 0.0 (TID 1) on 172.25.0.7, executor 0: java.io.FileNotFoundException (File file:/data/test.txt does not exist) [duplicate 1]
 INFO [2020-02-19 07:02:10,861] ({dispatcher-event-loop-3} Logging.scala[logInfo]:54) - Starting task 0.2 in stage 0.0 (TID 2, 172.25.0.6, executor 1, partition 0, PROCESS_LOCAL, 7883 bytes)
 INFO [2020-02-19 07:02:10,923] ({task-result-getter-2} Logging.scala[logInfo]:54) - Lost task 0.2 in stage 0.0 (TID 2) on 172.25.0.6, executor 1: java.io.FileNotFoundException (File file:/data/test.txt does not exist) [duplicate 2]
 INFO [2020-02-19 07:02:10,929] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Starting task 0.3 in stage 0.0 (TID 3, 172.25.0.7, executor 0, partition 0, PROCESS_LOCAL, 7883 bytes)
 INFO [2020-02-19 07:02:10,987] ({task-result-getter-3} Logging.scala[logInfo]:54) - Lost task 0.3 in stage 0.0 (TID 3) on 172.25.0.7, executor 0: java.io.FileNotFoundException (File file:/data/test.txt does not exist) [duplicate 3]
ERROR [2020-02-19 07:02:10,990] ({task-result-getter-3} Logging.scala[logError]:70) - Task 0 in stage 0.0 failed 4 times; aborting job
 INFO [2020-02-19 07:02:10,994] ({task-result-getter-3} Logging.scala[logInfo]:54) - Removed TaskSet 0.0, whose tasks have all completed, from pool default
 INFO [2020-02-19 07:02:11,003] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Cancelling stage 0
 INFO [2020-02-19 07:02:11,004] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Killing all running tasks in stage 0: Stage cancelled
 INFO [2020-02-19 07:02:11,008] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - ResultStage 0 (first at <console>:26) failed in 3.015 s due to Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 3, 172.25.0.7, executor 0): java.io.FileNotFoundException: File file:/data/test.txt does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:611)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:824)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:601)
	at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:421)
	at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:142)
	at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:346)
	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:769)
	at org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:109)
	at org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.liftedTree1$1(HadoopRDD.scala:267)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:266)
	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:224)
	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:95)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
 INFO [2020-02-19 07:02:11,013] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Job 0 failed: first at <console>:26, took 3.087946 s
 INFO [2020-02-19 07:02:11,226] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 13
 INFO [2020-02-19 07:02:11,229] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 19
 INFO [2020-02-19 07:02:11,229] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 2
 INFO [2020-02-19 07:02:11,230] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 10
 INFO [2020-02-19 07:02:11,231] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 23
 INFO [2020-02-19 07:02:11,232] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 8
 INFO [2020-02-19 07:02:11,233] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 14
 INFO [2020-02-19 07:02:11,236] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 5
 INFO [2020-02-19 07:02:11,237] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 21
 INFO [2020-02-19 07:02:11,238] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 17
 INFO [2020-02-19 07:02:11,242] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 20
 INFO [2020-02-19 07:02:11,244] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 4
 INFO [2020-02-19 07:02:11,245] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 6
 INFO [2020-02-19 07:02:11,246] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 9
 INFO [2020-02-19 07:02:11,248] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 22
 INFO [2020-02-19 07:02:11,250] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 3
 INFO [2020-02-19 07:02:11,254] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 15
 INFO [2020-02-19 07:02:11,256] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 24
 INFO [2020-02-19 07:02:11,257] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 7
 INFO [2020-02-19 07:02:11,257] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 1
 INFO [2020-02-19 07:02:11,258] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 0
 INFO [2020-02-19 07:02:11,259] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 11
 INFO [2020-02-19 07:02:11,259] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 18
 INFO [2020-02-19 07:02:11,328] ({dispatcher-event-loop-3} Logging.scala[logInfo]:54) - Removed broadcast_2_piece0 on zeppelin:45809 in memory (size: 2.0 KB, free: 366.3 MB)
 INFO [2020-02-19 07:02:11,330] ({dispatcher-event-loop-3} Logging.scala[logInfo]:54) - Removed broadcast_2_piece0 on 172.25.0.7:44975 in memory (size: 2.0 KB, free: 912.3 MB)
 INFO [2020-02-19 07:02:11,338] ({dispatcher-event-loop-3} Logging.scala[logInfo]:54) - Removed broadcast_2_piece0 on 172.25.0.6:45543 in memory (size: 2.0 KB, free: 912.3 MB)
 INFO [2020-02-19 07:02:11,408] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 12
 INFO [2020-02-19 07:02:11,409] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 16
 INFO [2020-02-19 07:02:11,447] ({pool-2-thread-2} SchedulerFactory.java[jobFinished]:120) - Job 20200219-065917_1853343063 finished by scheduler interpreter_672731064
 INFO [2020-02-19 07:12:32,644] ({dispatcher-event-loop-2} Logging.scala[logInfo]:54) - Executor updated: app-20200219070104-0000/0 is now LOST (worker lost)
ERROR [2020-02-19 07:12:32,685] ({dispatcher-event-loop-3} Logging.scala[logError]:70) - Lost executor 0 on 172.25.0.7: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
 INFO [2020-02-19 07:12:32,686] ({dispatcher-event-loop-2} Logging.scala[logInfo]:54) - Executor app-20200219070104-0000/0 removed: worker lost
 INFO [2020-02-19 07:12:32,699] ({dispatcher-event-loop-2} Logging.scala[logInfo]:54) - Master removed worker worker-20200219065748-172.25.0.7-39297: 172.25.0.7:39297 got disassociated
 INFO [2020-02-19 07:12:32,704] ({dispatcher-event-loop-2} Logging.scala[logInfo]:54) - Worker worker-20200219065748-172.25.0.7-39297 removed: 172.25.0.7:39297 got disassociated
 INFO [2020-02-19 07:12:32,707] ({dispatcher-event-loop-2} Logging.scala[logInfo]:54) - Executor updated: app-20200219070104-0000/1 is now LOST (worker lost)
 INFO [2020-02-19 07:12:32,709] ({dispatcher-event-loop-2} Logging.scala[logInfo]:54) - Executor app-20200219070104-0000/1 removed: worker lost
 INFO [2020-02-19 07:12:32,713] ({dispatcher-event-loop-2} Logging.scala[logInfo]:54) - Master removed worker worker-20200219065748-172.25.0.6-38847: 172.25.0.6:38847 got disassociated
 INFO [2020-02-19 07:12:32,716] ({dispatcher-event-loop-2} Logging.scala[logInfo]:54) - Worker worker-20200219065748-172.25.0.6-38847 removed: 172.25.0.6:38847 got disassociated
 INFO [2020-02-19 07:12:32,719] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Executor lost: 0 (epoch 0)
 INFO [2020-02-19 07:12:32,722] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Trying to remove executor 0 from BlockManagerMaster.
 INFO [2020-02-19 07:12:32,726] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Removing block manager BlockManagerId(0, 172.25.0.7, 44975, None)
 INFO [2020-02-19 07:12:32,724] ({dispatcher-event-loop-3} Logging.scala[logInfo]:54) - Removal of executor 0 requested
 INFO [2020-02-19 07:12:32,728] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Removed 0 successfully in removeExecutor
 INFO [2020-02-19 07:12:32,727] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Trying to remove executor 0 from BlockManagerMaster.
 INFO [2020-02-19 07:12:32,730] ({dispatcher-event-loop-3} Logging.scala[logInfo]:54) - Asked to remove non-existent executor 0
 INFO [2020-02-19 07:12:32,732] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Shuffle files lost for executor: 0 (epoch 0)
 INFO [2020-02-19 07:12:32,736] ({dispatcher-event-loop-3} Logging.scala[logInfo]:54) - Handle removed worker worker-20200219065748-172.25.0.7-39297: 172.25.0.7:39297 got disassociated
 INFO [2020-02-19 07:12:32,738] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Shuffle files lost for worker worker-20200219065748-172.25.0.7-39297 on host 172.25.0.7
ERROR [2020-02-19 07:12:32,739] ({dispatcher-event-loop-3} Logging.scala[logError]:70) - Lost executor 1 on 172.25.0.6: worker lost
 INFO [2020-02-19 07:12:32,741] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Executor lost: 1 (epoch 2)
 INFO [2020-02-19 07:12:32,741] ({dispatcher-event-loop-3} Logging.scala[logInfo]:54) - Handle removed worker worker-20200219065748-172.25.0.6-38847: 172.25.0.6:38847 got disassociated
 INFO [2020-02-19 07:12:32,741] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Trying to remove executor 1 from BlockManagerMaster.
 INFO [2020-02-19 07:12:32,743] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Removing block manager BlockManagerId(1, 172.25.0.6, 45543, None)
 INFO [2020-02-19 07:12:32,744] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Removed 1 successfully in removeExecutor
 INFO [2020-02-19 07:12:32,745] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Shuffle files lost for executor: 1 (epoch 2)
 INFO [2020-02-19 07:12:32,746] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Shuffle files lost for worker worker-20200219065748-172.25.0.6-38847 on host 172.25.0.6
 WARN [2020-02-19 07:12:43,544] ({dispatcher-event-loop-1} Logging.scala[logWarning]:66) - Connection to spark-master:7077 failed; waiting for master to reconnect...
 WARN [2020-02-19 07:12:43,546] ({dispatcher-event-loop-1} Logging.scala[logWarning]:66) - Disconnected from Spark cluster! Waiting for reconnection...
 WARN [2020-02-19 07:12:43,547] ({dispatcher-event-loop-1} Logging.scala[logWarning]:66) - Connection to spark-master:7077 failed; waiting for master to reconnect...
 INFO [2020-02-19 07:12:50,336] ({pool-1-thread-2} OldSparkInterpreter.java[close]:1243) - Close interpreter
 INFO [2020-02-19 07:12:50,348] ({pool-1-thread-2} AbstractConnector.java[doStop]:318) - Stopped Spark@96b35f4{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
 INFO [2020-02-19 07:12:50,352] ({pool-1-thread-2} Logging.scala[logInfo]:54) - Stopped Spark web UI at http://zeppelin:4040
 INFO [2020-02-19 07:12:50,358] ({pool-1-thread-2} Logging.scala[logInfo]:54) - Shutting down all executors
 INFO [2020-02-19 07:12:50,359] ({dispatcher-event-loop-3} Logging.scala[logInfo]:54) - Asking each executor to shut down
 INFO [2020-02-19 07:12:50,375] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - MapOutputTrackerMasterEndpoint stopped!
 INFO [2020-02-19 07:12:50,413] ({pool-1-thread-2} Logging.scala[logInfo]:54) - MemoryStore cleared
 INFO [2020-02-19 07:12:50,415] ({pool-1-thread-2} Logging.scala[logInfo]:54) - BlockManager stopped
 INFO [2020-02-19 07:12:50,417] ({pool-1-thread-2} Logging.scala[logInfo]:54) - BlockManagerMaster stopped
 INFO [2020-02-19 07:12:50,422] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - OutputCommitCoordinator stopped!
 INFO [2020-02-19 07:12:50,431] ({pool-1-thread-2} Logging.scala[logInfo]:54) - Successfully stopped SparkContext
 INFO [2020-02-19 07:12:50,451] ({pool-1-thread-2} RemoteInterpreterServer.java[shutdown]:209) - Shutting down...
 INFO [2020-02-19 07:12:52,622] ({Thread-1} Logging.scala[logInfo]:54) - Shutdown hook called
 INFO [2020-02-19 07:12:52,626] ({Thread-1} Logging.scala[logInfo]:54) - Deleting directory /tmp/spark-283eaf63-0fa5-463f-b7ae-167fe23dac26
 INFO [2020-02-19 07:12:52,642] ({Thread-1} Logging.scala[logInfo]:54) - Deleting directory /tmp/spark-541e7a31-fe73-4f0d-8811-c456f1b6701b
 INFO [2020-02-19 07:12:52,648] ({Thread-1} Logging.scala[logInfo]:54) - Deleting directory /tmp/spark-0188349c-15f0-4c69-88c8-456112df3fa7
 WARN [2020-02-19 07:17:20,494] ({main} NativeCodeLoader.java[<clinit>]:62) - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
 INFO [2020-02-19 07:17:20,749] ({main} RemoteInterpreterServer.java[main]:261) - URL:jar:file:/zeppelin/interpreter/spark/spark-interpreter-0.8.2.jar!/org/apache/zeppelin/interpreter/remote/RemoteInterpreterServer.class
 INFO [2020-02-19 07:17:20,778] ({main} RemoteInterpreterServer.java[<init>]:162) - Launching ThriftServer at 172.26.0.2:36579
 INFO [2020-02-19 07:17:20,782] ({main} RemoteInterpreterServer.java[<init>]:166) - Starting remote interpreter server on port 36579
 INFO [2020-02-19 07:17:20,784] ({Thread-3} RemoteInterpreterServer.java[run]:203) - Starting remote interpreter server on port 36579
 INFO [2020-02-19 07:17:21,810] ({Thread-4} RemoteInterpreterUtils.java[registerInterpreter]:165) - callbackHost: 172.26.0.2, callbackPort: 35037, callbackInfo: CallbackInfo(host:172.26.0.2, port:36579)
 INFO [2020-02-19 07:17:21,998] ({pool-1-thread-1} RemoteInterpreterServer.java[createInterpreter]:311) - Instantiate interpreter org.apache.zeppelin.spark.SparkInterpreter
 INFO [2020-02-19 07:17:22,003] ({pool-1-thread-1} RemoteInterpreterServer.java[createInterpreter]:311) - Instantiate interpreter org.apache.zeppelin.spark.SparkSqlInterpreter
 INFO [2020-02-19 07:17:22,007] ({pool-1-thread-1} RemoteInterpreterServer.java[createInterpreter]:311) - Instantiate interpreter org.apache.zeppelin.spark.DepInterpreter
 INFO [2020-02-19 07:17:22,012] ({pool-1-thread-1} RemoteInterpreterServer.java[createInterpreter]:311) - Instantiate interpreter org.apache.zeppelin.spark.PySparkInterpreter
 INFO [2020-02-19 07:17:22,018] ({pool-1-thread-1} RemoteInterpreterServer.java[createInterpreter]:311) - Instantiate interpreter org.apache.zeppelin.spark.IPySparkInterpreter
 INFO [2020-02-19 07:17:22,021] ({pool-1-thread-1} RemoteInterpreterServer.java[createInterpreter]:311) - Instantiate interpreter org.apache.zeppelin.spark.SparkRInterpreter
 WARN [2020-02-19 07:17:22,140] ({pool-1-thread-1} ZeppelinConfiguration.java[create]:117) - Failed to load configuration, proceeding with a default
 INFO [2020-02-19 07:17:22,165] ({pool-1-thread-1} ZeppelinConfiguration.java[create]:129) - Server Host: 0.0.0.0
 INFO [2020-02-19 07:17:22,166] ({pool-1-thread-1} ZeppelinConfiguration.java[create]:131) - Server Port: 8080
 INFO [2020-02-19 07:17:22,167] ({pool-1-thread-1} ZeppelinConfiguration.java[create]:135) - Context Path: /
 INFO [2020-02-19 07:17:22,172] ({pool-1-thread-1} ZeppelinConfiguration.java[create]:136) - Zeppelin Version: 0.8.2
 INFO [2020-02-19 07:17:22,173] ({pool-1-thread-1} SchedulerFactory.java[<init>]:59) - Scheduler Thread Pool Size: 100
 INFO [2020-02-19 07:17:22,179] ({pool-2-thread-2} SchedulerFactory.java[jobStarted]:114) - Job 20200219-065917_1853343063 started by scheduler interpreter_1181365328
 INFO [2020-02-19 07:17:26,161] ({pool-2-thread-2} OldSparkInterpreter.java[createSparkSession]:311) - ------ Create new SparkSession spark://spark-master:7077 -------
 INFO [2020-02-19 07:17:26,460] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Running Spark version 2.4.0
 INFO [2020-02-19 07:17:26,494] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Submitted application: Zeppelin
 INFO [2020-02-19 07:17:26,570] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Changing view acls to: root
 INFO [2020-02-19 07:17:26,571] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Changing modify acls to: root
 INFO [2020-02-19 07:17:26,572] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Changing view acls groups to: 
 INFO [2020-02-19 07:17:26,573] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Changing modify acls groups to: 
 INFO [2020-02-19 07:17:26,575] ({pool-2-thread-2} Logging.scala[logInfo]:54) - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
 INFO [2020-02-19 07:17:26,894] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Successfully started service 'sparkDriver' on port 38019.
 INFO [2020-02-19 07:17:26,925] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Registering MapOutputTracker
 INFO [2020-02-19 07:17:26,949] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Registering BlockManagerMaster
 INFO [2020-02-19 07:17:26,956] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
 INFO [2020-02-19 07:17:26,958] ({pool-2-thread-2} Logging.scala[logInfo]:54) - BlockManagerMasterEndpoint up
 INFO [2020-02-19 07:17:26,968] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Created local directory at /tmp/blockmgr-ac606060-579e-4196-917a-450aa5505949
 INFO [2020-02-19 07:17:26,984] ({pool-2-thread-2} Logging.scala[logInfo]:54) - MemoryStore started with capacity 366.3 MB
 INFO [2020-02-19 07:17:27,001] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Registering OutputCommitCoordinator
 INFO [2020-02-19 07:17:27,100] ({pool-2-thread-2} Log.java[initialized]:192) - Logging initialized @8041ms
 INFO [2020-02-19 07:17:27,192] ({pool-2-thread-2} Server.java[doStart]:351) - jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
 INFO [2020-02-19 07:17:27,537] ({pool-2-thread-2} Server.java[doStart]:419) - Started @8479ms
 INFO [2020-02-19 07:17:27,576] ({pool-2-thread-2} AbstractConnector.java[doStart]:278) - Started ServerConnector@76d5a8de{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
 INFO [2020-02-19 07:17:27,577] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Successfully started service 'SparkUI' on port 4040.
 INFO [2020-02-19 07:17:27,614] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@7d9560a9{/jobs,null,AVAILABLE,@Spark}
 INFO [2020-02-19 07:17:27,616] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@4c5c702b{/jobs/json,null,AVAILABLE,@Spark}
 INFO [2020-02-19 07:17:27,618] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@158ba31{/jobs/job,null,AVAILABLE,@Spark}
 INFO [2020-02-19 07:17:27,620] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@374420d4{/jobs/job/json,null,AVAILABLE,@Spark}
 INFO [2020-02-19 07:17:27,624] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@18b6c33f{/stages,null,AVAILABLE,@Spark}
 INFO [2020-02-19 07:17:27,627] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@5c7b6b2a{/stages/json,null,AVAILABLE,@Spark}
 INFO [2020-02-19 07:17:27,629] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@3bee36b0{/stages/stage,null,AVAILABLE,@Spark}
 INFO [2020-02-19 07:17:27,631] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@49e7b5c{/stages/stage/json,null,AVAILABLE,@Spark}
 INFO [2020-02-19 07:17:27,633] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@4e8f86c9{/stages/pool,null,AVAILABLE,@Spark}
 INFO [2020-02-19 07:17:27,636] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@345302ba{/stages/pool/json,null,AVAILABLE,@Spark}
 INFO [2020-02-19 07:17:27,641] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@6c219ca8{/storage,null,AVAILABLE,@Spark}
 INFO [2020-02-19 07:17:27,645] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@1b125eeb{/storage/json,null,AVAILABLE,@Spark}
 INFO [2020-02-19 07:17:27,646] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@29193a8e{/storage/rdd,null,AVAILABLE,@Spark}
 INFO [2020-02-19 07:17:27,648] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@573abc5{/storage/rdd/json,null,AVAILABLE,@Spark}
 INFO [2020-02-19 07:17:27,650] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@6557b31f{/environment,null,AVAILABLE,@Spark}
 INFO [2020-02-19 07:17:27,652] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@6cbb4c58{/environment/json,null,AVAILABLE,@Spark}
 INFO [2020-02-19 07:17:27,654] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@c96d60b{/executors,null,AVAILABLE,@Spark}
 INFO [2020-02-19 07:17:27,656] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@142379df{/executors/json,null,AVAILABLE,@Spark}
 INFO [2020-02-19 07:17:27,659] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@4cb4ef0f{/executors/threadDump,null,AVAILABLE,@Spark}
 INFO [2020-02-19 07:17:27,661] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@7a5ba34d{/executors/threadDump/json,null,AVAILABLE,@Spark}
 INFO [2020-02-19 07:17:27,672] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@40c71483{/static,null,AVAILABLE,@Spark}
 INFO [2020-02-19 07:17:27,674] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@34f78bd6{/,null,AVAILABLE,@Spark}
 INFO [2020-02-19 07:17:27,677] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@162549e0{/api,null,AVAILABLE,@Spark}
 INFO [2020-02-19 07:17:27,680] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@76aa7564{/jobs/job/kill,null,AVAILABLE,@Spark}
 INFO [2020-02-19 07:17:27,681] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@5aa321b2{/stages/stage/kill,null,AVAILABLE,@Spark}
 INFO [2020-02-19 07:17:27,685] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Bound SparkUI to 0.0.0.0, and started at http://zeppelin:4040
 INFO [2020-02-19 07:17:27,712] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Added JAR file:/zeppelin/interpreter/spark/spark-interpreter-0.8.2.jar at spark://zeppelin:38019/jars/spark-interpreter-0.8.2.jar with timestamp 1582096647710
 WARN [2020-02-19 07:17:27,771] ({pool-2-thread-2} Logging.scala[logWarning]:66) - Fair Scheduler configuration file not found so jobs will be scheduled in FIFO order. To use fair scheduling, configure pools in fairscheduler.xml or set spark.scheduler.allocation.file to a file that contains the configuration.
 INFO [2020-02-19 07:17:27,779] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Created default pool: default, schedulingMode: FIFO, minShare: 0, weight: 1
 INFO [2020-02-19 07:17:27,833] ({appclient-register-master-threadpool-0} Logging.scala[logInfo]:54) - Connecting to master spark://spark-master:7077...
 INFO [2020-02-19 07:17:27,894] ({netty-rpc-connection-0} TransportClientFactory.java[createClient]:267) - Successfully created connection to spark-master/172.26.0.5:7077 after 39 ms (0 ms spent in bootstraps)
 INFO [2020-02-19 07:17:28,030] ({dispatcher-event-loop-2} Logging.scala[logInfo]:54) - Connected to Spark cluster with app ID app-20200219071728-0000
 INFO [2020-02-19 07:17:28,040] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43939.
 INFO [2020-02-19 07:17:28,042] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Server created on zeppelin:43939
 INFO [2020-02-19 07:17:28,048] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
 INFO [2020-02-19 07:17:28,071] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Executor added: app-20200219071728-0000/0 on worker-20200219071324-172.26.0.6-34283 (172.26.0.6:34283) with 4 core(s)
 INFO [2020-02-19 07:17:28,086] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Granted executor ID app-20200219071728-0000/0 on hostPort 172.26.0.6:34283 with 4 core(s), 2.0 GB RAM
 INFO [2020-02-19 07:17:28,091] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Executor added: app-20200219071728-0000/1 on worker-20200219071324-172.26.0.7-46007 (172.26.0.7:46007) with 4 core(s)
 INFO [2020-02-19 07:17:28,092] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Granted executor ID app-20200219071728-0000/1 on hostPort 172.26.0.7:46007 with 4 core(s), 2.0 GB RAM
 INFO [2020-02-19 07:17:28,145] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Registering BlockManager BlockManagerId(driver, zeppelin, 43939, None)
 INFO [2020-02-19 07:17:28,153] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Registering block manager zeppelin:43939 with 366.3 MB RAM, BlockManagerId(driver, zeppelin, 43939, None)
 INFO [2020-02-19 07:17:28,161] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Registered BlockManager BlockManagerId(driver, zeppelin, 43939, None)
 INFO [2020-02-19 07:17:28,169] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Initialized BlockManager: BlockManagerId(driver, zeppelin, 43939, None)
 INFO [2020-02-19 07:17:28,287] ({dispatcher-event-loop-2} Logging.scala[logInfo]:54) - Executor updated: app-20200219071728-0000/1 is now RUNNING
 INFO [2020-02-19 07:17:28,300] ({dispatcher-event-loop-3} Logging.scala[logInfo]:54) - Executor updated: app-20200219071728-0000/0 is now RUNNING
 INFO [2020-02-19 07:17:28,663] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@27056e3e{/metrics/json,null,AVAILABLE,@Spark}
 INFO [2020-02-19 07:17:28,728] ({pool-2-thread-2} Logging.scala[logInfo]:54) - SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
 INFO [2020-02-19 07:17:28,765] ({pool-2-thread-2} OldSparkInterpreter.java[createSparkSession]:347) - Created Spark session with Hive support
 INFO [2020-02-19 07:17:34,260] ({dispatcher-event-loop-3} Logging.scala[logInfo]:54) - Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.26.0.7:55418) with ID 1
 INFO [2020-02-19 07:17:34,337] ({dispatcher-event-loop-3} Logging.scala[logInfo]:54) - Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.26.0.6:36906) with ID 0
 INFO [2020-02-19 07:17:35,042] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Registering block manager 172.26.0.6:37531 with 912.3 MB RAM, BlockManagerId(0, 172.26.0.6, 37531, None)
 INFO [2020-02-19 07:17:35,054] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Registering block manager 172.26.0.7:37451 with 912.3 MB RAM, BlockManagerId(1, 172.26.0.7, 37451, None)
 INFO [2020-02-19 07:17:41,074] ({pool-2-thread-2} SparkShims.java[loadShims]:62) - Initializing shims for Spark 2.x
 INFO [2020-02-19 07:17:41,084] ({pool-2-thread-2} OldSparkInterpreter.java[populateSparkWebUrl]:931) - Sending metadata to Zeppelin server: {message=Spark UI enabled, url=http://zeppelin:4040}
 INFO [2020-02-19 07:17:42,607] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Block broadcast_0 stored as values in memory (estimated size 236.7 KB, free 366.1 MB)
 INFO [2020-02-19 07:17:42,715] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Block broadcast_0_piece0 stored as bytes in memory (estimated size 22.9 KB, free 366.0 MB)
 INFO [2020-02-19 07:17:42,720] ({dispatcher-event-loop-2} Logging.scala[logInfo]:54) - Added broadcast_0_piece0 in memory on zeppelin:43939 (size: 22.9 KB, free: 366.3 MB)
 INFO [2020-02-19 07:17:42,732] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Created broadcast 0 from textFile at <console>:25
 INFO [2020-02-19 07:17:43,199] ({pool-2-thread-2} FileInputFormat.java[listStatus]:249) - Total input paths to process : 1
 INFO [2020-02-19 07:17:43,276] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Starting job: first at <console>:26
 INFO [2020-02-19 07:17:43,314] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Got job 0 (first at <console>:26) with 1 output partitions
 INFO [2020-02-19 07:17:43,316] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Final stage: ResultStage 0 (first at <console>:26)
 INFO [2020-02-19 07:17:43,319] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Parents of final stage: List()
 INFO [2020-02-19 07:17:43,323] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Missing parents: List()
 INFO [2020-02-19 07:17:43,337] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting ResultStage 0 (/data/test.txt MapPartitionsRDD[1] at textFile at <console>:25), which has no missing parents
 INFO [2020-02-19 07:17:43,447] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_1 stored as values in memory (estimated size 3.4 KB, free 366.0 MB)
 INFO [2020-02-19 07:17:43,466] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.0 KB, free 366.0 MB)
 INFO [2020-02-19 07:17:43,475] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Added broadcast_1_piece0 in memory on zeppelin:43939 (size: 2.0 KB, free: 366.3 MB)
 INFO [2020-02-19 07:17:43,477] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Created broadcast 1 from broadcast at DAGScheduler.scala:1161
 INFO [2020-02-19 07:17:43,527] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting 1 missing tasks from ResultStage 0 (/data/test.txt MapPartitionsRDD[1] at textFile at <console>:25) (first 15 tasks are for partitions Vector(0))
 INFO [2020-02-19 07:17:43,530] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Adding task set 0.0 with 1 tasks
 INFO [2020-02-19 07:17:43,595] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Added task set TaskSet_0.0 tasks to pool default
 INFO [2020-02-19 07:17:43,637] ({dispatcher-event-loop-3} Logging.scala[logInfo]:54) - Starting task 0.0 in stage 0.0 (TID 0, 172.26.0.7, executor 1, partition 0, PROCESS_LOCAL, 7883 bytes)
 INFO [2020-02-19 07:17:44,542] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Added broadcast_1_piece0 in memory on 172.26.0.7:37451 (size: 2.0 KB, free: 912.3 MB)
 INFO [2020-02-19 07:17:45,066] ({dispatcher-event-loop-2} Logging.scala[logInfo]:54) - Added broadcast_0_piece0 in memory on 172.26.0.7:37451 (size: 22.9 KB, free: 912.3 MB)
 WARN [2020-02-19 07:17:45,649] ({task-result-getter-0} Logging.scala[logWarning]:66) - Lost task 0.0 in stage 0.0 (TID 0, 172.26.0.7, executor 1): java.io.FileNotFoundException: File file:/data/test.txt does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:611)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:824)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:601)
	at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:421)
	at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:142)
	at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:346)
	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:769)
	at org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:109)
	at org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.liftedTree1$1(HadoopRDD.scala:267)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:266)
	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:224)
	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:95)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

 INFO [2020-02-19 07:17:45,655] ({dispatcher-event-loop-2} Logging.scala[logInfo]:54) - Starting task 0.1 in stage 0.0 (TID 1, 172.26.0.6, executor 0, partition 0, PROCESS_LOCAL, 7883 bytes)
 INFO [2020-02-19 07:17:46,570] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Added broadcast_1_piece0 in memory on 172.26.0.6:37531 (size: 2.0 KB, free: 912.3 MB)
 INFO [2020-02-19 07:17:46,759] ({dispatcher-event-loop-2} Logging.scala[logInfo]:54) - Added broadcast_0_piece0 in memory on 172.26.0.6:37531 (size: 22.9 KB, free: 912.3 MB)
 INFO [2020-02-19 07:17:47,295] ({task-result-getter-1} Logging.scala[logInfo]:54) - Lost task 0.1 in stage 0.0 (TID 1) on 172.26.0.6, executor 0: java.io.FileNotFoundException (File file:/data/test.txt does not exist) [duplicate 1]
 INFO [2020-02-19 07:17:47,298] ({dispatcher-event-loop-3} Logging.scala[logInfo]:54) - Starting task 0.2 in stage 0.0 (TID 2, 172.26.0.6, executor 0, partition 0, PROCESS_LOCAL, 7883 bytes)
 INFO [2020-02-19 07:17:47,377] ({task-result-getter-2} Logging.scala[logInfo]:54) - Lost task 0.2 in stage 0.0 (TID 2) on 172.26.0.6, executor 0: java.io.FileNotFoundException (File file:/data/test.txt does not exist) [duplicate 2]
 INFO [2020-02-19 07:17:47,380] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Starting task 0.3 in stage 0.0 (TID 3, 172.26.0.7, executor 1, partition 0, PROCESS_LOCAL, 7883 bytes)
 INFO [2020-02-19 07:17:47,446] ({task-result-getter-3} Logging.scala[logInfo]:54) - Lost task 0.3 in stage 0.0 (TID 3) on 172.26.0.7, executor 1: java.io.FileNotFoundException (File file:/data/test.txt does not exist) [duplicate 3]
ERROR [2020-02-19 07:17:47,448] ({task-result-getter-3} Logging.scala[logError]:70) - Task 0 in stage 0.0 failed 4 times; aborting job
 INFO [2020-02-19 07:17:47,453] ({task-result-getter-3} Logging.scala[logInfo]:54) - Removed TaskSet 0.0, whose tasks have all completed, from pool default
 INFO [2020-02-19 07:17:47,457] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Cancelling stage 0
 INFO [2020-02-19 07:17:47,461] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Killing all running tasks in stage 0: Stage cancelled
 INFO [2020-02-19 07:17:47,468] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - ResultStage 0 (first at <console>:26) failed in 4.023 s due to Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 3, 172.26.0.7, executor 1): java.io.FileNotFoundException: File file:/data/test.txt does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:611)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:824)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:601)
	at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:421)
	at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:142)
	at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:346)
	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:769)
	at org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:109)
	at org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.liftedTree1$1(HadoopRDD.scala:267)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:266)
	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:224)
	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:95)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
 INFO [2020-02-19 07:17:47,479] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Job 0 failed: first at <console>:26, took 4.201534 s
 INFO [2020-02-19 07:17:47,997] ({pool-2-thread-2} SchedulerFactory.java[jobFinished]:120) - Job 20200219-065917_1853343063 finished by scheduler interpreter_1181365328
 INFO [2020-02-19 07:18:59,405] ({pool-2-thread-2} SchedulerFactory.java[jobStarted]:114) - Job 20200219-065917_1853343063 started by scheduler interpreter_1181365328
 INFO [2020-02-19 07:18:59,418] ({pool-2-thread-2} OldSparkInterpreter.java[populateSparkWebUrl]:931) - Sending metadata to Zeppelin server: {message=Spark UI enabled, url=http://zeppelin:4040}
ERROR [2020-02-19 07:18:59,705] ({pool-2-thread-2} OldSparkInterpreter.java[putLatestVarInResourcePool]:1209) - 
java.lang.NullPointerException
	at scala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:793)
	at org.apache.zeppelin.spark.OldSparkInterpreter.getLastObject(OldSparkInterpreter.java:1069)
	at org.apache.zeppelin.spark.OldSparkInterpreter.putLatestVarInResourcePool(OldSparkInterpreter.java:1205)
	at org.apache.zeppelin.spark.OldSparkInterpreter.interpretInput(OldSparkInterpreter.java:1189)
	at org.apache.zeppelin.spark.OldSparkInterpreter.interpret(OldSparkInterpreter.java:1101)
	at org.apache.zeppelin.spark.OldSparkInterpreter.interpret(OldSparkInterpreter.java:1092)
	at org.apache.zeppelin.spark.SparkInterpreter.interpret(SparkInterpreter.java:73)
	at org.apache.zeppelin.interpreter.LazyOpenInterpreter.interpret(LazyOpenInterpreter.java:103)
	at org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:632)
	at org.apache.zeppelin.scheduler.Job.run(Job.java:188)
	at org.apache.zeppelin.scheduler.FIFOScheduler$1.run(FIFOScheduler.java:140)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
 INFO [2020-02-19 07:18:59,715] ({pool-2-thread-2} SchedulerFactory.java[jobFinished]:120) - Job 20200219-065917_1853343063 finished by scheduler interpreter_1181365328
 INFO [2020-02-19 07:19:35,507] ({pool-2-thread-3} SchedulerFactory.java[jobStarted]:114) - Job 20200219-065917_1853343063 started by scheduler interpreter_1181365328
 INFO [2020-02-19 07:19:35,518] ({pool-2-thread-3} OldSparkInterpreter.java[populateSparkWebUrl]:931) - Sending metadata to Zeppelin server: {message=Spark UI enabled, url=http://zeppelin:4040}
 INFO [2020-02-19 07:19:36,035] ({pool-2-thread-3} SchedulerFactory.java[jobFinished]:120) - Job 20200219-065917_1853343063 finished by scheduler interpreter_1181365328
 INFO [2020-02-19 07:21:04,974] ({pool-2-thread-2} SchedulerFactory.java[jobStarted]:114) - Job 20200219-065917_1853343063 started by scheduler interpreter_1181365328
 INFO [2020-02-19 07:21:04,980] ({pool-2-thread-2} OldSparkInterpreter.java[populateSparkWebUrl]:931) - Sending metadata to Zeppelin server: {message=Spark UI enabled, url=http://zeppelin:4040}
 INFO [2020-02-19 07:21:05,736] ({pool-2-thread-2} SchedulerFactory.java[jobFinished]:120) - Job 20200219-065917_1853343063 finished by scheduler interpreter_1181365328
 INFO [2020-02-19 07:21:14,034] ({pool-2-thread-4} SchedulerFactory.java[jobStarted]:114) - Job 20200219-065917_1853343063 started by scheduler interpreter_1181365328
 INFO [2020-02-19 07:21:14,046] ({pool-2-thread-4} OldSparkInterpreter.java[populateSparkWebUrl]:931) - Sending metadata to Zeppelin server: {message=Spark UI enabled, url=http://zeppelin:4040}
 INFO [2020-02-19 07:21:14,531] ({pool-2-thread-4} SchedulerFactory.java[jobFinished]:120) - Job 20200219-065917_1853343063 finished by scheduler interpreter_1181365328
 INFO [2020-02-19 07:21:31,276] ({pool-2-thread-3} SchedulerFactory.java[jobStarted]:114) - Job 20200219-065917_1853343063 started by scheduler interpreter_1181365328
 INFO [2020-02-19 07:21:31,291] ({pool-2-thread-3} OldSparkInterpreter.java[populateSparkWebUrl]:931) - Sending metadata to Zeppelin server: {message=Spark UI enabled, url=http://zeppelin:4040}
 INFO [2020-02-19 07:21:32,051] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 9
 INFO [2020-02-19 07:21:32,052] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 1
 INFO [2020-02-19 07:21:32,053] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 17
 INFO [2020-02-19 07:21:32,054] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 10
 INFO [2020-02-19 07:21:32,068] ({pool-2-thread-3} Logging.scala[logInfo]:54) - Block broadcast_2 stored as values in memory (estimated size 236.7 KB, free 365.8 MB)
 INFO [2020-02-19 07:21:32,093] ({pool-2-thread-3} Logging.scala[logInfo]:54) - Block broadcast_2_piece0 stored as bytes in memory (estimated size 22.9 KB, free 365.8 MB)
 INFO [2020-02-19 07:21:32,098] ({dispatcher-event-loop-2} Logging.scala[logInfo]:54) - Added broadcast_2_piece0 in memory on zeppelin:43939 (size: 22.9 KB, free: 366.3 MB)
 INFO [2020-02-19 07:21:32,105] ({pool-2-thread-3} Logging.scala[logInfo]:54) - Created broadcast 2 from textFile at <console>:34
 INFO [2020-02-19 07:21:32,148] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Removed broadcast_1_piece0 on zeppelin:43939 in memory (size: 2.0 KB, free: 366.3 MB)
 INFO [2020-02-19 07:21:32,152] ({dispatcher-event-loop-3} Logging.scala[logInfo]:54) - Removed broadcast_1_piece0 on 172.26.0.7:37451 in memory (size: 2.0 KB, free: 912.3 MB)
 INFO [2020-02-19 07:21:32,158] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Removed broadcast_1_piece0 on 172.26.0.6:37531 in memory (size: 2.0 KB, free: 912.3 MB)
 INFO [2020-02-19 07:21:32,199] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 18
 INFO [2020-02-19 07:21:32,200] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 2
 INFO [2020-02-19 07:21:32,201] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 15
 INFO [2020-02-19 07:21:32,202] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 19
 INFO [2020-02-19 07:21:32,204] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 6
 INFO [2020-02-19 07:21:32,204] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 22
 INFO [2020-02-19 07:21:32,205] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 12
 INFO [2020-02-19 07:21:32,206] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 7
 INFO [2020-02-19 07:21:32,207] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 5
 INFO [2020-02-19 07:21:32,208] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 23
 INFO [2020-02-19 07:21:32,211] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 21
 INFO [2020-02-19 07:21:32,213] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 16
 INFO [2020-02-19 07:21:32,214] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 8
 INFO [2020-02-19 07:21:32,215] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 13
 INFO [2020-02-19 07:21:32,216] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 0
 INFO [2020-02-19 07:21:32,217] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 11
 INFO [2020-02-19 07:21:32,218] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 14
 INFO [2020-02-19 07:21:32,218] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 3
 INFO [2020-02-19 07:21:32,219] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 24
 INFO [2020-02-19 07:21:32,220] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 20
 INFO [2020-02-19 07:21:32,220] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 4
 INFO [2020-02-19 07:21:32,441] ({pool-2-thread-3} FileInputFormat.java[listStatus]:249) - Total input paths to process : 1
 INFO [2020-02-19 07:21:32,455] ({pool-2-thread-3} Logging.scala[logInfo]:54) - Starting job: first at <console>:35
 INFO [2020-02-19 07:21:32,457] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Got job 1 (first at <console>:35) with 1 output partitions
 INFO [2020-02-19 07:21:32,458] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Final stage: ResultStage 1 (first at <console>:35)
 INFO [2020-02-19 07:21:32,459] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Parents of final stage: List()
 INFO [2020-02-19 07:21:32,460] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Missing parents: List()
 INFO [2020-02-19 07:21:32,461] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting ResultStage 1 (///data/test.txt MapPartitionsRDD[3] at textFile at <console>:34), which has no missing parents
 INFO [2020-02-19 07:21:32,469] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_3 stored as values in memory (estimated size 3.4 KB, free 365.8 MB)
 INFO [2020-02-19 07:21:32,472] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.0 KB, free 365.8 MB)
 INFO [2020-02-19 07:21:32,474] ({dispatcher-event-loop-2} Logging.scala[logInfo]:54) - Added broadcast_3_piece0 in memory on zeppelin:43939 (size: 2.0 KB, free: 366.3 MB)
 INFO [2020-02-19 07:21:32,476] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Created broadcast 3 from broadcast at DAGScheduler.scala:1161
 INFO [2020-02-19 07:21:32,478] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting 1 missing tasks from ResultStage 1 (///data/test.txt MapPartitionsRDD[3] at textFile at <console>:34) (first 15 tasks are for partitions Vector(0))
 INFO [2020-02-19 07:21:32,478] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Adding task set 1.0 with 1 tasks
 INFO [2020-02-19 07:21:32,480] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Added task set TaskSet_1.0 tasks to pool default
 INFO [2020-02-19 07:21:32,481] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Starting task 0.0 in stage 1.0 (TID 4, 172.26.0.6, executor 0, partition 0, PROCESS_LOCAL, 7883 bytes)
 INFO [2020-02-19 07:21:32,502] ({dispatcher-event-loop-2} Logging.scala[logInfo]:54) - Added broadcast_3_piece0 in memory on 172.26.0.6:37531 (size: 2.0 KB, free: 912.3 MB)
 INFO [2020-02-19 07:21:32,523] ({dispatcher-event-loop-3} Logging.scala[logInfo]:54) - Added broadcast_2_piece0 in memory on 172.26.0.6:37531 (size: 22.9 KB, free: 912.3 MB)
 WARN [2020-02-19 07:21:32,563] ({task-result-getter-0} Logging.scala[logWarning]:66) - Lost task 0.0 in stage 1.0 (TID 4, 172.26.0.6, executor 0): java.io.FileNotFoundException: File file:/data/test.txt does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:611)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:824)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:601)
	at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:421)
	at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:142)
	at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:346)
	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:769)
	at org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:109)
	at org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.liftedTree1$1(HadoopRDD.scala:267)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:266)
	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:224)
	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:95)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

 INFO [2020-02-19 07:21:32,565] ({dispatcher-event-loop-2} Logging.scala[logInfo]:54) - Starting task 0.1 in stage 1.0 (TID 5, 172.26.0.7, executor 1, partition 0, PROCESS_LOCAL, 7883 bytes)
 INFO [2020-02-19 07:21:32,587] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Added broadcast_3_piece0 in memory on 172.26.0.7:37451 (size: 2.0 KB, free: 912.3 MB)
 INFO [2020-02-19 07:21:32,608] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Added broadcast_2_piece0 in memory on 172.26.0.7:37451 (size: 22.9 KB, free: 912.3 MB)
 INFO [2020-02-19 07:21:32,649] ({task-result-getter-1} Logging.scala[logInfo]:54) - Lost task 0.1 in stage 1.0 (TID 5) on 172.26.0.7, executor 1: java.io.FileNotFoundException (File file:/data/test.txt does not exist) [duplicate 1]
 INFO [2020-02-19 07:21:32,651] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Starting task 0.2 in stage 1.0 (TID 6, 172.26.0.6, executor 0, partition 0, PROCESS_LOCAL, 7883 bytes)
 INFO [2020-02-19 07:21:32,668] ({task-result-getter-2} Logging.scala[logInfo]:54) - Lost task 0.2 in stage 1.0 (TID 6) on 172.26.0.6, executor 0: java.io.FileNotFoundException (File file:/data/test.txt does not exist) [duplicate 2]
 INFO [2020-02-19 07:21:32,670] ({dispatcher-event-loop-3} Logging.scala[logInfo]:54) - Starting task 0.3 in stage 1.0 (TID 7, 172.26.0.6, executor 0, partition 0, PROCESS_LOCAL, 7883 bytes)
 INFO [2020-02-19 07:21:32,686] ({task-result-getter-3} Logging.scala[logInfo]:54) - Lost task 0.3 in stage 1.0 (TID 7) on 172.26.0.6, executor 0: java.io.FileNotFoundException (File file:/data/test.txt does not exist) [duplicate 3]
ERROR [2020-02-19 07:21:32,687] ({task-result-getter-3} Logging.scala[logError]:70) - Task 0 in stage 1.0 failed 4 times; aborting job
 INFO [2020-02-19 07:21:32,688] ({task-result-getter-3} Logging.scala[logInfo]:54) - Removed TaskSet 1.0, whose tasks have all completed, from pool default
 INFO [2020-02-19 07:21:32,689] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Cancelling stage 1
 INFO [2020-02-19 07:21:32,690] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Killing all running tasks in stage 1: Stage cancelled
 INFO [2020-02-19 07:21:32,691] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - ResultStage 1 (first at <console>:35) failed in 0.224 s due to Job aborted due to stage failure: Task 0 in stage 1.0 failed 4 times, most recent failure: Lost task 0.3 in stage 1.0 (TID 7, 172.26.0.6, executor 0): java.io.FileNotFoundException: File file:/data/test.txt does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:611)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:824)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:601)
	at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:421)
	at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:142)
	at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:346)
	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:769)
	at org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:109)
	at org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.liftedTree1$1(HadoopRDD.scala:267)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:266)
	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:224)
	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:95)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
 INFO [2020-02-19 07:21:32,693] ({pool-2-thread-3} Logging.scala[logInfo]:54) - Job 1 failed: first at <console>:35, took 0.237192 s
 INFO [2020-02-19 07:21:32,942] ({pool-2-thread-3} SchedulerFactory.java[jobFinished]:120) - Job 20200219-065917_1853343063 finished by scheduler interpreter_1181365328
 INFO [2020-02-19 07:21:55,133] ({pool-2-thread-5} SchedulerFactory.java[jobStarted]:114) - Job 20200219-065917_1853343063 started by scheduler interpreter_1181365328
 INFO [2020-02-19 07:21:55,139] ({pool-2-thread-5} OldSparkInterpreter.java[populateSparkWebUrl]:931) - Sending metadata to Zeppelin server: {message=Spark UI enabled, url=http://zeppelin:4040}
 INFO [2020-02-19 07:21:55,879] ({pool-2-thread-5} Logging.scala[logInfo]:54) - Block broadcast_4 stored as values in memory (estimated size 236.7 KB, free 365.6 MB)
 INFO [2020-02-19 07:21:55,890] ({pool-2-thread-5} Logging.scala[logInfo]:54) - Block broadcast_4_piece0 stored as bytes in memory (estimated size 22.9 KB, free 365.5 MB)
 INFO [2020-02-19 07:21:55,894] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Added broadcast_4_piece0 in memory on zeppelin:43939 (size: 22.9 KB, free: 366.2 MB)
 INFO [2020-02-19 07:21:55,895] ({pool-2-thread-5} Logging.scala[logInfo]:54) - Created broadcast 4 from textFile at <console>:36
 INFO [2020-02-19 07:21:56,137] ({pool-2-thread-5} FileInputFormat.java[listStatus]:249) - Total input paths to process : 1
 INFO [2020-02-19 07:21:56,152] ({pool-2-thread-5} Logging.scala[logInfo]:54) - Starting job: first at <console>:37
 INFO [2020-02-19 07:21:56,154] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Got job 2 (first at <console>:37) with 1 output partitions
 INFO [2020-02-19 07:21:56,155] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Final stage: ResultStage 2 (first at <console>:37)
 INFO [2020-02-19 07:21:56,156] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Parents of final stage: List()
 INFO [2020-02-19 07:21:56,157] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Missing parents: List()
 INFO [2020-02-19 07:21:56,159] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting ResultStage 2 (/data/test.txt MapPartitionsRDD[5] at textFile at <console>:36), which has no missing parents
 INFO [2020-02-19 07:21:56,163] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_5 stored as values in memory (estimated size 3.4 KB, free 365.5 MB)
 INFO [2020-02-19 07:21:56,166] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_5_piece0 stored as bytes in memory (estimated size 2.0 KB, free 365.5 MB)
 INFO [2020-02-19 07:21:56,168] ({dispatcher-event-loop-3} Logging.scala[logInfo]:54) - Added broadcast_5_piece0 in memory on zeppelin:43939 (size: 2.0 KB, free: 366.2 MB)
 INFO [2020-02-19 07:21:56,169] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Created broadcast 5 from broadcast at DAGScheduler.scala:1161
 INFO [2020-02-19 07:21:56,170] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting 1 missing tasks from ResultStage 2 (/data/test.txt MapPartitionsRDD[5] at textFile at <console>:36) (first 15 tasks are for partitions Vector(0))
 INFO [2020-02-19 07:21:56,172] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Adding task set 2.0 with 1 tasks
 INFO [2020-02-19 07:21:56,174] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Added task set TaskSet_2.0 tasks to pool default
 INFO [2020-02-19 07:21:56,175] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Starting task 0.0 in stage 2.0 (TID 8, 172.26.0.6, executor 0, partition 0, PROCESS_LOCAL, 7883 bytes)
 INFO [2020-02-19 07:21:56,201] ({dispatcher-event-loop-3} Logging.scala[logInfo]:54) - Added broadcast_5_piece0 in memory on 172.26.0.6:37531 (size: 2.0 KB, free: 912.3 MB)
 INFO [2020-02-19 07:21:56,217] ({dispatcher-event-loop-2} Logging.scala[logInfo]:54) - Added broadcast_4_piece0 in memory on 172.26.0.6:37531 (size: 22.9 KB, free: 912.2 MB)
 WARN [2020-02-19 07:21:56,248] ({task-result-getter-0} Logging.scala[logWarning]:66) - Lost task 0.0 in stage 2.0 (TID 8, 172.26.0.6, executor 0): java.io.FileNotFoundException: File file:/data/test.txt does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:611)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:824)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:601)
	at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:421)
	at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:142)
	at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:346)
	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:769)
	at org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:109)
	at org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.liftedTree1$1(HadoopRDD.scala:267)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:266)
	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:224)
	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:95)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

 INFO [2020-02-19 07:21:56,250] ({dispatcher-event-loop-3} Logging.scala[logInfo]:54) - Starting task 0.1 in stage 2.0 (TID 9, 172.26.0.6, executor 0, partition 0, PROCESS_LOCAL, 7883 bytes)
 INFO [2020-02-19 07:21:56,274] ({task-result-getter-1} Logging.scala[logInfo]:54) - Lost task 0.1 in stage 2.0 (TID 9) on 172.26.0.6, executor 0: java.io.FileNotFoundException (File file:/data/test.txt does not exist) [duplicate 1]
 INFO [2020-02-19 07:21:56,276] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Starting task 0.2 in stage 2.0 (TID 10, 172.26.0.7, executor 1, partition 0, PROCESS_LOCAL, 7883 bytes)
 INFO [2020-02-19 07:21:56,365] ({dispatcher-event-loop-2} Logging.scala[logInfo]:54) - Added broadcast_5_piece0 in memory on 172.26.0.7:37451 (size: 2.0 KB, free: 912.3 MB)
 INFO [2020-02-19 07:21:56,392] ({dispatcher-event-loop-3} Logging.scala[logInfo]:54) - Added broadcast_4_piece0 in memory on 172.26.0.7:37451 (size: 22.9 KB, free: 912.2 MB)
 INFO [2020-02-19 07:21:56,425] ({task-result-getter-2} Logging.scala[logInfo]:54) - Lost task 0.2 in stage 2.0 (TID 10) on 172.26.0.7, executor 1: java.io.FileNotFoundException (File file:/data/test.txt does not exist) [duplicate 2]
 INFO [2020-02-19 07:21:56,426] ({dispatcher-event-loop-2} Logging.scala[logInfo]:54) - Starting task 0.3 in stage 2.0 (TID 11, 172.26.0.6, executor 0, partition 0, PROCESS_LOCAL, 7883 bytes)
 INFO [2020-02-19 07:21:56,457] ({task-result-getter-3} Logging.scala[logInfo]:54) - Lost task 0.3 in stage 2.0 (TID 11) on 172.26.0.6, executor 0: java.io.FileNotFoundException (File file:/data/test.txt does not exist) [duplicate 3]
ERROR [2020-02-19 07:21:56,458] ({task-result-getter-3} Logging.scala[logError]:70) - Task 0 in stage 2.0 failed 4 times; aborting job
 INFO [2020-02-19 07:21:56,458] ({task-result-getter-3} Logging.scala[logInfo]:54) - Removed TaskSet 2.0, whose tasks have all completed, from pool default
 INFO [2020-02-19 07:21:56,459] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Cancelling stage 2
 INFO [2020-02-19 07:21:56,460] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Killing all running tasks in stage 2: Stage cancelled
 INFO [2020-02-19 07:21:56,461] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - ResultStage 2 (first at <console>:37) failed in 0.299 s due to Job aborted due to stage failure: Task 0 in stage 2.0 failed 4 times, most recent failure: Lost task 0.3 in stage 2.0 (TID 11, 172.26.0.6, executor 0): java.io.FileNotFoundException: File file:/data/test.txt does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:611)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:824)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:601)
	at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:421)
	at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:142)
	at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:346)
	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:769)
	at org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:109)
	at org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.liftedTree1$1(HadoopRDD.scala:267)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:266)
	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:224)
	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:95)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
 INFO [2020-02-19 07:21:56,462] ({pool-2-thread-5} Logging.scala[logInfo]:54) - Job 2 failed: first at <console>:37, took 0.308652 s
 INFO [2020-02-19 07:21:56,755] ({pool-2-thread-5} SchedulerFactory.java[jobFinished]:120) - Job 20200219-065917_1853343063 finished by scheduler interpreter_1181365328
 INFO [2020-02-19 07:22:09,427] ({pool-2-thread-9} SchedulerFactory.java[jobStarted]:114) - Job 20200219-065917_1853343063 started by scheduler interpreter_1181365328
 INFO [2020-02-19 07:22:09,432] ({pool-2-thread-9} OldSparkInterpreter.java[populateSparkWebUrl]:931) - Sending metadata to Zeppelin server: {message=Spark UI enabled, url=http://zeppelin:4040}
 INFO [2020-02-19 07:22:09,883] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 44
 INFO [2020-02-19 07:22:09,907] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Removed broadcast_3_piece0 on 172.26.0.6:37531 in memory (size: 2.0 KB, free: 912.2 MB)
 INFO [2020-02-19 07:22:09,908] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Removed broadcast_3_piece0 on 172.26.0.7:37451 in memory (size: 2.0 KB, free: 912.2 MB)
 INFO [2020-02-19 07:22:09,919] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Removed broadcast_3_piece0 on zeppelin:43939 in memory (size: 2.0 KB, free: 366.2 MB)
 INFO [2020-02-19 07:22:09,998] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 71
 INFO [2020-02-19 07:22:10,000] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 29
 INFO [2020-02-19 07:22:10,001] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 45
 INFO [2020-02-19 07:22:10,006] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 57
 INFO [2020-02-19 07:22:10,009] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 27
 INFO [2020-02-19 07:22:10,010] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 30
 INFO [2020-02-19 07:22:10,011] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 47
 INFO [2020-02-19 07:22:10,012] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 49
 INFO [2020-02-19 07:22:10,012] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 36
 INFO [2020-02-19 07:22:10,013] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 41
 INFO [2020-02-19 07:22:10,014] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 35
 INFO [2020-02-19 07:22:10,014] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 25
 INFO [2020-02-19 07:22:10,015] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 39
 INFO [2020-02-19 07:22:10,016] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 55
 INFO [2020-02-19 07:22:10,016] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 67
 INFO [2020-02-19 07:22:10,017] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 42
 INFO [2020-02-19 07:22:10,018] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 62
 INFO [2020-02-19 07:22:10,018] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 66
 INFO [2020-02-19 07:22:10,019] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 70
 INFO [2020-02-19 07:22:10,020] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 32
 INFO [2020-02-19 07:22:10,030] ({dispatcher-event-loop-2} Logging.scala[logInfo]:54) - Removed broadcast_5_piece0 on 172.26.0.7:37451 in memory (size: 2.0 KB, free: 912.2 MB)
 INFO [2020-02-19 07:22:10,034] ({dispatcher-event-loop-2} Logging.scala[logInfo]:54) - Removed broadcast_5_piece0 on 172.26.0.6:37531 in memory (size: 2.0 KB, free: 912.2 MB)
 INFO [2020-02-19 07:22:10,038] ({dispatcher-event-loop-2} Logging.scala[logInfo]:54) - Removed broadcast_5_piece0 on zeppelin:43939 in memory (size: 2.0 KB, free: 366.2 MB)
 INFO [2020-02-19 07:22:10,083] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 56
 INFO [2020-02-19 07:22:10,084] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 51
 INFO [2020-02-19 07:22:10,085] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 68
 INFO [2020-02-19 07:22:10,086] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 61
 INFO [2020-02-19 07:22:10,087] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 60
 INFO [2020-02-19 07:22:10,088] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 40
 INFO [2020-02-19 07:22:10,089] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 43
 INFO [2020-02-19 07:22:10,090] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 64
 INFO [2020-02-19 07:22:10,091] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 58
 INFO [2020-02-19 07:22:10,092] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 38
 INFO [2020-02-19 07:22:10,093] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 73
 INFO [2020-02-19 07:22:10,094] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 50
 INFO [2020-02-19 07:22:10,094] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 53
 INFO [2020-02-19 07:22:10,095] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 72
 INFO [2020-02-19 07:22:10,098] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 48
 INFO [2020-02-19 07:22:10,099] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 59
 INFO [2020-02-19 07:22:10,106] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 37
 INFO [2020-02-19 07:22:10,107] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 52
 INFO [2020-02-19 07:22:10,108] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 69
 INFO [2020-02-19 07:22:10,109] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 31
 INFO [2020-02-19 07:22:10,109] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 65
 INFO [2020-02-19 07:22:10,110] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 54
 INFO [2020-02-19 07:22:10,111] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 33
 INFO [2020-02-19 07:22:10,112] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 63
 INFO [2020-02-19 07:22:10,114] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 34
 INFO [2020-02-19 07:22:10,116] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 28
 INFO [2020-02-19 07:22:10,117] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 46
 INFO [2020-02-19 07:22:10,118] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 74
 INFO [2020-02-19 07:22:10,118] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 26
 INFO [2020-02-19 07:22:10,339] ({pool-2-thread-9} Logging.scala[logInfo]:54) - Block broadcast_6 stored as values in memory (estimated size 236.7 KB, free 365.3 MB)
 INFO [2020-02-19 07:22:10,350] ({pool-2-thread-9} Logging.scala[logInfo]:54) - Block broadcast_6_piece0 stored as bytes in memory (estimated size 22.9 KB, free 365.3 MB)
 INFO [2020-02-19 07:22:10,352] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Added broadcast_6_piece0 in memory on zeppelin:43939 (size: 22.9 KB, free: 366.2 MB)
 INFO [2020-02-19 07:22:10,354] ({pool-2-thread-9} Logging.scala[logInfo]:54) - Created broadcast 6 from textFile at <console>:38
 INFO [2020-02-19 07:22:10,607] ({pool-2-thread-9} FileInputFormat.java[listStatus]:249) - Total input paths to process : 1
 INFO [2020-02-19 07:22:10,618] ({pool-2-thread-9} Logging.scala[logInfo]:54) - Starting job: first at <console>:39
 INFO [2020-02-19 07:22:10,622] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Got job 3 (first at <console>:39) with 1 output partitions
 INFO [2020-02-19 07:22:10,623] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Final stage: ResultStage 3 (first at <console>:39)
 INFO [2020-02-19 07:22:10,624] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Parents of final stage: List()
 INFO [2020-02-19 07:22:10,624] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Missing parents: List()
 INFO [2020-02-19 07:22:10,625] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting ResultStage 3 (/data/test.txt MapPartitionsRDD[7] at textFile at <console>:38), which has no missing parents
 INFO [2020-02-19 07:22:10,629] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_7 stored as values in memory (estimated size 3.4 KB, free 365.3 MB)
 INFO [2020-02-19 07:22:10,632] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_7_piece0 stored as bytes in memory (estimated size 2.0 KB, free 365.3 MB)
 INFO [2020-02-19 07:22:10,634] ({dispatcher-event-loop-3} Logging.scala[logInfo]:54) - Added broadcast_7_piece0 in memory on zeppelin:43939 (size: 2.0 KB, free: 366.2 MB)
 INFO [2020-02-19 07:22:10,637] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Created broadcast 7 from broadcast at DAGScheduler.scala:1161
 INFO [2020-02-19 07:22:10,639] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting 1 missing tasks from ResultStage 3 (/data/test.txt MapPartitionsRDD[7] at textFile at <console>:38) (first 15 tasks are for partitions Vector(0))
 INFO [2020-02-19 07:22:10,640] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Adding task set 3.0 with 1 tasks
 INFO [2020-02-19 07:22:10,642] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Added task set TaskSet_3.0 tasks to pool default
 INFO [2020-02-19 07:22:10,644] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Starting task 0.0 in stage 3.0 (TID 12, 172.26.0.6, executor 0, partition 0, PROCESS_LOCAL, 7883 bytes)
 INFO [2020-02-19 07:22:10,665] ({dispatcher-event-loop-3} Logging.scala[logInfo]:54) - Added broadcast_7_piece0 in memory on 172.26.0.6:37531 (size: 2.0 KB, free: 912.2 MB)
 INFO [2020-02-19 07:22:10,684] ({dispatcher-event-loop-2} Logging.scala[logInfo]:54) - Added broadcast_6_piece0 in memory on 172.26.0.6:37531 (size: 22.9 KB, free: 912.2 MB)
 WARN [2020-02-19 07:22:10,718] ({task-result-getter-0} Logging.scala[logWarning]:66) - Lost task 0.0 in stage 3.0 (TID 12, 172.26.0.6, executor 0): java.io.FileNotFoundException: File file:/data/test.txt does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:611)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:824)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:601)
	at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:421)
	at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:142)
	at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:346)
	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:769)
	at org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:109)
	at org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.liftedTree1$1(HadoopRDD.scala:267)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:266)
	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:224)
	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:95)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

 INFO [2020-02-19 07:22:10,722] ({dispatcher-event-loop-3} Logging.scala[logInfo]:54) - Starting task 0.1 in stage 3.0 (TID 13, 172.26.0.6, executor 0, partition 0, PROCESS_LOCAL, 7883 bytes)
 INFO [2020-02-19 07:22:10,740] ({task-result-getter-1} Logging.scala[logInfo]:54) - Lost task 0.1 in stage 3.0 (TID 13) on 172.26.0.6, executor 0: java.io.FileNotFoundException (File file:/data/test.txt does not exist) [duplicate 1]
 INFO [2020-02-19 07:22:10,742] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Starting task 0.2 in stage 3.0 (TID 14, 172.26.0.6, executor 0, partition 0, PROCESS_LOCAL, 7883 bytes)
 INFO [2020-02-19 07:22:10,758] ({task-result-getter-2} Logging.scala[logInfo]:54) - Lost task 0.2 in stage 3.0 (TID 14) on 172.26.0.6, executor 0: java.io.FileNotFoundException (File file:/data/test.txt does not exist) [duplicate 2]
 INFO [2020-02-19 07:22:10,760] ({dispatcher-event-loop-2} Logging.scala[logInfo]:54) - Starting task 0.3 in stage 3.0 (TID 15, 172.26.0.6, executor 0, partition 0, PROCESS_LOCAL, 7883 bytes)
 INFO [2020-02-19 07:22:10,775] ({task-result-getter-3} Logging.scala[logInfo]:54) - Lost task 0.3 in stage 3.0 (TID 15) on 172.26.0.6, executor 0: java.io.FileNotFoundException (File file:/data/test.txt does not exist) [duplicate 3]
ERROR [2020-02-19 07:22:10,776] ({task-result-getter-3} Logging.scala[logError]:70) - Task 0 in stage 3.0 failed 4 times; aborting job
 INFO [2020-02-19 07:22:10,777] ({task-result-getter-3} Logging.scala[logInfo]:54) - Removed TaskSet 3.0, whose tasks have all completed, from pool default
 INFO [2020-02-19 07:22:10,779] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Cancelling stage 3
 INFO [2020-02-19 07:22:10,780] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Killing all running tasks in stage 3: Stage cancelled
 INFO [2020-02-19 07:22:10,782] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - ResultStage 3 (first at <console>:39) failed in 0.155 s due to Job aborted due to stage failure: Task 0 in stage 3.0 failed 4 times, most recent failure: Lost task 0.3 in stage 3.0 (TID 15, 172.26.0.6, executor 0): java.io.FileNotFoundException: File file:/data/test.txt does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:611)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:824)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:601)
	at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:421)
	at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:142)
	at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:346)
	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:769)
	at org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:109)
	at org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.liftedTree1$1(HadoopRDD.scala:267)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:266)
	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:224)
	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:95)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
 INFO [2020-02-19 07:22:10,784] ({pool-2-thread-9} Logging.scala[logInfo]:54) - Job 3 failed: first at <console>:39, took 0.163977 s
 INFO [2020-02-19 07:22:11,104] ({pool-2-thread-9} SchedulerFactory.java[jobFinished]:120) - Job 20200219-065917_1853343063 finished by scheduler interpreter_1181365328
 INFO [2020-02-19 07:23:10,588] ({pool-2-thread-6} SchedulerFactory.java[jobStarted]:114) - Job 20200219-065917_1853343063 started by scheduler interpreter_1181365328
 INFO [2020-02-19 07:23:10,599] ({pool-2-thread-6} OldSparkInterpreter.java[populateSparkWebUrl]:931) - Sending metadata to Zeppelin server: {message=Spark UI enabled, url=http://zeppelin:4040}
 INFO [2020-02-19 07:23:11,155] ({pool-2-thread-6} SchedulerFactory.java[jobFinished]:120) - Job 20200219-065917_1853343063 finished by scheduler interpreter_1181365328
 INFO [2020-02-19 07:25:37,075] ({pool-2-thread-4} SchedulerFactory.java[jobStarted]:114) - Job 20200219-065917_1853343063 started by scheduler interpreter_1181365328
 INFO [2020-02-19 07:25:37,085] ({pool-2-thread-4} OldSparkInterpreter.java[populateSparkWebUrl]:931) - Sending metadata to Zeppelin server: {message=Spark UI enabled, url=http://zeppelin:4040}
 INFO [2020-02-19 07:25:37,847] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 76
 INFO [2020-02-19 07:25:37,850] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 82
 INFO [2020-02-19 07:25:37,851] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 93
 INFO [2020-02-19 07:25:37,853] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 96
 INFO [2020-02-19 07:25:37,854] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 86
 INFO [2020-02-19 07:25:37,855] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 95
 INFO [2020-02-19 07:25:37,855] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 87
 INFO [2020-02-19 07:25:37,856] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 99
 INFO [2020-02-19 07:25:37,864] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 77
 INFO [2020-02-19 07:25:37,865] ({pool-2-thread-4} Logging.scala[logInfo]:54) - Block broadcast_8 stored as values in memory (estimated size 236.7 KB, free 365.0 MB)
 INFO [2020-02-19 07:25:37,867] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 75
 INFO [2020-02-19 07:25:37,869] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 83
 INFO [2020-02-19 07:25:37,870] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 84
 INFO [2020-02-19 07:25:37,871] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 90
 INFO [2020-02-19 07:25:37,872] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 94
 INFO [2020-02-19 07:25:37,873] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 80
 INFO [2020-02-19 07:25:37,873] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 79
 INFO [2020-02-19 07:25:37,874] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 81
 INFO [2020-02-19 07:25:37,875] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 98
 INFO [2020-02-19 07:25:37,875] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 97
 INFO [2020-02-19 07:25:37,876] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 92
 INFO [2020-02-19 07:25:37,876] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 88
 INFO [2020-02-19 07:25:37,877] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 89
 INFO [2020-02-19 07:25:37,877] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 78
 INFO [2020-02-19 07:25:37,878] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 91
 INFO [2020-02-19 07:25:37,886] ({pool-2-thread-4} Logging.scala[logInfo]:54) - Block broadcast_8_piece0 stored as bytes in memory (estimated size 22.9 KB, free 365.0 MB)
 INFO [2020-02-19 07:25:37,900] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Added broadcast_8_piece0 in memory on zeppelin:43939 (size: 22.9 KB, free: 366.2 MB)
 INFO [2020-02-19 07:25:37,903] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Removed broadcast_7_piece0 on 172.26.0.6:37531 in memory (size: 2.0 KB, free: 912.2 MB)
 INFO [2020-02-19 07:25:37,903] ({pool-2-thread-4} Logging.scala[logInfo]:54) - Created broadcast 8 from textFile at <console>:42
 INFO [2020-02-19 07:25:37,904] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Removed broadcast_7_piece0 on zeppelin:43939 in memory (size: 2.0 KB, free: 366.2 MB)
 INFO [2020-02-19 07:25:37,933] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 85
 INFO [2020-02-19 07:25:38,480] ({pool-2-thread-4} SchedulerFactory.java[jobFinished]:120) - Job 20200219-065917_1853343063 finished by scheduler interpreter_1181365328
ERROR [2020-02-19 07:27:36,777] ({dispatcher-event-loop-3} Logging.scala[logError]:70) - Lost executor 1 on 172.26.0.7: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
 INFO [2020-02-19 07:27:36,796] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Executor lost: 1 (epoch 0)
 INFO [2020-02-19 07:27:36,799] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Trying to remove executor 1 from BlockManagerMaster.
 INFO [2020-02-19 07:27:36,800] ({dispatcher-event-loop-2} Logging.scala[logInfo]:54) - Executor updated: app-20200219071728-0000/0 is now LOST (worker lost)
ERROR [2020-02-19 07:27:36,805] ({dispatcher-event-loop-3} Logging.scala[logError]:70) - Lost executor 0 on 172.26.0.6: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
 INFO [2020-02-19 07:27:36,811] ({dispatcher-event-loop-2} Logging.scala[logInfo]:54) - Executor app-20200219071728-0000/0 removed: worker lost
 INFO [2020-02-19 07:27:36,812] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Removing block manager BlockManagerId(1, 172.26.0.7, 37451, None)
 INFO [2020-02-19 07:27:36,819] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Removed 1 successfully in removeExecutor
 INFO [2020-02-19 07:27:36,832] ({dispatcher-event-loop-2} Logging.scala[logInfo]:54) - Master removed worker worker-20200219071324-172.26.0.6-34283: 172.26.0.6:34283 got disassociated
 INFO [2020-02-19 07:27:36,832] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Shuffle files lost for executor: 1 (epoch 0)
 INFO [2020-02-19 07:27:36,834] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Removal of executor 0 requested
 INFO [2020-02-19 07:27:36,833] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Trying to remove executor 0 from BlockManagerMaster.
 INFO [2020-02-19 07:27:36,837] ({dispatcher-event-loop-2} Logging.scala[logInfo]:54) - Worker worker-20200219071324-172.26.0.6-34283 removed: 172.26.0.6:34283 got disassociated
 INFO [2020-02-19 07:27:36,838] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Removing block manager BlockManagerId(0, 172.26.0.6, 37531, None)
 INFO [2020-02-19 07:27:36,841] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Asked to remove non-existent executor 0
 INFO [2020-02-19 07:27:36,842] ({dispatcher-event-loop-2} Logging.scala[logInfo]:54) - Executor updated: app-20200219071728-0000/1 is now LOST (worker lost)
 INFO [2020-02-19 07:27:36,844] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Executor lost: 0 (epoch 1)
 INFO [2020-02-19 07:27:36,845] ({dispatcher-event-loop-2} Logging.scala[logInfo]:54) - Executor app-20200219071728-0000/1 removed: worker lost
 INFO [2020-02-19 07:27:36,846] ({dispatcher-event-loop-3} Logging.scala[logInfo]:54) - Trying to remove executor 0 from BlockManagerMaster.
 INFO [2020-02-19 07:27:36,847] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Handle removed worker worker-20200219071324-172.26.0.6-34283: 172.26.0.6:34283 got disassociated
 INFO [2020-02-19 07:27:36,847] ({dispatcher-event-loop-2} Logging.scala[logInfo]:54) - Master removed worker worker-20200219071324-172.26.0.7-46007: 172.26.0.7:46007 got disassociated
 INFO [2020-02-19 07:27:36,849] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Removal of executor 1 requested
 INFO [2020-02-19 07:27:36,850] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Asked to remove non-existent executor 1
 INFO [2020-02-19 07:27:36,849] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Trying to remove executor 1 from BlockManagerMaster.
 INFO [2020-02-19 07:27:36,848] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Removed 0 successfully in removeExecutor
 INFO [2020-02-19 07:27:36,850] ({dispatcher-event-loop-2} Logging.scala[logInfo]:54) - Worker worker-20200219071324-172.26.0.7-46007 removed: 172.26.0.7:46007 got disassociated
 INFO [2020-02-19 07:27:36,852] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Shuffle files lost for executor: 0 (epoch 1)
 INFO [2020-02-19 07:27:36,853] ({dispatcher-event-loop-3} Logging.scala[logInfo]:54) - Handle removed worker worker-20200219071324-172.26.0.7-46007: 172.26.0.7:46007 got disassociated
 INFO [2020-02-19 07:27:36,855] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Shuffle files lost for worker worker-20200219071324-172.26.0.6-34283 on host 172.26.0.6
 INFO [2020-02-19 07:27:36,857] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Shuffle files lost for worker worker-20200219071324-172.26.0.7-46007 on host 172.26.0.7
 WARN [2020-02-19 07:27:47,602] ({dispatcher-event-loop-3} Logging.scala[logWarning]:66) - Connection to spark-master:7077 failed; waiting for master to reconnect...
 WARN [2020-02-19 07:27:47,605] ({dispatcher-event-loop-3} Logging.scala[logWarning]:66) - Disconnected from Spark cluster! Waiting for reconnection...
 WARN [2020-02-19 07:27:47,606] ({dispatcher-event-loop-3} Logging.scala[logWarning]:66) - Connection to spark-master:7077 failed; waiting for master to reconnect...
 INFO [2020-02-19 07:27:50,066] ({pool-1-thread-1} OldSparkInterpreter.java[close]:1243) - Close interpreter
 INFO [2020-02-19 07:27:50,080] ({pool-1-thread-1} AbstractConnector.java[doStop]:318) - Stopped Spark@76d5a8de{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
 INFO [2020-02-19 07:27:50,084] ({pool-1-thread-1} Logging.scala[logInfo]:54) - Stopped Spark web UI at http://zeppelin:4040
 INFO [2020-02-19 07:27:50,092] ({pool-1-thread-1} Logging.scala[logInfo]:54) - Shutting down all executors
 INFO [2020-02-19 07:27:50,093] ({dispatcher-event-loop-3} Logging.scala[logInfo]:54) - Asking each executor to shut down
 INFO [2020-02-19 07:27:50,113] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - MapOutputTrackerMasterEndpoint stopped!
 INFO [2020-02-19 07:27:50,177] ({pool-1-thread-1} Logging.scala[logInfo]:54) - MemoryStore cleared
 INFO [2020-02-19 07:27:50,178] ({pool-1-thread-1} Logging.scala[logInfo]:54) - BlockManager stopped
 INFO [2020-02-19 07:27:50,180] ({pool-1-thread-1} Logging.scala[logInfo]:54) - BlockManagerMaster stopped
 INFO [2020-02-19 07:27:50,184] ({dispatcher-event-loop-3} Logging.scala[logInfo]:54) - OutputCommitCoordinator stopped!
 INFO [2020-02-19 07:27:50,192] ({pool-1-thread-1} Logging.scala[logInfo]:54) - Successfully stopped SparkContext
 INFO [2020-02-19 07:27:50,213] ({pool-1-thread-1} RemoteInterpreterServer.java[shutdown]:209) - Shutting down...
 INFO [2020-02-19 07:27:52,360] ({Thread-1} Logging.scala[logInfo]:54) - Shutdown hook called
 INFO [2020-02-19 07:27:52,366] ({Thread-1} Logging.scala[logInfo]:54) - Deleting directory /tmp/spark-5d77dc82-b483-467b-8d61-e89cfd923639
 INFO [2020-02-19 07:27:52,407] ({Thread-1} Logging.scala[logInfo]:54) - Deleting directory /tmp/spark-d55a1861-d7a4-40dd-b5c6-55a9098b62f2
 INFO [2020-02-19 07:27:52,412] ({Thread-1} Logging.scala[logInfo]:54) - Deleting directory /tmp/spark-3bd03e79-241b-4822-9c1c-b7557c0a2ac9
 WARN [2020-02-19 07:29:07,266] ({main} NativeCodeLoader.java[<clinit>]:62) - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
 INFO [2020-02-19 07:29:07,574] ({main} RemoteInterpreterServer.java[main]:261) - URL:jar:file:/zeppelin/interpreter/spark/spark-interpreter-0.8.2.jar!/org/apache/zeppelin/interpreter/remote/RemoteInterpreterServer.class
 INFO [2020-02-19 07:29:07,602] ({main} RemoteInterpreterServer.java[<init>]:162) - Launching ThriftServer at 172.27.0.3:39881
 INFO [2020-02-19 07:29:07,605] ({main} RemoteInterpreterServer.java[<init>]:166) - Starting remote interpreter server on port 39881
 INFO [2020-02-19 07:29:07,607] ({Thread-3} RemoteInterpreterServer.java[run]:203) - Starting remote interpreter server on port 39881
 INFO [2020-02-19 07:29:08,633] ({Thread-4} RemoteInterpreterUtils.java[registerInterpreter]:165) - callbackHost: 172.27.0.3, callbackPort: 36979, callbackInfo: CallbackInfo(host:172.27.0.3, port:39881)
 INFO [2020-02-19 07:29:08,924] ({pool-1-thread-1} RemoteInterpreterServer.java[createInterpreter]:311) - Instantiate interpreter org.apache.zeppelin.spark.SparkInterpreter
 INFO [2020-02-19 07:29:08,929] ({pool-1-thread-1} RemoteInterpreterServer.java[createInterpreter]:311) - Instantiate interpreter org.apache.zeppelin.spark.SparkSqlInterpreter
 INFO [2020-02-19 07:29:08,934] ({pool-1-thread-1} RemoteInterpreterServer.java[createInterpreter]:311) - Instantiate interpreter org.apache.zeppelin.spark.DepInterpreter
 INFO [2020-02-19 07:29:08,939] ({pool-1-thread-1} RemoteInterpreterServer.java[createInterpreter]:311) - Instantiate interpreter org.apache.zeppelin.spark.PySparkInterpreter
 INFO [2020-02-19 07:29:08,945] ({pool-1-thread-1} RemoteInterpreterServer.java[createInterpreter]:311) - Instantiate interpreter org.apache.zeppelin.spark.IPySparkInterpreter
 INFO [2020-02-19 07:29:08,949] ({pool-1-thread-1} RemoteInterpreterServer.java[createInterpreter]:311) - Instantiate interpreter org.apache.zeppelin.spark.SparkRInterpreter
 WARN [2020-02-19 07:29:09,086] ({pool-1-thread-1} ZeppelinConfiguration.java[create]:117) - Failed to load configuration, proceeding with a default
 INFO [2020-02-19 07:29:09,122] ({pool-1-thread-1} ZeppelinConfiguration.java[create]:129) - Server Host: 0.0.0.0
 INFO [2020-02-19 07:29:09,123] ({pool-1-thread-1} ZeppelinConfiguration.java[create]:131) - Server Port: 8080
 INFO [2020-02-19 07:29:09,124] ({pool-1-thread-1} ZeppelinConfiguration.java[create]:135) - Context Path: /
 INFO [2020-02-19 07:29:09,129] ({pool-1-thread-1} ZeppelinConfiguration.java[create]:136) - Zeppelin Version: 0.8.2
 INFO [2020-02-19 07:29:09,130] ({pool-1-thread-1} SchedulerFactory.java[<init>]:59) - Scheduler Thread Pool Size: 100
 INFO [2020-02-19 07:29:09,139] ({pool-2-thread-2} SchedulerFactory.java[jobStarted]:114) - Job 20200219-065917_1853343063 started by scheduler interpreter_299988080
 INFO [2020-02-19 07:29:13,475] ({pool-2-thread-2} OldSparkInterpreter.java[createSparkSession]:311) - ------ Create new SparkSession spark://spark-master:7077 -------
 INFO [2020-02-19 07:29:13,777] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Running Spark version 2.4.0
 INFO [2020-02-19 07:29:13,812] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Submitted application: Zeppelin
 INFO [2020-02-19 07:29:13,900] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Changing view acls to: root
 INFO [2020-02-19 07:29:13,901] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Changing modify acls to: root
 INFO [2020-02-19 07:29:13,902] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Changing view acls groups to: 
 INFO [2020-02-19 07:29:13,903] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Changing modify acls groups to: 
 INFO [2020-02-19 07:29:13,904] ({pool-2-thread-2} Logging.scala[logInfo]:54) - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
 INFO [2020-02-19 07:29:14,252] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Successfully started service 'sparkDriver' on port 40417.
 INFO [2020-02-19 07:29:14,290] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Registering MapOutputTracker
 INFO [2020-02-19 07:29:14,321] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Registering BlockManagerMaster
 INFO [2020-02-19 07:29:14,327] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
 INFO [2020-02-19 07:29:14,329] ({pool-2-thread-2} Logging.scala[logInfo]:54) - BlockManagerMasterEndpoint up
 INFO [2020-02-19 07:29:14,355] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Created local directory at /tmp/blockmgr-adc3bd28-558b-42c5-98db-aae0b427cb56
 INFO [2020-02-19 07:29:14,377] ({pool-2-thread-2} Logging.scala[logInfo]:54) - MemoryStore started with capacity 366.3 MB
 INFO [2020-02-19 07:29:14,406] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Registering OutputCommitCoordinator
 INFO [2020-02-19 07:29:14,557] ({pool-2-thread-2} Log.java[initialized]:192) - Logging initialized @9126ms
 INFO [2020-02-19 07:29:14,663] ({pool-2-thread-2} Server.java[doStart]:351) - jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
 INFO [2020-02-19 07:29:15,028] ({pool-2-thread-2} Server.java[doStart]:419) - Started @9598ms
 INFO [2020-02-19 07:29:15,051] ({pool-2-thread-2} AbstractConnector.java[doStart]:278) - Started ServerConnector@536a4302{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
 INFO [2020-02-19 07:29:15,053] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Successfully started service 'SparkUI' on port 4040.
 INFO [2020-02-19 07:29:15,088] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@3aacc84f{/jobs,null,AVAILABLE,@Spark}
 INFO [2020-02-19 07:29:15,092] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@1fa95566{/jobs/json,null,AVAILABLE,@Spark}
 INFO [2020-02-19 07:29:15,094] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@403bf899{/jobs/job,null,AVAILABLE,@Spark}
 INFO [2020-02-19 07:29:15,096] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@7e3921cc{/jobs/job/json,null,AVAILABLE,@Spark}
 INFO [2020-02-19 07:29:15,098] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@2bee5908{/stages,null,AVAILABLE,@Spark}
 INFO [2020-02-19 07:29:15,100] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@348e93dc{/stages/json,null,AVAILABLE,@Spark}
 INFO [2020-02-19 07:29:15,101] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@4f6e2b9c{/stages/stage,null,AVAILABLE,@Spark}
 INFO [2020-02-19 07:29:15,104] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@61c1136d{/stages/stage/json,null,AVAILABLE,@Spark}
 INFO [2020-02-19 07:29:15,109] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@5f4de602{/stages/pool,null,AVAILABLE,@Spark}
 INFO [2020-02-19 07:29:15,111] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@558e10b3{/stages/pool/json,null,AVAILABLE,@Spark}
 INFO [2020-02-19 07:29:15,113] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@7f924886{/storage,null,AVAILABLE,@Spark}
 INFO [2020-02-19 07:29:15,114] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@16a16662{/storage/json,null,AVAILABLE,@Spark}
 INFO [2020-02-19 07:29:15,116] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@266cef42{/storage/rdd,null,AVAILABLE,@Spark}
 INFO [2020-02-19 07:29:15,118] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@34278b4{/storage/rdd/json,null,AVAILABLE,@Spark}
 INFO [2020-02-19 07:29:15,119] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@6e79c622{/environment,null,AVAILABLE,@Spark}
 INFO [2020-02-19 07:29:15,121] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@73f62ff9{/environment/json,null,AVAILABLE,@Spark}
 INFO [2020-02-19 07:29:15,123] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@3221c83a{/executors,null,AVAILABLE,@Spark}
 INFO [2020-02-19 07:29:15,129] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@22368112{/executors/json,null,AVAILABLE,@Spark}
 INFO [2020-02-19 07:29:15,130] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@2de1ba0{/executors/threadDump,null,AVAILABLE,@Spark}
 INFO [2020-02-19 07:29:15,132] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@409565e5{/executors/threadDump/json,null,AVAILABLE,@Spark}
 INFO [2020-02-19 07:29:15,144] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@7c4915d5{/static,null,AVAILABLE,@Spark}
 INFO [2020-02-19 07:29:15,146] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@71e7a805{/,null,AVAILABLE,@Spark}
 INFO [2020-02-19 07:29:15,149] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@6b197607{/api,null,AVAILABLE,@Spark}
 INFO [2020-02-19 07:29:15,150] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@1fc429e4{/jobs/job/kill,null,AVAILABLE,@Spark}
 INFO [2020-02-19 07:29:15,152] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@54270789{/stages/stage/kill,null,AVAILABLE,@Spark}
 INFO [2020-02-19 07:29:15,155] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Bound SparkUI to 0.0.0.0, and started at http://zeppelin:4040
 INFO [2020-02-19 07:29:15,193] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Added JAR file:/zeppelin/interpreter/spark/spark-interpreter-0.8.2.jar at spark://zeppelin:40417/jars/spark-interpreter-0.8.2.jar with timestamp 1582097355193
 WARN [2020-02-19 07:29:15,259] ({pool-2-thread-2} Logging.scala[logWarning]:66) - Fair Scheduler configuration file not found so jobs will be scheduled in FIFO order. To use fair scheduling, configure pools in fairscheduler.xml or set spark.scheduler.allocation.file to a file that contains the configuration.
 INFO [2020-02-19 07:29:15,267] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Created default pool: default, schedulingMode: FIFO, minShare: 0, weight: 1
 INFO [2020-02-19 07:29:15,326] ({appclient-register-master-threadpool-0} Logging.scala[logInfo]:54) - Connecting to master spark://spark-master:7077...
 INFO [2020-02-19 07:29:15,403] ({netty-rpc-connection-0} TransportClientFactory.java[createClient]:267) - Successfully created connection to spark-master/172.27.0.5:7077 after 50 ms (0 ms spent in bootstraps)
 INFO [2020-02-19 07:29:15,553] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Connected to Spark cluster with app ID app-20200219072915-0000
 INFO [2020-02-19 07:29:15,568] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44945.
 INFO [2020-02-19 07:29:15,569] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Server created on zeppelin:44945
 INFO [2020-02-19 07:29:15,573] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
 INFO [2020-02-19 07:29:15,617] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Executor added: app-20200219072915-0000/0 on worker-20200219072825-172.27.0.6-44371 (172.27.0.6:44371) with 4 core(s)
 INFO [2020-02-19 07:29:15,624] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Granted executor ID app-20200219072915-0000/0 on hostPort 172.27.0.6:44371 with 4 core(s), 2.0 GB RAM
 INFO [2020-02-19 07:29:15,636] ({dispatcher-event-loop-2} Logging.scala[logInfo]:54) - Executor added: app-20200219072915-0000/1 on worker-20200219072825-172.27.0.7-41643 (172.27.0.7:41643) with 4 core(s)
 INFO [2020-02-19 07:29:15,639] ({dispatcher-event-loop-2} Logging.scala[logInfo]:54) - Granted executor ID app-20200219072915-0000/1 on hostPort 172.27.0.7:41643 with 4 core(s), 2.0 GB RAM
 INFO [2020-02-19 07:29:15,677] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Registering BlockManager BlockManagerId(driver, zeppelin, 44945, None)
 INFO [2020-02-19 07:29:15,686] ({dispatcher-event-loop-3} Logging.scala[logInfo]:54) - Registering block manager zeppelin:44945 with 366.3 MB RAM, BlockManagerId(driver, zeppelin, 44945, None)
 INFO [2020-02-19 07:29:15,701] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Registered BlockManager BlockManagerId(driver, zeppelin, 44945, None)
 INFO [2020-02-19 07:29:15,711] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Initialized BlockManager: BlockManagerId(driver, zeppelin, 44945, None)
 INFO [2020-02-19 07:29:15,840] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Executor updated: app-20200219072915-0000/1 is now RUNNING
 INFO [2020-02-19 07:29:15,859] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Executor updated: app-20200219072915-0000/0 is now RUNNING
 INFO [2020-02-19 07:29:16,270] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@27233fe8{/metrics/json,null,AVAILABLE,@Spark}
 INFO [2020-02-19 07:29:16,343] ({pool-2-thread-2} Logging.scala[logInfo]:54) - SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
 INFO [2020-02-19 07:29:16,422] ({pool-2-thread-2} OldSparkInterpreter.java[createSparkSession]:347) - Created Spark session with Hive support
 INFO [2020-02-19 07:29:21,562] ({dispatcher-event-loop-2} Logging.scala[logInfo]:54) - Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.27.0.7:37746) with ID 1
 INFO [2020-02-19 07:29:21,613] ({dispatcher-event-loop-2} Logging.scala[logInfo]:54) - Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.27.0.6:47354) with ID 0
 INFO [2020-02-19 07:29:22,150] ({dispatcher-event-loop-3} Logging.scala[logInfo]:54) - Registering block manager 172.27.0.6:36245 with 912.3 MB RAM, BlockManagerId(0, 172.27.0.6, 36245, None)
 INFO [2020-02-19 07:29:22,187] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Registering block manager 172.27.0.7:42597 with 912.3 MB RAM, BlockManagerId(1, 172.27.0.7, 42597, None)
 INFO [2020-02-19 07:29:27,789] ({pool-2-thread-2} SparkShims.java[loadShims]:62) - Initializing shims for Spark 2.x
 INFO [2020-02-19 07:29:27,797] ({pool-2-thread-2} OldSparkInterpreter.java[populateSparkWebUrl]:931) - Sending metadata to Zeppelin server: {message=Spark UI enabled, url=http://zeppelin:4040}
 INFO [2020-02-19 07:29:28,439] ({pool-2-thread-2} SchedulerFactory.java[jobFinished]:120) - Job 20200219-065917_1853343063 finished by scheduler interpreter_299988080
 INFO [2020-02-19 07:29:41,939] ({pool-2-thread-2} SchedulerFactory.java[jobStarted]:114) - Job 20200219-065917_1853343063 started by scheduler interpreter_299988080
 INFO [2020-02-19 07:29:41,955] ({pool-2-thread-2} OldSparkInterpreter.java[populateSparkWebUrl]:931) - Sending metadata to Zeppelin server: {message=Spark UI enabled, url=http://zeppelin:4040}
 INFO [2020-02-19 07:29:42,441] ({pool-2-thread-2} SchedulerFactory.java[jobFinished]:120) - Job 20200219-065917_1853343063 finished by scheduler interpreter_299988080
 INFO [2020-02-19 07:32:01,832] ({pool-2-thread-4} SchedulerFactory.java[jobStarted]:114) - Job 20200219-065917_1853343063 started by scheduler interpreter_299988080
 INFO [2020-02-19 07:32:01,838] ({pool-2-thread-4} OldSparkInterpreter.java[populateSparkWebUrl]:931) - Sending metadata to Zeppelin server: {message=Spark UI enabled, url=http://zeppelin:4040}
 INFO [2020-02-19 07:32:02,415] ({pool-2-thread-4} SchedulerFactory.java[jobFinished]:120) - Job 20200219-065917_1853343063 finished by scheduler interpreter_299988080
 INFO [2020-02-19 07:33:07,613] ({pool-2-thread-2} SchedulerFactory.java[jobStarted]:114) - Job 20200219-065917_1853343063 started by scheduler interpreter_299988080
 INFO [2020-02-19 07:33:07,624] ({pool-2-thread-2} OldSparkInterpreter.java[populateSparkWebUrl]:931) - Sending metadata to Zeppelin server: {message=Spark UI enabled, url=http://zeppelin:4040}
 INFO [2020-02-19 07:33:09,168] ({pool-2-thread-2} SchedulerFactory.java[jobFinished]:120) - Job 20200219-065917_1853343063 finished by scheduler interpreter_299988080
 INFO [2020-02-19 07:34:40,694] ({pool-2-thread-3} SchedulerFactory.java[jobStarted]:114) - Job 20200219-065917_1853343063 started by scheduler interpreter_299988080
 INFO [2020-02-19 07:34:40,701] ({pool-2-thread-3} OldSparkInterpreter.java[populateSparkWebUrl]:931) - Sending metadata to Zeppelin server: {message=Spark UI enabled, url=http://zeppelin:4040}
 INFO [2020-02-19 07:34:42,572] ({pool-2-thread-3} Logging.scala[logInfo]:54) - Block broadcast_0 stored as values in memory (estimated size 236.7 KB, free 366.1 MB)
 INFO [2020-02-19 07:34:42,641] ({pool-2-thread-3} Logging.scala[logInfo]:54) - Block broadcast_0_piece0 stored as bytes in memory (estimated size 22.9 KB, free 366.0 MB)
 INFO [2020-02-19 07:34:42,644] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Added broadcast_0_piece0 in memory on zeppelin:44945 (size: 22.9 KB, free: 366.3 MB)
 INFO [2020-02-19 07:34:42,657] ({pool-2-thread-3} Logging.scala[logInfo]:54) - Created broadcast 0 from textFile at <console>:36
 INFO [2020-02-19 07:34:43,095] ({pool-2-thread-3} FileInputFormat.java[listStatus]:249) - Total input paths to process : 1
 INFO [2020-02-19 07:34:43,163] ({pool-2-thread-3} Logging.scala[logInfo]:54) - Starting job: first at <console>:37
 INFO [2020-02-19 07:34:43,195] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Got job 0 (first at <console>:37) with 1 output partitions
 INFO [2020-02-19 07:34:43,197] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Final stage: ResultStage 0 (first at <console>:37)
 INFO [2020-02-19 07:34:43,199] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Parents of final stage: List()
 INFO [2020-02-19 07:34:43,202] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Missing parents: List()
 INFO [2020-02-19 07:34:43,216] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting ResultStage 0 (data/test.txt MapPartitionsRDD[1] at textFile at <console>:36), which has no missing parents
 INFO [2020-02-19 07:34:43,252] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_1 stored as values in memory (estimated size 3.4 KB, free 366.0 MB)
 INFO [2020-02-19 07:34:43,257] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.0 KB, free 366.0 MB)
 INFO [2020-02-19 07:34:43,259] ({dispatcher-event-loop-2} Logging.scala[logInfo]:54) - Added broadcast_1_piece0 in memory on zeppelin:44945 (size: 2.0 KB, free: 366.3 MB)
 INFO [2020-02-19 07:34:43,260] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Created broadcast 1 from broadcast at DAGScheduler.scala:1161
 INFO [2020-02-19 07:34:43,294] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting 1 missing tasks from ResultStage 0 (data/test.txt MapPartitionsRDD[1] at textFile at <console>:36) (first 15 tasks are for partitions Vector(0))
 INFO [2020-02-19 07:34:43,296] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Adding task set 0.0 with 1 tasks
 INFO [2020-02-19 07:34:43,321] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Added task set TaskSet_0.0 tasks to pool default
 INFO [2020-02-19 07:34:43,364] ({dispatcher-event-loop-3} Logging.scala[logInfo]:54) - Starting task 0.0 in stage 0.0 (TID 0, 172.27.0.7, executor 1, partition 0, PROCESS_LOCAL, 7892 bytes)
 INFO [2020-02-19 07:34:44,165] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Added broadcast_1_piece0 in memory on 172.27.0.7:42597 (size: 2.0 KB, free: 912.3 MB)
 INFO [2020-02-19 07:34:44,437] ({dispatcher-event-loop-2} Logging.scala[logInfo]:54) - Added broadcast_0_piece0 in memory on 172.27.0.7:42597 (size: 22.9 KB, free: 912.3 MB)
 WARN [2020-02-19 07:34:44,952] ({task-result-getter-0} Logging.scala[logWarning]:66) - Lost task 0.0 in stage 0.0 (TID 0, 172.27.0.7, executor 1): java.io.FileNotFoundException: File file:/zeppelin/data/test.txt does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:611)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:824)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:601)
	at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:421)
	at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:142)
	at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:346)
	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:769)
	at org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:109)
	at org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.liftedTree1$1(HadoopRDD.scala:267)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:266)
	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:224)
	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:95)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

 INFO [2020-02-19 07:34:44,956] ({dispatcher-event-loop-3} Logging.scala[logInfo]:54) - Starting task 0.1 in stage 0.0 (TID 1, 172.27.0.7, executor 1, partition 0, PROCESS_LOCAL, 7892 bytes)
 INFO [2020-02-19 07:34:45,023] ({task-result-getter-1} Logging.scala[logInfo]:54) - Lost task 0.1 in stage 0.0 (TID 1) on 172.27.0.7, executor 1: java.io.FileNotFoundException (File file:/zeppelin/data/test.txt does not exist) [duplicate 1]
 INFO [2020-02-19 07:34:45,025] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Starting task 0.2 in stage 0.0 (TID 2, 172.27.0.7, executor 1, partition 0, PROCESS_LOCAL, 7892 bytes)
 INFO [2020-02-19 07:34:45,047] ({task-result-getter-2} Logging.scala[logInfo]:54) - Lost task 0.2 in stage 0.0 (TID 2) on 172.27.0.7, executor 1: java.io.FileNotFoundException (File file:/zeppelin/data/test.txt does not exist) [duplicate 2]
 INFO [2020-02-19 07:34:45,054] ({dispatcher-event-loop-3} Logging.scala[logInfo]:54) - Starting task 0.3 in stage 0.0 (TID 3, 172.27.0.6, executor 0, partition 0, PROCESS_LOCAL, 7892 bytes)
 INFO [2020-02-19 07:34:45,571] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Added broadcast_1_piece0 in memory on 172.27.0.6:36245 (size: 2.0 KB, free: 912.3 MB)
 INFO [2020-02-19 07:34:45,824] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Added broadcast_0_piece0 in memory on 172.27.0.6:36245 (size: 22.9 KB, free: 912.3 MB)
 INFO [2020-02-19 07:34:46,163] ({task-result-getter-3} Logging.scala[logInfo]:54) - Lost task 0.3 in stage 0.0 (TID 3) on 172.27.0.6, executor 0: java.io.FileNotFoundException (File file:/zeppelin/data/test.txt does not exist) [duplicate 3]
ERROR [2020-02-19 07:34:46,166] ({task-result-getter-3} Logging.scala[logError]:70) - Task 0 in stage 0.0 failed 4 times; aborting job
 INFO [2020-02-19 07:34:46,172] ({task-result-getter-3} Logging.scala[logInfo]:54) - Removed TaskSet 0.0, whose tasks have all completed, from pool default
 INFO [2020-02-19 07:34:46,175] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Cancelling stage 0
 INFO [2020-02-19 07:34:46,176] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Killing all running tasks in stage 0: Stage cancelled
 INFO [2020-02-19 07:34:46,182] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - ResultStage 0 (first at <console>:37) failed in 2.930 s due to Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 3, 172.27.0.6, executor 0): java.io.FileNotFoundException: File file:/zeppelin/data/test.txt does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:611)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:824)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:601)
	at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:421)
	at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:142)
	at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:346)
	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:769)
	at org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:109)
	at org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.liftedTree1$1(HadoopRDD.scala:267)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:266)
	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:224)
	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:95)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
 INFO [2020-02-19 07:34:46,193] ({pool-2-thread-3} Logging.scala[logInfo]:54) - Job 0 failed: first at <console>:37, took 3.028915 s
 INFO [2020-02-19 07:34:46,564] ({pool-2-thread-3} SchedulerFactory.java[jobFinished]:120) - Job 20200219-065917_1853343063 finished by scheduler interpreter_299988080
 INFO [2020-02-19 07:36:45,091] ({pool-2-thread-4} SchedulerFactory.java[jobStarted]:114) - Job 20200219-065917_1853343063 started by scheduler interpreter_299988080
 INFO [2020-02-19 07:36:45,102] ({pool-2-thread-4} OldSparkInterpreter.java[populateSparkWebUrl]:931) - Sending metadata to Zeppelin server: {message=Spark UI enabled, url=http://zeppelin:4040}
 INFO [2020-02-19 07:36:45,853] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 23
 INFO [2020-02-19 07:36:45,856] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 12
 INFO [2020-02-19 07:36:45,859] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 11
 INFO [2020-02-19 07:36:45,860] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 15
 INFO [2020-02-19 07:36:45,861] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 24
 INFO [2020-02-19 07:36:45,862] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 19
 INFO [2020-02-19 07:36:45,863] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 13
 INFO [2020-02-19 07:36:45,865] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 4
 INFO [2020-02-19 07:36:45,866] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 0
 INFO [2020-02-19 07:36:45,867] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 2
 INFO [2020-02-19 07:36:45,867] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 6
 INFO [2020-02-19 07:36:45,868] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 3
 INFO [2020-02-19 07:36:45,869] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 10
 INFO [2020-02-19 07:36:45,869] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 16
 INFO [2020-02-19 07:36:45,870] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 18
 INFO [2020-02-19 07:36:45,871] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 9
 INFO [2020-02-19 07:36:45,874] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 8
 INFO [2020-02-19 07:36:45,877] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 7
 INFO [2020-02-19 07:36:45,878] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 1
 INFO [2020-02-19 07:36:45,879] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 5
 INFO [2020-02-19 07:36:45,880] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 14
 INFO [2020-02-19 07:36:45,882] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 20
 INFO [2020-02-19 07:36:45,882] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 22
 INFO [2020-02-19 07:36:45,992] ({dispatcher-event-loop-3} Logging.scala[logInfo]:54) - Removed broadcast_1_piece0 on 172.27.0.6:36245 in memory (size: 2.0 KB, free: 912.3 MB)
 INFO [2020-02-19 07:36:46,015] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Removed broadcast_1_piece0 on zeppelin:44945 in memory (size: 2.0 KB, free: 366.3 MB)
 INFO [2020-02-19 07:36:46,319] ({dispatcher-event-loop-2} Logging.scala[logInfo]:54) - Removed broadcast_1_piece0 on 172.27.0.7:42597 in memory (size: 2.0 KB, free: 912.3 MB)
 INFO [2020-02-19 07:36:46,379] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 21
 INFO [2020-02-19 07:36:46,384] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 17
 INFO [2020-02-19 07:36:46,496] ({pool-2-thread-4} SchedulerFactory.java[jobFinished]:120) - Job 20200219-065917_1853343063 finished by scheduler interpreter_299988080
 INFO [2020-02-19 07:37:17,931] ({pool-2-thread-5} SchedulerFactory.java[jobStarted]:114) - Job 20200219-065917_1853343063 started by scheduler interpreter_299988080
 INFO [2020-02-19 07:37:17,936] ({pool-2-thread-5} OldSparkInterpreter.java[populateSparkWebUrl]:931) - Sending metadata to Zeppelin server: {message=Spark UI enabled, url=http://zeppelin:4040}
 INFO [2020-02-19 07:37:18,855] ({pool-2-thread-5} SchedulerFactory.java[jobFinished]:120) - Job 20200219-065917_1853343063 finished by scheduler interpreter_299988080
 INFO [2020-02-19 07:38:12,913] ({pool-2-thread-2} SchedulerFactory.java[jobStarted]:114) - Job 20200219-065917_1853343063 started by scheduler interpreter_299988080
 INFO [2020-02-19 07:38:12,920] ({pool-2-thread-2} OldSparkInterpreter.java[populateSparkWebUrl]:931) - Sending metadata to Zeppelin server: {message=Spark UI enabled, url=http://zeppelin:4040}
 INFO [2020-02-19 07:38:14,145] ({pool-2-thread-2} SchedulerFactory.java[jobFinished]:120) - Job 20200219-065917_1853343063 finished by scheduler interpreter_299988080
 INFO [2020-02-19 07:39:43,930] ({pool-2-thread-6} SchedulerFactory.java[jobStarted]:114) - Job 20200219-065917_1853343063 started by scheduler interpreter_299988080
 INFO [2020-02-19 07:39:43,937] ({pool-2-thread-6} OldSparkInterpreter.java[populateSparkWebUrl]:931) - Sending metadata to Zeppelin server: {message=Spark UI enabled, url=http://zeppelin:4040}
 INFO [2020-02-19 07:39:45,218] ({pool-2-thread-6} SchedulerFactory.java[jobFinished]:120) - Job 20200219-065917_1853343063 finished by scheduler interpreter_299988080
 INFO [2020-02-19 07:40:13,662] ({pool-2-thread-3} SchedulerFactory.java[jobStarted]:114) - Job 20200219-065917_1853343063 started by scheduler interpreter_299988080
 INFO [2020-02-19 07:40:13,666] ({pool-2-thread-3} OldSparkInterpreter.java[populateSparkWebUrl]:931) - Sending metadata to Zeppelin server: {message=Spark UI enabled, url=http://zeppelin:4040}
 INFO [2020-02-19 07:40:14,775] ({pool-2-thread-3} SchedulerFactory.java[jobFinished]:120) - Job 20200219-065917_1853343063 finished by scheduler interpreter_299988080
 INFO [2020-02-19 07:40:27,220] ({pool-2-thread-7} SchedulerFactory.java[jobStarted]:114) - Job 20200219-065917_1853343063 started by scheduler interpreter_299988080
 INFO [2020-02-19 07:40:27,224] ({pool-2-thread-7} OldSparkInterpreter.java[populateSparkWebUrl]:931) - Sending metadata to Zeppelin server: {message=Spark UI enabled, url=http://zeppelin:4040}
 INFO [2020-02-19 07:40:28,536] ({pool-2-thread-7} Logging.scala[logInfo]:54) - Block broadcast_2 stored as values in memory (estimated size 236.7 KB, free 365.8 MB)
 INFO [2020-02-19 07:40:28,563] ({pool-2-thread-7} Logging.scala[logInfo]:54) - Block broadcast_2_piece0 stored as bytes in memory (estimated size 22.9 KB, free 365.8 MB)
 INFO [2020-02-19 07:40:28,565] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Added broadcast_2_piece0 in memory on zeppelin:44945 (size: 22.9 KB, free: 366.3 MB)
 INFO [2020-02-19 07:40:28,569] ({pool-2-thread-7} Logging.scala[logInfo]:54) - Created broadcast 2 from textFile at <console>:54
 INFO [2020-02-19 07:40:28,808] ({pool-2-thread-7} FileInputFormat.java[listStatus]:249) - Total input paths to process : 1
 INFO [2020-02-19 07:40:28,835] ({pool-2-thread-7} Logging.scala[logInfo]:54) - Starting job: first at <console>:55
 INFO [2020-02-19 07:40:28,836] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Got job 1 (first at <console>:55) with 1 output partitions
 INFO [2020-02-19 07:40:28,840] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Final stage: ResultStage 1 (first at <console>:55)
 INFO [2020-02-19 07:40:28,841] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Parents of final stage: List()
 INFO [2020-02-19 07:40:28,842] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Missing parents: List()
 INFO [2020-02-19 07:40:28,846] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting ResultStage 1 (data/test.txt MapPartitionsRDD[3] at textFile at <console>:54), which has no missing parents
 INFO [2020-02-19 07:40:28,848] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_3 stored as values in memory (estimated size 3.4 KB, free 365.8 MB)
 INFO [2020-02-19 07:40:28,850] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.0 KB, free 365.8 MB)
 INFO [2020-02-19 07:40:28,853] ({dispatcher-event-loop-2} Logging.scala[logInfo]:54) - Added broadcast_3_piece0 in memory on zeppelin:44945 (size: 2.0 KB, free: 366.3 MB)
 INFO [2020-02-19 07:40:28,854] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Created broadcast 3 from broadcast at DAGScheduler.scala:1161
 INFO [2020-02-19 07:40:28,855] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting 1 missing tasks from ResultStage 1 (data/test.txt MapPartitionsRDD[3] at textFile at <console>:54) (first 15 tasks are for partitions Vector(0))
 INFO [2020-02-19 07:40:28,857] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Adding task set 1.0 with 1 tasks
 INFO [2020-02-19 07:40:28,859] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Added task set TaskSet_1.0 tasks to pool default
 INFO [2020-02-19 07:40:28,860] ({dispatcher-event-loop-3} Logging.scala[logInfo]:54) - Starting task 0.0 in stage 1.0 (TID 4, 172.27.0.6, executor 0, partition 0, PROCESS_LOCAL, 7892 bytes)
 INFO [2020-02-19 07:40:28,926] ({dispatcher-event-loop-2} Logging.scala[logInfo]:54) - Added broadcast_3_piece0 in memory on 172.27.0.6:36245 (size: 2.0 KB, free: 912.3 MB)
 INFO [2020-02-19 07:40:28,946] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Added broadcast_2_piece0 in memory on 172.27.0.6:36245 (size: 22.9 KB, free: 912.3 MB)
 WARN [2020-02-19 07:40:28,987] ({task-result-getter-0} Logging.scala[logWarning]:66) - Lost task 0.0 in stage 1.0 (TID 4, 172.27.0.6, executor 0): java.io.FileNotFoundException: File file:/zeppelin/data/test.txt does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:611)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:824)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:601)
	at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:421)
	at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:142)
	at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:346)
	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:769)
	at org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:109)
	at org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.liftedTree1$1(HadoopRDD.scala:267)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:266)
	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:224)
	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:95)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

 INFO [2020-02-19 07:40:28,988] ({dispatcher-event-loop-2} Logging.scala[logInfo]:54) - Starting task 0.1 in stage 1.0 (TID 5, 172.27.0.7, executor 1, partition 0, PROCESS_LOCAL, 7892 bytes)
 INFO [2020-02-19 07:40:29,052] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Added broadcast_3_piece0 in memory on 172.27.0.7:42597 (size: 2.0 KB, free: 912.3 MB)
 INFO [2020-02-19 07:40:29,071] ({dispatcher-event-loop-3} Logging.scala[logInfo]:54) - Added broadcast_2_piece0 in memory on 172.27.0.7:42597 (size: 22.9 KB, free: 912.3 MB)
 INFO [2020-02-19 07:40:29,118] ({task-result-getter-1} Logging.scala[logInfo]:54) - Lost task 0.1 in stage 1.0 (TID 5) on 172.27.0.7, executor 1: java.io.FileNotFoundException (File file:/zeppelin/data/test.txt does not exist) [duplicate 1]
 INFO [2020-02-19 07:40:29,119] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Starting task 0.2 in stage 1.0 (TID 6, 172.27.0.7, executor 1, partition 0, PROCESS_LOCAL, 7892 bytes)
 INFO [2020-02-19 07:40:29,138] ({task-result-getter-2} Logging.scala[logInfo]:54) - Lost task 0.2 in stage 1.0 (TID 6) on 172.27.0.7, executor 1: java.io.FileNotFoundException (File file:/zeppelin/data/test.txt does not exist) [duplicate 2]
 INFO [2020-02-19 07:40:29,139] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Starting task 0.3 in stage 1.0 (TID 7, 172.27.0.7, executor 1, partition 0, PROCESS_LOCAL, 7892 bytes)
 INFO [2020-02-19 07:40:29,160] ({task-result-getter-3} Logging.scala[logInfo]:54) - Lost task 0.3 in stage 1.0 (TID 7) on 172.27.0.7, executor 1: java.io.FileNotFoundException (File file:/zeppelin/data/test.txt does not exist) [duplicate 3]
ERROR [2020-02-19 07:40:29,161] ({task-result-getter-3} Logging.scala[logError]:70) - Task 0 in stage 1.0 failed 4 times; aborting job
 INFO [2020-02-19 07:40:29,162] ({task-result-getter-3} Logging.scala[logInfo]:54) - Removed TaskSet 1.0, whose tasks have all completed, from pool default
 INFO [2020-02-19 07:40:29,166] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Cancelling stage 1
 INFO [2020-02-19 07:40:29,167] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Killing all running tasks in stage 1: Stage cancelled
 INFO [2020-02-19 07:40:29,168] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - ResultStage 1 (first at <console>:55) failed in 0.321 s due to Job aborted due to stage failure: Task 0 in stage 1.0 failed 4 times, most recent failure: Lost task 0.3 in stage 1.0 (TID 7, 172.27.0.7, executor 1): java.io.FileNotFoundException: File file:/zeppelin/data/test.txt does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:611)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:824)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:601)
	at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:421)
	at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:142)
	at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:346)
	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:769)
	at org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:109)
	at org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.liftedTree1$1(HadoopRDD.scala:267)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:266)
	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:224)
	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:95)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
 INFO [2020-02-19 07:40:29,169] ({pool-2-thread-7} Logging.scala[logInfo]:54) - Job 1 failed: first at <console>:55, took 0.333438 s
 INFO [2020-02-19 07:40:29,405] ({pool-2-thread-7} SchedulerFactory.java[jobFinished]:120) - Job 20200219-065917_1853343063 finished by scheduler interpreter_299988080
 INFO [2020-02-19 07:40:49,296] ({pool-2-thread-4} SchedulerFactory.java[jobStarted]:114) - Job 20200219-065917_1853343063 started by scheduler interpreter_299988080
 INFO [2020-02-19 07:40:49,300] ({pool-2-thread-4} OldSparkInterpreter.java[populateSparkWebUrl]:931) - Sending metadata to Zeppelin server: {message=Spark UI enabled, url=http://zeppelin:4040}
 INFO [2020-02-19 07:40:49,484] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 38
 INFO [2020-02-19 07:40:49,485] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 49
 INFO [2020-02-19 07:40:49,488] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 35
 INFO [2020-02-19 07:40:49,490] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 34
 INFO [2020-02-19 07:40:49,494] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 41
 INFO [2020-02-19 07:40:49,495] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 30
 INFO [2020-02-19 07:40:49,496] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 25
 INFO [2020-02-19 07:40:49,497] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 44
 INFO [2020-02-19 07:40:49,497] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 46
 INFO [2020-02-19 07:40:49,512] ({dispatcher-event-loop-2} Logging.scala[logInfo]:54) - Removed broadcast_3_piece0 on 172.27.0.7:42597 in memory (size: 2.0 KB, free: 912.3 MB)
 INFO [2020-02-19 07:40:49,519] ({dispatcher-event-loop-2} Logging.scala[logInfo]:54) - Removed broadcast_3_piece0 on zeppelin:44945 in memory (size: 2.0 KB, free: 366.3 MB)
 INFO [2020-02-19 07:40:49,520] ({dispatcher-event-loop-2} Logging.scala[logInfo]:54) - Removed broadcast_3_piece0 on 172.27.0.6:36245 in memory (size: 2.0 KB, free: 912.3 MB)
 INFO [2020-02-19 07:40:49,541] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 26
 INFO [2020-02-19 07:40:49,544] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 28
 INFO [2020-02-19 07:40:49,545] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 47
 INFO [2020-02-19 07:40:49,546] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 29
 INFO [2020-02-19 07:40:49,548] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 43
 INFO [2020-02-19 07:40:49,551] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 31
 INFO [2020-02-19 07:40:49,553] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 40
 INFO [2020-02-19 07:40:49,555] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 27
 INFO [2020-02-19 07:40:49,556] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 32
 INFO [2020-02-19 07:40:49,557] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 37
 INFO [2020-02-19 07:40:49,558] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 36
 INFO [2020-02-19 07:40:49,561] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 48
 INFO [2020-02-19 07:40:49,562] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 33
 INFO [2020-02-19 07:40:49,565] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 45
 INFO [2020-02-19 07:40:49,567] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 39
 INFO [2020-02-19 07:40:49,568] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 42
 INFO [2020-02-19 07:40:50,621] ({pool-2-thread-4} Logging.scala[logInfo]:54) - Block broadcast_4 stored as values in memory (estimated size 236.7 KB, free 365.6 MB)
 INFO [2020-02-19 07:40:50,634] ({pool-2-thread-4} Logging.scala[logInfo]:54) - Block broadcast_4_piece0 stored as bytes in memory (estimated size 22.9 KB, free 365.5 MB)
 INFO [2020-02-19 07:40:50,635] ({dispatcher-event-loop-3} Logging.scala[logInfo]:54) - Added broadcast_4_piece0 in memory on zeppelin:44945 (size: 22.9 KB, free: 366.2 MB)
 INFO [2020-02-19 07:40:50,637] ({pool-2-thread-4} Logging.scala[logInfo]:54) - Created broadcast 4 from textFile at <console>:57
 INFO [2020-02-19 07:40:51,436] ({pool-2-thread-4} SchedulerFactory.java[jobFinished]:120) - Job 20200219-065917_1853343063 finished by scheduler interpreter_299988080
 INFO [2020-02-19 07:41:21,578] ({pool-2-thread-8} SchedulerFactory.java[jobStarted]:114) - Job 20200219-065917_1853343063 started by scheduler interpreter_299988080
 INFO [2020-02-19 07:41:21,582] ({pool-2-thread-8} OldSparkInterpreter.java[populateSparkWebUrl]:931) - Sending metadata to Zeppelin server: {message=Spark UI enabled, url=http://zeppelin:4040}
 INFO [2020-02-19 07:41:22,848] ({pool-2-thread-8} Logging.scala[logInfo]:54) - Block broadcast_5 stored as values in memory (estimated size 236.7 KB, free 365.3 MB)
 INFO [2020-02-19 07:41:22,860] ({pool-2-thread-8} Logging.scala[logInfo]:54) - Block broadcast_5_piece0 stored as bytes in memory (estimated size 22.9 KB, free 365.3 MB)
 INFO [2020-02-19 07:41:22,863] ({dispatcher-event-loop-3} Logging.scala[logInfo]:54) - Added broadcast_5_piece0 in memory on zeppelin:44945 (size: 22.9 KB, free: 366.2 MB)
 INFO [2020-02-19 07:41:22,866] ({pool-2-thread-8} Logging.scala[logInfo]:54) - Created broadcast 5 from textFile at <console>:60
 INFO [2020-02-19 07:41:23,106] ({pool-2-thread-8} FileInputFormat.java[listStatus]:249) - Total input paths to process : 1
 INFO [2020-02-19 07:41:23,118] ({pool-2-thread-8} Logging.scala[logInfo]:54) - Starting job: first at <console>:61
 INFO [2020-02-19 07:41:23,120] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Got job 2 (first at <console>:61) with 1 output partitions
 INFO [2020-02-19 07:41:23,121] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Final stage: ResultStage 2 (first at <console>:61)
 INFO [2020-02-19 07:41:23,121] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Parents of final stage: List()
 INFO [2020-02-19 07:41:23,122] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Missing parents: List()
 INFO [2020-02-19 07:41:23,125] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting ResultStage 2 (/zeppelin/data/test.txt MapPartitionsRDD[7] at textFile at <console>:60), which has no missing parents
 INFO [2020-02-19 07:41:23,128] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_6 stored as values in memory (estimated size 3.4 KB, free 365.3 MB)
 INFO [2020-02-19 07:41:23,130] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_6_piece0 stored as bytes in memory (estimated size 2.0 KB, free 365.3 MB)
 INFO [2020-02-19 07:41:23,134] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Added broadcast_6_piece0 in memory on zeppelin:44945 (size: 2.0 KB, free: 366.2 MB)
 INFO [2020-02-19 07:41:23,136] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Created broadcast 6 from broadcast at DAGScheduler.scala:1161
 INFO [2020-02-19 07:41:23,137] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting 1 missing tasks from ResultStage 2 (/zeppelin/data/test.txt MapPartitionsRDD[7] at textFile at <console>:60) (first 15 tasks are for partitions Vector(0))
 INFO [2020-02-19 07:41:23,138] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Adding task set 2.0 with 1 tasks
 INFO [2020-02-19 07:41:23,139] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Added task set TaskSet_2.0 tasks to pool default
 INFO [2020-02-19 07:41:23,141] ({dispatcher-event-loop-2} Logging.scala[logInfo]:54) - Starting task 0.0 in stage 2.0 (TID 8, 172.27.0.7, executor 1, partition 0, PROCESS_LOCAL, 7892 bytes)
 INFO [2020-02-19 07:41:23,161] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Added broadcast_6_piece0 in memory on 172.27.0.7:42597 (size: 2.0 KB, free: 912.3 MB)
 INFO [2020-02-19 07:41:23,180] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Added broadcast_5_piece0 in memory on 172.27.0.7:42597 (size: 22.9 KB, free: 912.2 MB)
 WARN [2020-02-19 07:41:23,212] ({task-result-getter-0} Logging.scala[logWarning]:66) - Lost task 0.0 in stage 2.0 (TID 8, 172.27.0.7, executor 1): java.io.FileNotFoundException: File file:/zeppelin/data/test.txt does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:611)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:824)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:601)
	at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:421)
	at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:142)
	at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:346)
	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:769)
	at org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:109)
	at org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.liftedTree1$1(HadoopRDD.scala:267)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:266)
	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:224)
	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:95)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

 INFO [2020-02-19 07:41:23,213] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Starting task 0.1 in stage 2.0 (TID 9, 172.27.0.7, executor 1, partition 0, PROCESS_LOCAL, 7892 bytes)
 INFO [2020-02-19 07:41:23,232] ({task-result-getter-1} Logging.scala[logInfo]:54) - Lost task 0.1 in stage 2.0 (TID 9) on 172.27.0.7, executor 1: java.io.FileNotFoundException (File file:/zeppelin/data/test.txt does not exist) [duplicate 1]
 INFO [2020-02-19 07:41:23,234] ({dispatcher-event-loop-3} Logging.scala[logInfo]:54) - Starting task 0.2 in stage 2.0 (TID 10, 172.27.0.7, executor 1, partition 0, PROCESS_LOCAL, 7892 bytes)
 INFO [2020-02-19 07:41:23,251] ({task-result-getter-2} Logging.scala[logInfo]:54) - Lost task 0.2 in stage 2.0 (TID 10) on 172.27.0.7, executor 1: java.io.FileNotFoundException (File file:/zeppelin/data/test.txt does not exist) [duplicate 2]
 INFO [2020-02-19 07:41:23,252] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Starting task 0.3 in stage 2.0 (TID 11, 172.27.0.7, executor 1, partition 0, PROCESS_LOCAL, 7892 bytes)
 INFO [2020-02-19 07:41:23,267] ({task-result-getter-3} Logging.scala[logInfo]:54) - Lost task 0.3 in stage 2.0 (TID 11) on 172.27.0.7, executor 1: java.io.FileNotFoundException (File file:/zeppelin/data/test.txt does not exist) [duplicate 3]
ERROR [2020-02-19 07:41:23,268] ({task-result-getter-3} Logging.scala[logError]:70) - Task 0 in stage 2.0 failed 4 times; aborting job
 INFO [2020-02-19 07:41:23,269] ({task-result-getter-3} Logging.scala[logInfo]:54) - Removed TaskSet 2.0, whose tasks have all completed, from pool default
 INFO [2020-02-19 07:41:23,271] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Cancelling stage 2
 INFO [2020-02-19 07:41:23,272] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Killing all running tasks in stage 2: Stage cancelled
 INFO [2020-02-19 07:41:23,273] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - ResultStage 2 (first at <console>:61) failed in 0.147 s due to Job aborted due to stage failure: Task 0 in stage 2.0 failed 4 times, most recent failure: Lost task 0.3 in stage 2.0 (TID 11, 172.27.0.7, executor 1): java.io.FileNotFoundException: File file:/zeppelin/data/test.txt does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:611)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:824)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:601)
	at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:421)
	at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:142)
	at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:346)
	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:769)
	at org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:109)
	at org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.liftedTree1$1(HadoopRDD.scala:267)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:266)
	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:224)
	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:95)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
 INFO [2020-02-19 07:41:23,274] ({pool-2-thread-8} Logging.scala[logInfo]:54) - Job 2 failed: first at <console>:61, took 0.155010 s
 INFO [2020-02-19 07:41:23,530] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 52
 INFO [2020-02-19 07:41:23,534] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 62
 INFO [2020-02-19 07:41:23,535] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 63
 INFO [2020-02-19 07:41:23,536] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 50
 INFO [2020-02-19 07:41:23,537] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 61
 INFO [2020-02-19 07:41:23,542] ({pool-2-thread-8} SchedulerFactory.java[jobFinished]:120) - Job 20200219-065917_1853343063 finished by scheduler interpreter_299988080
 INFO [2020-02-19 07:41:23,545] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Removed broadcast_6_piece0 on zeppelin:44945 in memory (size: 2.0 KB, free: 366.2 MB)
 INFO [2020-02-19 07:41:23,562] ({dispatcher-event-loop-2} Logging.scala[logInfo]:54) - Removed broadcast_6_piece0 on 172.27.0.7:42597 in memory (size: 2.0 KB, free: 912.2 MB)
 INFO [2020-02-19 07:41:23,620] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 68
 INFO [2020-02-19 07:41:23,621] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 51
 INFO [2020-02-19 07:41:23,624] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 72
 INFO [2020-02-19 07:41:23,625] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 55
 INFO [2020-02-19 07:41:23,625] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 64
 INFO [2020-02-19 07:41:23,626] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 60
 INFO [2020-02-19 07:41:23,627] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 54
 INFO [2020-02-19 07:41:23,628] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 58
 INFO [2020-02-19 07:41:23,629] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 73
 INFO [2020-02-19 07:41:23,630] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 56
 INFO [2020-02-19 07:41:23,632] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 53
 INFO [2020-02-19 07:41:23,633] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 69
 INFO [2020-02-19 07:41:23,635] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 59
 INFO [2020-02-19 07:41:23,638] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 57
 INFO [2020-02-19 07:41:23,640] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 71
 INFO [2020-02-19 07:41:23,641] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 70
 INFO [2020-02-19 07:41:23,643] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 74
 INFO [2020-02-19 07:41:23,644] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 65
 INFO [2020-02-19 07:41:23,645] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 66
 INFO [2020-02-19 07:41:23,646] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 67
 INFO [2020-02-19 07:41:36,372] ({pool-2-thread-5} SchedulerFactory.java[jobStarted]:114) - Job 20200219-065917_1853343063 started by scheduler interpreter_299988080
 INFO [2020-02-19 07:41:36,385] ({pool-2-thread-5} OldSparkInterpreter.java[populateSparkWebUrl]:931) - Sending metadata to Zeppelin server: {message=Spark UI enabled, url=http://zeppelin:4040}
 INFO [2020-02-19 07:41:37,620] ({pool-2-thread-5} Logging.scala[logInfo]:54) - Block broadcast_7 stored as values in memory (estimated size 236.7 KB, free 365.1 MB)
 INFO [2020-02-19 07:41:37,635] ({pool-2-thread-5} Logging.scala[logInfo]:54) - Block broadcast_7_piece0 stored as bytes in memory (estimated size 22.9 KB, free 365.0 MB)
 INFO [2020-02-19 07:41:37,638] ({dispatcher-event-loop-3} Logging.scala[logInfo]:54) - Added broadcast_7_piece0 in memory on zeppelin:44945 (size: 22.9 KB, free: 366.2 MB)
 INFO [2020-02-19 07:41:37,640] ({pool-2-thread-5} Logging.scala[logInfo]:54) - Created broadcast 7 from textFile at <console>:63
 INFO [2020-02-19 07:41:37,863] ({pool-2-thread-5} FileInputFormat.java[listStatus]:249) - Total input paths to process : 1
 INFO [2020-02-19 07:41:37,872] ({pool-2-thread-5} Logging.scala[logInfo]:54) - Starting job: first at <console>:64
 INFO [2020-02-19 07:41:37,873] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Got job 3 (first at <console>:64) with 1 output partitions
 INFO [2020-02-19 07:41:37,874] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Final stage: ResultStage 3 (first at <console>:64)
 INFO [2020-02-19 07:41:37,874] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Parents of final stage: List()
 INFO [2020-02-19 07:41:37,875] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Missing parents: List()
 INFO [2020-02-19 07:41:37,876] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting ResultStage 3 (///zeppelin/data/test.txt MapPartitionsRDD[9] at textFile at <console>:63), which has no missing parents
 INFO [2020-02-19 07:41:37,880] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_8 stored as values in memory (estimated size 3.5 KB, free 365.0 MB)
 INFO [2020-02-19 07:41:37,883] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_8_piece0 stored as bytes in memory (estimated size 2.0 KB, free 365.0 MB)
 INFO [2020-02-19 07:41:37,885] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Added broadcast_8_piece0 in memory on zeppelin:44945 (size: 2.0 KB, free: 366.2 MB)
 INFO [2020-02-19 07:41:37,886] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Created broadcast 8 from broadcast at DAGScheduler.scala:1161
 INFO [2020-02-19 07:41:37,887] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting 1 missing tasks from ResultStage 3 (///zeppelin/data/test.txt MapPartitionsRDD[9] at textFile at <console>:63) (first 15 tasks are for partitions Vector(0))
 INFO [2020-02-19 07:41:37,888] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Adding task set 3.0 with 1 tasks
 INFO [2020-02-19 07:41:37,889] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Added task set TaskSet_3.0 tasks to pool default
 INFO [2020-02-19 07:41:37,890] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Starting task 0.0 in stage 3.0 (TID 12, 172.27.0.6, executor 0, partition 0, PROCESS_LOCAL, 7892 bytes)
 INFO [2020-02-19 07:41:37,917] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Added broadcast_8_piece0 in memory on 172.27.0.6:36245 (size: 2.0 KB, free: 912.3 MB)
 INFO [2020-02-19 07:41:37,937] ({dispatcher-event-loop-2} Logging.scala[logInfo]:54) - Added broadcast_7_piece0 in memory on 172.27.0.6:36245 (size: 22.9 KB, free: 912.2 MB)
 WARN [2020-02-19 07:41:37,978] ({task-result-getter-0} Logging.scala[logWarning]:66) - Lost task 0.0 in stage 3.0 (TID 12, 172.27.0.6, executor 0): java.io.FileNotFoundException: File file:/zeppelin/data/test.txt does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:611)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:824)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:601)
	at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:421)
	at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:142)
	at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:346)
	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:769)
	at org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:109)
	at org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.liftedTree1$1(HadoopRDD.scala:267)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:266)
	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:224)
	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:95)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

 INFO [2020-02-19 07:41:37,979] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Starting task 0.1 in stage 3.0 (TID 13, 172.27.0.6, executor 0, partition 0, PROCESS_LOCAL, 7892 bytes)
 INFO [2020-02-19 07:41:38,014] ({task-result-getter-1} Logging.scala[logInfo]:54) - Lost task 0.1 in stage 3.0 (TID 13) on 172.27.0.6, executor 0: java.io.FileNotFoundException (File file:/zeppelin/data/test.txt does not exist) [duplicate 1]
 INFO [2020-02-19 07:41:38,020] ({dispatcher-event-loop-3} Logging.scala[logInfo]:54) - Starting task 0.2 in stage 3.0 (TID 14, 172.27.0.7, executor 1, partition 0, PROCESS_LOCAL, 7892 bytes)
 INFO [2020-02-19 07:41:38,048] ({dispatcher-event-loop-2} Logging.scala[logInfo]:54) - Added broadcast_8_piece0 in memory on 172.27.0.7:42597 (size: 2.0 KB, free: 912.2 MB)
 INFO [2020-02-19 07:41:38,069] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Added broadcast_7_piece0 in memory on 172.27.0.7:42597 (size: 22.9 KB, free: 912.2 MB)
 INFO [2020-02-19 07:41:38,100] ({task-result-getter-2} Logging.scala[logInfo]:54) - Lost task 0.2 in stage 3.0 (TID 14) on 172.27.0.7, executor 1: java.io.FileNotFoundException (File file:/zeppelin/data/test.txt does not exist) [duplicate 2]
 INFO [2020-02-19 07:41:38,102] ({dispatcher-event-loop-2} Logging.scala[logInfo]:54) - Starting task 0.3 in stage 3.0 (TID 15, 172.27.0.6, executor 0, partition 0, PROCESS_LOCAL, 7892 bytes)
 INFO [2020-02-19 07:41:38,128] ({task-result-getter-3} Logging.scala[logInfo]:54) - Lost task 0.3 in stage 3.0 (TID 15) on 172.27.0.6, executor 0: java.io.FileNotFoundException (File file:/zeppelin/data/test.txt does not exist) [duplicate 3]
ERROR [2020-02-19 07:41:38,131] ({task-result-getter-3} Logging.scala[logError]:70) - Task 0 in stage 3.0 failed 4 times; aborting job
 INFO [2020-02-19 07:41:38,134] ({task-result-getter-3} Logging.scala[logInfo]:54) - Removed TaskSet 3.0, whose tasks have all completed, from pool default
 INFO [2020-02-19 07:41:38,135] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Cancelling stage 3
 INFO [2020-02-19 07:41:38,136] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Killing all running tasks in stage 3: Stage cancelled
 INFO [2020-02-19 07:41:38,137] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - ResultStage 3 (first at <console>:64) failed in 0.257 s due to Job aborted due to stage failure: Task 0 in stage 3.0 failed 4 times, most recent failure: Lost task 0.3 in stage 3.0 (TID 15, 172.27.0.6, executor 0): java.io.FileNotFoundException: File file:/zeppelin/data/test.txt does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:611)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:824)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:601)
	at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:421)
	at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:142)
	at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:346)
	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:769)
	at org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:109)
	at org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.liftedTree1$1(HadoopRDD.scala:267)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:266)
	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:224)
	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:95)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
 INFO [2020-02-19 07:41:38,138] ({pool-2-thread-5} Logging.scala[logInfo]:54) - Job 3 failed: first at <console>:64, took 0.265063 s
 INFO [2020-02-19 07:41:38,340] ({pool-2-thread-5} SchedulerFactory.java[jobFinished]:120) - Job 20200219-065917_1853343063 finished by scheduler interpreter_299988080
 INFO [2020-02-19 07:43:03,987] ({pool-2-thread-9} SchedulerFactory.java[jobStarted]:114) - Job 20200219-065917_1853343063 started by scheduler interpreter_299988080
 INFO [2020-02-19 07:43:03,998] ({pool-2-thread-9} OldSparkInterpreter.java[populateSparkWebUrl]:931) - Sending metadata to Zeppelin server: {message=Spark UI enabled, url=http://zeppelin:4040}
 INFO [2020-02-19 07:43:04,295] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 80
 INFO [2020-02-19 07:43:04,297] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 98
 INFO [2020-02-19 07:43:04,303] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 78
 INFO [2020-02-19 07:43:04,303] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 91
 INFO [2020-02-19 07:43:04,304] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 95
 INFO [2020-02-19 07:43:04,305] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 82
 INFO [2020-02-19 07:43:04,305] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 94
 INFO [2020-02-19 07:43:04,307] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 90
 INFO [2020-02-19 07:43:04,308] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 99
 INFO [2020-02-19 07:43:04,309] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 83
 INFO [2020-02-19 07:43:04,310] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 75
 INFO [2020-02-19 07:43:04,311] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 77
 INFO [2020-02-19 07:43:04,312] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 97
 INFO [2020-02-19 07:43:04,312] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 85
 INFO [2020-02-19 07:43:04,313] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 84
 INFO [2020-02-19 07:43:04,314] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 93
 INFO [2020-02-19 07:43:04,314] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 79
 INFO [2020-02-19 07:43:04,317] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 86
 INFO [2020-02-19 07:43:04,321] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 96
 INFO [2020-02-19 07:43:04,344] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 89
 INFO [2020-02-19 07:43:04,346] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 76
 INFO [2020-02-19 07:43:04,347] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 87
 INFO [2020-02-19 07:43:04,347] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 92
 INFO [2020-02-19 07:43:04,381] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Removed broadcast_8_piece0 on 172.27.0.7:42597 in memory (size: 2.0 KB, free: 912.2 MB)
 INFO [2020-02-19 07:43:04,392] ({dispatcher-event-loop-2} Logging.scala[logInfo]:54) - Removed broadcast_8_piece0 on 172.27.0.6:36245 in memory (size: 2.0 KB, free: 912.2 MB)
 INFO [2020-02-19 07:43:04,394] ({dispatcher-event-loop-2} Logging.scala[logInfo]:54) - Removed broadcast_8_piece0 on zeppelin:44945 in memory (size: 2.0 KB, free: 366.2 MB)
 INFO [2020-02-19 07:43:04,488] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 88
 INFO [2020-02-19 07:43:04,489] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 81
 INFO [2020-02-19 07:43:05,542] ({pool-2-thread-9} Logging.scala[logInfo]:54) - Block broadcast_9 stored as values in memory (estimated size 236.7 KB, free 364.8 MB)
 INFO [2020-02-19 07:43:05,555] ({pool-2-thread-9} Logging.scala[logInfo]:54) - Block broadcast_9_piece0 stored as bytes in memory (estimated size 22.9 KB, free 364.8 MB)
 INFO [2020-02-19 07:43:05,557] ({dispatcher-event-loop-2} Logging.scala[logInfo]:54) - Added broadcast_9_piece0 in memory on zeppelin:44945 (size: 22.9 KB, free: 366.2 MB)
 INFO [2020-02-19 07:43:05,559] ({pool-2-thread-9} Logging.scala[logInfo]:54) - Created broadcast 9 from textFile at <console>:66
 INFO [2020-02-19 07:43:05,999] ({pool-2-thread-9} SchedulerFactory.java[jobFinished]:120) - Job 20200219-065917_1853343063 finished by scheduler interpreter_299988080
 INFO [2020-02-19 07:43:28,013] ({pool-2-thread-2} SchedulerFactory.java[jobStarted]:114) - Job 20200219-065917_1853343063 started by scheduler interpreter_299988080
 INFO [2020-02-19 07:43:28,026] ({pool-2-thread-2} OldSparkInterpreter.java[populateSparkWebUrl]:931) - Sending metadata to Zeppelin server: {message=Spark UI enabled, url=http://zeppelin:4040}
 INFO [2020-02-19 07:43:29,181] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Block broadcast_10 stored as values in memory (estimated size 236.7 KB, free 364.5 MB)
 INFO [2020-02-19 07:43:29,191] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Block broadcast_10_piece0 stored as bytes in memory (estimated size 22.9 KB, free 364.5 MB)
 INFO [2020-02-19 07:43:29,192] ({dispatcher-event-loop-3} Logging.scala[logInfo]:54) - Added broadcast_10_piece0 in memory on zeppelin:44945 (size: 22.9 KB, free: 366.1 MB)
 INFO [2020-02-19 07:43:29,193] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Created broadcast 10 from textFile at <console>:69
 INFO [2020-02-19 07:43:29,416] ({pool-2-thread-2} FileInputFormat.java[listStatus]:249) - Total input paths to process : 1
 INFO [2020-02-19 07:43:29,425] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Starting job: first at <console>:70
 INFO [2020-02-19 07:43:29,428] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Got job 4 (first at <console>:70) with 1 output partitions
 INFO [2020-02-19 07:43:29,429] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Final stage: ResultStage 4 (first at <console>:70)
 INFO [2020-02-19 07:43:29,429] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Parents of final stage: List()
 INFO [2020-02-19 07:43:29,430] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Missing parents: List()
 INFO [2020-02-19 07:43:29,431] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting ResultStage 4 (data/test.txt MapPartitionsRDD[13] at textFile at <console>:69), which has no missing parents
 INFO [2020-02-19 07:43:29,434] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_11 stored as values in memory (estimated size 3.4 KB, free 364.5 MB)
 INFO [2020-02-19 07:43:29,437] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_11_piece0 stored as bytes in memory (estimated size 2.0 KB, free 364.5 MB)
 INFO [2020-02-19 07:43:29,438] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Added broadcast_11_piece0 in memory on zeppelin:44945 (size: 2.0 KB, free: 366.1 MB)
 INFO [2020-02-19 07:43:29,439] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Created broadcast 11 from broadcast at DAGScheduler.scala:1161
 INFO [2020-02-19 07:43:29,441] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting 1 missing tasks from ResultStage 4 (data/test.txt MapPartitionsRDD[13] at textFile at <console>:69) (first 15 tasks are for partitions Vector(0))
 INFO [2020-02-19 07:43:29,443] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Adding task set 4.0 with 1 tasks
 INFO [2020-02-19 07:43:29,445] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Added task set TaskSet_4.0 tasks to pool default
 INFO [2020-02-19 07:43:29,447] ({dispatcher-event-loop-2} Logging.scala[logInfo]:54) - Starting task 0.0 in stage 4.0 (TID 16, 172.27.0.6, executor 0, partition 0, PROCESS_LOCAL, 7892 bytes)
 INFO [2020-02-19 07:43:29,471] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Added broadcast_11_piece0 in memory on 172.27.0.6:36245 (size: 2.0 KB, free: 912.2 MB)
 INFO [2020-02-19 07:43:29,518] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Added broadcast_10_piece0 in memory on 172.27.0.6:36245 (size: 22.9 KB, free: 912.2 MB)
 WARN [2020-02-19 07:43:29,600] ({task-result-getter-0} Logging.scala[logWarning]:66) - Lost task 0.0 in stage 4.0 (TID 16, 172.27.0.6, executor 0): java.io.FileNotFoundException: File file:/zeppelin/data/test.txt does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:611)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:824)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:601)
	at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:421)
	at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:142)
	at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:346)
	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:769)
	at org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:109)
	at org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.liftedTree1$1(HadoopRDD.scala:267)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:266)
	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:224)
	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:95)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

 INFO [2020-02-19 07:43:29,601] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Starting task 0.1 in stage 4.0 (TID 17, 172.27.0.6, executor 0, partition 0, PROCESS_LOCAL, 7892 bytes)
 INFO [2020-02-19 07:43:29,632] ({task-result-getter-1} Logging.scala[logInfo]:54) - Lost task 0.1 in stage 4.0 (TID 17) on 172.27.0.6, executor 0: java.io.FileNotFoundException (File file:/zeppelin/data/test.txt does not exist) [duplicate 1]
 INFO [2020-02-19 07:43:29,637] ({dispatcher-event-loop-3} Logging.scala[logInfo]:54) - Starting task 0.2 in stage 4.0 (TID 18, 172.27.0.6, executor 0, partition 0, PROCESS_LOCAL, 7892 bytes)
 INFO [2020-02-19 07:43:29,675] ({task-result-getter-2} Logging.scala[logInfo]:54) - Lost task 0.2 in stage 4.0 (TID 18) on 172.27.0.6, executor 0: java.io.FileNotFoundException (File file:/zeppelin/data/test.txt does not exist) [duplicate 2]
 INFO [2020-02-19 07:43:29,679] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Starting task 0.3 in stage 4.0 (TID 19, 172.27.0.7, executor 1, partition 0, PROCESS_LOCAL, 7892 bytes)
 INFO [2020-02-19 07:43:29,705] ({dispatcher-event-loop-2} Logging.scala[logInfo]:54) - Added broadcast_11_piece0 in memory on 172.27.0.7:42597 (size: 2.0 KB, free: 912.2 MB)
 INFO [2020-02-19 07:43:29,745] ({dispatcher-event-loop-3} Logging.scala[logInfo]:54) - Added broadcast_10_piece0 in memory on 172.27.0.7:42597 (size: 22.9 KB, free: 912.2 MB)
 INFO [2020-02-19 07:43:29,780] ({task-result-getter-3} Logging.scala[logInfo]:54) - Lost task 0.3 in stage 4.0 (TID 19) on 172.27.0.7, executor 1: java.io.FileNotFoundException (File file:/zeppelin/data/test.txt does not exist) [duplicate 3]
ERROR [2020-02-19 07:43:29,781] ({task-result-getter-3} Logging.scala[logError]:70) - Task 0 in stage 4.0 failed 4 times; aborting job
 INFO [2020-02-19 07:43:29,782] ({task-result-getter-3} Logging.scala[logInfo]:54) - Removed TaskSet 4.0, whose tasks have all completed, from pool default
 INFO [2020-02-19 07:43:29,785] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Cancelling stage 4
 INFO [2020-02-19 07:43:29,789] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Killing all running tasks in stage 4: Stage cancelled
 INFO [2020-02-19 07:43:29,790] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - ResultStage 4 (first at <console>:70) failed in 0.356 s due to Job aborted due to stage failure: Task 0 in stage 4.0 failed 4 times, most recent failure: Lost task 0.3 in stage 4.0 (TID 19, 172.27.0.7, executor 1): java.io.FileNotFoundException: File file:/zeppelin/data/test.txt does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:611)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:824)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:601)
	at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:421)
	at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:142)
	at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:346)
	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:769)
	at org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:109)
	at org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.liftedTree1$1(HadoopRDD.scala:267)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:266)
	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:224)
	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:95)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
 INFO [2020-02-19 07:43:29,791] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Job 4 failed: first at <console>:70, took 0.363292 s
 INFO [2020-02-19 07:43:30,012] ({pool-2-thread-2} SchedulerFactory.java[jobFinished]:120) - Job 20200219-065917_1853343063 finished by scheduler interpreter_299988080
 INFO [2020-02-19 07:44:01,855] ({pool-2-thread-10} SchedulerFactory.java[jobStarted]:114) - Job 20200219-065917_1853343063 started by scheduler interpreter_299988080
 INFO [2020-02-19 07:44:01,861] ({pool-2-thread-10} OldSparkInterpreter.java[populateSparkWebUrl]:931) - Sending metadata to Zeppelin server: {message=Spark UI enabled, url=http://zeppelin:4040}
 INFO [2020-02-19 07:44:02,261] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 113
 INFO [2020-02-19 07:44:02,261] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 121
 INFO [2020-02-19 07:44:02,262] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 112
 INFO [2020-02-19 07:44:02,263] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 122
 INFO [2020-02-19 07:44:02,264] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 104
 INFO [2020-02-19 07:44:02,264] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 123
 INFO [2020-02-19 07:44:02,279] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 120
 INFO [2020-02-19 07:44:02,281] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 101
 INFO [2020-02-19 07:44:02,282] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 115
 INFO [2020-02-19 07:44:02,283] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 110
 INFO [2020-02-19 07:44:02,284] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 102
 INFO [2020-02-19 07:44:02,284] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 105
 INFO [2020-02-19 07:44:02,285] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 107
 INFO [2020-02-19 07:44:02,285] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 111
 INFO [2020-02-19 07:44:02,286] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 117
 INFO [2020-02-19 07:44:02,287] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 103
 INFO [2020-02-19 07:44:02,287] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 106
 INFO [2020-02-19 07:44:02,288] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 108
 INFO [2020-02-19 07:44:02,300] ({dispatcher-event-loop-2} Logging.scala[logInfo]:54) - Removed broadcast_11_piece0 on 172.27.0.6:36245 in memory (size: 2.0 KB, free: 912.2 MB)
 INFO [2020-02-19 07:44:02,302] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Removed broadcast_11_piece0 on 172.27.0.7:42597 in memory (size: 2.0 KB, free: 912.2 MB)
 INFO [2020-02-19 07:44:02,319] ({dispatcher-event-loop-2} Logging.scala[logInfo]:54) - Removed broadcast_11_piece0 on zeppelin:44945 in memory (size: 2.0 KB, free: 366.1 MB)
 INFO [2020-02-19 07:44:02,323] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 124
 INFO [2020-02-19 07:44:02,324] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 116
 INFO [2020-02-19 07:44:02,325] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 118
 INFO [2020-02-19 07:44:02,326] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 109
 INFO [2020-02-19 07:44:02,330] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 119
 INFO [2020-02-19 07:44:02,331] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 100
 INFO [2020-02-19 07:44:02,332] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 114
 INFO [2020-02-19 07:44:03,093] ({pool-2-thread-10} Logging.scala[logInfo]:54) - Block broadcast_12 stored as values in memory (estimated size 236.7 KB, free 364.3 MB)
 INFO [2020-02-19 07:44:03,108] ({pool-2-thread-10} Logging.scala[logInfo]:54) - Block broadcast_12_piece0 stored as bytes in memory (estimated size 22.9 KB, free 364.3 MB)
 INFO [2020-02-19 07:44:03,110] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Added broadcast_12_piece0 in memory on zeppelin:44945 (size: 22.9 KB, free: 366.1 MB)
 INFO [2020-02-19 07:44:03,112] ({pool-2-thread-10} Logging.scala[logInfo]:54) - Created broadcast 12 from textFile at <console>:72
 INFO [2020-02-19 07:44:03,361] ({pool-2-thread-10} FileInputFormat.java[listStatus]:249) - Total input paths to process : 1
 INFO [2020-02-19 07:44:03,369] ({pool-2-thread-10} Logging.scala[logInfo]:54) - Starting job: first at <console>:73
 INFO [2020-02-19 07:44:03,371] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Got job 5 (first at <console>:73) with 1 output partitions
 INFO [2020-02-19 07:44:03,372] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Final stage: ResultStage 5 (first at <console>:73)
 INFO [2020-02-19 07:44:03,372] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Parents of final stage: List()
 INFO [2020-02-19 07:44:03,373] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Missing parents: List()
 INFO [2020-02-19 07:44:03,375] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting ResultStage 5 (data/test.txt MapPartitionsRDD[15] at textFile at <console>:72), which has no missing parents
 INFO [2020-02-19 07:44:03,378] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_13 stored as values in memory (estimated size 3.4 KB, free 364.3 MB)
 INFO [2020-02-19 07:44:03,379] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_13_piece0 stored as bytes in memory (estimated size 2.0 KB, free 364.3 MB)
 INFO [2020-02-19 07:44:03,381] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Added broadcast_13_piece0 in memory on zeppelin:44945 (size: 2.0 KB, free: 366.1 MB)
 INFO [2020-02-19 07:44:03,381] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Created broadcast 13 from broadcast at DAGScheduler.scala:1161
 INFO [2020-02-19 07:44:03,382] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting 1 missing tasks from ResultStage 5 (data/test.txt MapPartitionsRDD[15] at textFile at <console>:72) (first 15 tasks are for partitions Vector(0))
 INFO [2020-02-19 07:44:03,383] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Adding task set 5.0 with 1 tasks
 INFO [2020-02-19 07:44:03,384] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Added task set TaskSet_5.0 tasks to pool default
 INFO [2020-02-19 07:44:03,384] ({dispatcher-event-loop-2} Logging.scala[logInfo]:54) - Starting task 0.0 in stage 5.0 (TID 20, 172.27.0.6, executor 0, partition 0, PROCESS_LOCAL, 7892 bytes)
 INFO [2020-02-19 07:44:03,402] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Added broadcast_13_piece0 in memory on 172.27.0.6:36245 (size: 2.0 KB, free: 912.2 MB)
 INFO [2020-02-19 07:44:03,420] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Added broadcast_12_piece0 in memory on 172.27.0.6:36245 (size: 22.9 KB, free: 912.2 MB)
 WARN [2020-02-19 07:44:03,447] ({task-result-getter-0} Logging.scala[logWarning]:66) - Lost task 0.0 in stage 5.0 (TID 20, 172.27.0.6, executor 0): java.io.FileNotFoundException: File file:/zeppelin/data/test.txt does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:611)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:824)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:601)
	at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:421)
	at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:142)
	at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:346)
	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:769)
	at org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:109)
	at org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.liftedTree1$1(HadoopRDD.scala:267)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:266)
	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:224)
	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:95)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

 INFO [2020-02-19 07:44:03,450] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Starting task 0.1 in stage 5.0 (TID 21, 172.27.0.6, executor 0, partition 0, PROCESS_LOCAL, 7892 bytes)
 INFO [2020-02-19 07:44:03,462] ({task-result-getter-1} Logging.scala[logInfo]:54) - Lost task 0.1 in stage 5.0 (TID 21) on 172.27.0.6, executor 0: java.io.FileNotFoundException (File file:/zeppelin/data/test.txt does not exist) [duplicate 1]
 INFO [2020-02-19 07:44:03,467] ({dispatcher-event-loop-3} Logging.scala[logInfo]:54) - Starting task 0.2 in stage 5.0 (TID 22, 172.27.0.7, executor 1, partition 0, PROCESS_LOCAL, 7892 bytes)
 INFO [2020-02-19 07:44:03,481] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Added broadcast_13_piece0 in memory on 172.27.0.7:42597 (size: 2.0 KB, free: 912.2 MB)
 INFO [2020-02-19 07:44:03,495] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Added broadcast_12_piece0 in memory on 172.27.0.7:42597 (size: 22.9 KB, free: 912.2 MB)
 INFO [2020-02-19 07:44:03,521] ({task-result-getter-2} Logging.scala[logInfo]:54) - Lost task 0.2 in stage 5.0 (TID 22) on 172.27.0.7, executor 1: java.io.FileNotFoundException (File file:/zeppelin/data/test.txt does not exist) [duplicate 2]
 INFO [2020-02-19 07:44:03,523] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Starting task 0.3 in stage 5.0 (TID 23, 172.27.0.7, executor 1, partition 0, PROCESS_LOCAL, 7892 bytes)
 INFO [2020-02-19 07:44:03,535] ({task-result-getter-3} Logging.scala[logInfo]:54) - Lost task 0.3 in stage 5.0 (TID 23) on 172.27.0.7, executor 1: java.io.FileNotFoundException (File file:/zeppelin/data/test.txt does not exist) [duplicate 3]
ERROR [2020-02-19 07:44:03,538] ({task-result-getter-3} Logging.scala[logError]:70) - Task 0 in stage 5.0 failed 4 times; aborting job
 INFO [2020-02-19 07:44:03,539] ({task-result-getter-3} Logging.scala[logInfo]:54) - Removed TaskSet 5.0, whose tasks have all completed, from pool default
 INFO [2020-02-19 07:44:03,540] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Cancelling stage 5
 INFO [2020-02-19 07:44:03,541] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Killing all running tasks in stage 5: Stage cancelled
 INFO [2020-02-19 07:44:03,542] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - ResultStage 5 (first at <console>:73) failed in 0.165 s due to Job aborted due to stage failure: Task 0 in stage 5.0 failed 4 times, most recent failure: Lost task 0.3 in stage 5.0 (TID 23, 172.27.0.7, executor 1): java.io.FileNotFoundException: File file:/zeppelin/data/test.txt does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:611)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:824)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:601)
	at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:421)
	at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:142)
	at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:346)
	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:769)
	at org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:109)
	at org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.liftedTree1$1(HadoopRDD.scala:267)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:266)
	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:224)
	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:95)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
 INFO [2020-02-19 07:44:03,543] ({pool-2-thread-10} Logging.scala[logInfo]:54) - Job 5 failed: first at <console>:73, took 0.172348 s
 INFO [2020-02-19 07:44:03,748] ({pool-2-thread-10} SchedulerFactory.java[jobFinished]:120) - Job 20200219-065917_1853343063 finished by scheduler interpreter_299988080
 INFO [2020-02-19 07:44:09,654] ({pool-2-thread-6} SchedulerFactory.java[jobStarted]:114) - Job 20200219-065917_1853343063 started by scheduler interpreter_299988080
 INFO [2020-02-19 07:44:09,671] ({pool-2-thread-6} OldSparkInterpreter.java[populateSparkWebUrl]:931) - Sending metadata to Zeppelin server: {message=Spark UI enabled, url=http://zeppelin:4040}
 INFO [2020-02-19 07:44:10,039] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 135
 INFO [2020-02-19 07:44:10,040] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 141
 INFO [2020-02-19 07:44:10,042] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 134
 INFO [2020-02-19 07:44:10,043] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 143
 INFO [2020-02-19 07:44:10,044] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 133
 INFO [2020-02-19 07:44:10,045] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 147
 INFO [2020-02-19 07:44:10,046] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 131
 INFO [2020-02-19 07:44:10,047] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 139
 INFO [2020-02-19 07:44:10,047] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 128
 INFO [2020-02-19 07:44:10,048] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 130
 INFO [2020-02-19 07:44:10,050] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 137
 INFO [2020-02-19 07:44:10,051] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 132
 INFO [2020-02-19 07:44:10,052] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 148
 INFO [2020-02-19 07:44:10,053] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 125
 INFO [2020-02-19 07:44:10,054] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 138
 INFO [2020-02-19 07:44:10,055] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 142
 INFO [2020-02-19 07:44:10,056] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 129
 INFO [2020-02-19 07:44:10,057] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 127
 INFO [2020-02-19 07:44:10,066] ({dispatcher-event-loop-3} Logging.scala[logInfo]:54) - Removed broadcast_13_piece0 on zeppelin:44945 in memory (size: 2.0 KB, free: 366.1 MB)
 INFO [2020-02-19 07:44:10,068] ({dispatcher-event-loop-3} Logging.scala[logInfo]:54) - Removed broadcast_13_piece0 on 172.27.0.7:42597 in memory (size: 2.0 KB, free: 912.2 MB)
 INFO [2020-02-19 07:44:10,069] ({dispatcher-event-loop-3} Logging.scala[logInfo]:54) - Removed broadcast_13_piece0 on 172.27.0.6:36245 in memory (size: 2.0 KB, free: 912.2 MB)
 INFO [2020-02-19 07:44:10,085] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 149
 INFO [2020-02-19 07:44:10,086] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 126
 INFO [2020-02-19 07:44:10,086] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 145
 INFO [2020-02-19 07:44:10,087] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 144
 INFO [2020-02-19 07:44:10,089] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 146
 INFO [2020-02-19 07:44:10,090] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 140
 INFO [2020-02-19 07:44:10,091] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 136
 INFO [2020-02-19 07:44:10,923] ({pool-2-thread-6} Logging.scala[logInfo]:54) - Block broadcast_14 stored as values in memory (estimated size 236.7 KB, free 364.0 MB)
 INFO [2020-02-19 07:44:10,933] ({pool-2-thread-6} Logging.scala[logInfo]:54) - Block broadcast_14_piece0 stored as bytes in memory (estimated size 22.9 KB, free 364.0 MB)
 INFO [2020-02-19 07:44:10,934] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Added broadcast_14_piece0 in memory on zeppelin:44945 (size: 22.9 KB, free: 366.1 MB)
 INFO [2020-02-19 07:44:10,936] ({pool-2-thread-6} Logging.scala[logInfo]:54) - Created broadcast 14 from textFile at <console>:75
 INFO [2020-02-19 07:44:11,135] ({pool-2-thread-6} FileInputFormat.java[listStatus]:249) - Total input paths to process : 1
 INFO [2020-02-19 07:44:11,144] ({pool-2-thread-6} Logging.scala[logInfo]:54) - Starting job: first at <console>:76
 INFO [2020-02-19 07:44:11,146] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Got job 6 (first at <console>:76) with 1 output partitions
 INFO [2020-02-19 07:44:11,147] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Final stage: ResultStage 6 (first at <console>:76)
 INFO [2020-02-19 07:44:11,147] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Parents of final stage: List()
 INFO [2020-02-19 07:44:11,148] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Missing parents: List()
 INFO [2020-02-19 07:44:11,149] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting ResultStage 6 (data/test.txt MapPartitionsRDD[17] at textFile at <console>:75), which has no missing parents
 INFO [2020-02-19 07:44:11,151] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_15 stored as values in memory (estimated size 3.4 KB, free 364.0 MB)
 INFO [2020-02-19 07:44:11,156] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_15_piece0 stored as bytes in memory (estimated size 2.0 KB, free 364.0 MB)
 INFO [2020-02-19 07:44:11,157] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Added broadcast_15_piece0 in memory on zeppelin:44945 (size: 2.0 KB, free: 366.1 MB)
 INFO [2020-02-19 07:44:11,158] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Created broadcast 15 from broadcast at DAGScheduler.scala:1161
 INFO [2020-02-19 07:44:11,160] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting 1 missing tasks from ResultStage 6 (data/test.txt MapPartitionsRDD[17] at textFile at <console>:75) (first 15 tasks are for partitions Vector(0))
 INFO [2020-02-19 07:44:11,161] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Adding task set 6.0 with 1 tasks
 INFO [2020-02-19 07:44:11,162] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Added task set TaskSet_6.0 tasks to pool default
 INFO [2020-02-19 07:44:11,163] ({dispatcher-event-loop-3} Logging.scala[logInfo]:54) - Starting task 0.0 in stage 6.0 (TID 24, 172.27.0.7, executor 1, partition 0, PROCESS_LOCAL, 7892 bytes)
 INFO [2020-02-19 07:44:11,183] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Added broadcast_15_piece0 in memory on 172.27.0.7:42597 (size: 2.0 KB, free: 912.2 MB)
 INFO [2020-02-19 07:44:11,198] ({dispatcher-event-loop-2} Logging.scala[logInfo]:54) - Added broadcast_14_piece0 in memory on 172.27.0.7:42597 (size: 22.9 KB, free: 912.1 MB)
 WARN [2020-02-19 07:44:11,227] ({task-result-getter-0} Logging.scala[logWarning]:66) - Lost task 0.0 in stage 6.0 (TID 24, 172.27.0.7, executor 1): java.io.FileNotFoundException: File file:/zeppelin/data/test.txt does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:611)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:824)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:601)
	at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:421)
	at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:142)
	at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:346)
	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:769)
	at org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:109)
	at org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.liftedTree1$1(HadoopRDD.scala:267)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:266)
	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:224)
	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:95)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

 INFO [2020-02-19 07:44:11,231] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Starting task 0.1 in stage 6.0 (TID 25, 172.27.0.6, executor 0, partition 0, PROCESS_LOCAL, 7892 bytes)
 INFO [2020-02-19 07:44:11,264] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Added broadcast_15_piece0 in memory on 172.27.0.6:36245 (size: 2.0 KB, free: 912.2 MB)
 INFO [2020-02-19 07:44:11,297] ({dispatcher-event-loop-3} Logging.scala[logInfo]:54) - Added broadcast_14_piece0 in memory on 172.27.0.6:36245 (size: 22.9 KB, free: 912.2 MB)
 INFO [2020-02-19 07:44:11,334] ({task-result-getter-1} Logging.scala[logInfo]:54) - Lost task 0.1 in stage 6.0 (TID 25) on 172.27.0.6, executor 0: java.io.FileNotFoundException (File file:/zeppelin/data/test.txt does not exist) [duplicate 1]
 INFO [2020-02-19 07:44:11,337] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Starting task 0.2 in stage 6.0 (TID 26, 172.27.0.6, executor 0, partition 0, PROCESS_LOCAL, 7892 bytes)
 INFO [2020-02-19 07:44:11,355] ({task-result-getter-2} Logging.scala[logInfo]:54) - Lost task 0.2 in stage 6.0 (TID 26) on 172.27.0.6, executor 0: java.io.FileNotFoundException (File file:/zeppelin/data/test.txt does not exist) [duplicate 2]
 INFO [2020-02-19 07:44:11,356] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Starting task 0.3 in stage 6.0 (TID 27, 172.27.0.6, executor 0, partition 0, PROCESS_LOCAL, 7892 bytes)
 INFO [2020-02-19 07:44:11,367] ({task-result-getter-3} Logging.scala[logInfo]:54) - Lost task 0.3 in stage 6.0 (TID 27) on 172.27.0.6, executor 0: java.io.FileNotFoundException (File file:/zeppelin/data/test.txt does not exist) [duplicate 3]
ERROR [2020-02-19 07:44:11,368] ({task-result-getter-3} Logging.scala[logError]:70) - Task 0 in stage 6.0 failed 4 times; aborting job
 INFO [2020-02-19 07:44:11,375] ({task-result-getter-3} Logging.scala[logInfo]:54) - Removed TaskSet 6.0, whose tasks have all completed, from pool default
 INFO [2020-02-19 07:44:11,377] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Cancelling stage 6
 INFO [2020-02-19 07:44:11,378] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Killing all running tasks in stage 6: Stage cancelled
 INFO [2020-02-19 07:44:11,379] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - ResultStage 6 (first at <console>:76) failed in 0.229 s due to Job aborted due to stage failure: Task 0 in stage 6.0 failed 4 times, most recent failure: Lost task 0.3 in stage 6.0 (TID 27, 172.27.0.6, executor 0): java.io.FileNotFoundException: File file:/zeppelin/data/test.txt does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:611)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:824)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:601)
	at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:421)
	at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:142)
	at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:346)
	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:769)
	at org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:109)
	at org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.liftedTree1$1(HadoopRDD.scala:267)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:266)
	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:224)
	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:95)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
 INFO [2020-02-19 07:44:11,380] ({pool-2-thread-6} Logging.scala[logInfo]:54) - Job 6 failed: first at <console>:76, took 0.234840 s
 INFO [2020-02-19 07:44:11,626] ({pool-2-thread-6} SchedulerFactory.java[jobFinished]:120) - Job 20200219-065917_1853343063 finished by scheduler interpreter_299988080
 INFO [2020-02-19 07:46:10,075] ({pool-2-thread-11} SchedulerFactory.java[jobStarted]:114) - Job 20200219-065917_1853343063 started by scheduler interpreter_299988080
 INFO [2020-02-19 07:46:10,078] ({pool-2-thread-11} OldSparkInterpreter.java[populateSparkWebUrl]:931) - Sending metadata to Zeppelin server: {message=Spark UI enabled, url=http://zeppelin:4040}
 INFO [2020-02-19 07:46:10,493] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 172
 INFO [2020-02-19 07:46:10,493] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 154
 INFO [2020-02-19 07:46:10,494] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 159
 INFO [2020-02-19 07:46:10,495] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 174
 INFO [2020-02-19 07:46:10,499] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 151
 INFO [2020-02-19 07:46:10,499] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 171
 INFO [2020-02-19 07:46:10,500] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 157
 INFO [2020-02-19 07:46:10,501] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 165
 INFO [2020-02-19 07:46:10,501] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 170
 INFO [2020-02-19 07:46:10,502] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 153
 INFO [2020-02-19 07:46:10,502] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 161
 INFO [2020-02-19 07:46:10,503] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 167
 INFO [2020-02-19 07:46:10,503] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 156
 INFO [2020-02-19 07:46:10,504] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 160
 INFO [2020-02-19 07:46:10,504] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 168
 INFO [2020-02-19 07:46:10,505] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 163
 INFO [2020-02-19 07:46:10,506] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 166
 INFO [2020-02-19 07:46:10,506] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 158
 INFO [2020-02-19 07:46:10,507] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 152
 INFO [2020-02-19 07:46:10,508] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 164
 INFO [2020-02-19 07:46:10,508] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 150
 INFO [2020-02-19 07:46:10,509] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 155
 INFO [2020-02-19 07:46:10,509] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 173
 INFO [2020-02-19 07:46:10,510] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 162
 INFO [2020-02-19 07:46:10,521] ({dispatcher-event-loop-3} Logging.scala[logInfo]:54) - Removed broadcast_15_piece0 on zeppelin:44945 in memory (size: 2.0 KB, free: 366.1 MB)
 INFO [2020-02-19 07:46:10,524] ({dispatcher-event-loop-2} Logging.scala[logInfo]:54) - Removed broadcast_15_piece0 on 172.27.0.7:42597 in memory (size: 2.0 KB, free: 912.1 MB)
 INFO [2020-02-19 07:46:10,538] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Removed broadcast_15_piece0 on 172.27.0.6:36245 in memory (size: 2.0 KB, free: 912.2 MB)
 INFO [2020-02-19 07:46:10,612] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 169
 INFO [2020-02-19 07:46:11,407] ({pool-2-thread-11} Logging.scala[logInfo]:54) - Block broadcast_16 stored as values in memory (estimated size 236.7 KB, free 363.8 MB)
 INFO [2020-02-19 07:46:11,415] ({pool-2-thread-11} Logging.scala[logInfo]:54) - Block broadcast_16_piece0 stored as bytes in memory (estimated size 22.9 KB, free 363.8 MB)
 INFO [2020-02-19 07:46:11,418] ({dispatcher-event-loop-3} Logging.scala[logInfo]:54) - Added broadcast_16_piece0 in memory on zeppelin:44945 (size: 22.9 KB, free: 366.1 MB)
 INFO [2020-02-19 07:46:11,420] ({pool-2-thread-11} Logging.scala[logInfo]:54) - Created broadcast 16 from textFile at <console>:78
 INFO [2020-02-19 07:46:11,631] ({pool-2-thread-11} FileInputFormat.java[listStatus]:249) - Total input paths to process : 1
 INFO [2020-02-19 07:46:11,640] ({pool-2-thread-11} Logging.scala[logInfo]:54) - Starting job: first at <console>:79
 INFO [2020-02-19 07:46:11,642] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Got job 7 (first at <console>:79) with 1 output partitions
 INFO [2020-02-19 07:46:11,643] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Final stage: ResultStage 7 (first at <console>:79)
 INFO [2020-02-19 07:46:11,645] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Parents of final stage: List()
 INFO [2020-02-19 07:46:11,645] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Missing parents: List()
 INFO [2020-02-19 07:46:11,646] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting ResultStage 7 (file:/zeppelin/data/test.txt MapPartitionsRDD[19] at textFile at <console>:78), which has no missing parents
 INFO [2020-02-19 07:46:11,650] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_17 stored as values in memory (estimated size 3.5 KB, free 363.8 MB)
 INFO [2020-02-19 07:46:11,654] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_17_piece0 stored as bytes in memory (estimated size 2.0 KB, free 363.8 MB)
 INFO [2020-02-19 07:46:11,657] ({dispatcher-event-loop-2} Logging.scala[logInfo]:54) - Added broadcast_17_piece0 in memory on zeppelin:44945 (size: 2.0 KB, free: 366.1 MB)
 INFO [2020-02-19 07:46:11,658] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Created broadcast 17 from broadcast at DAGScheduler.scala:1161
 INFO [2020-02-19 07:46:11,660] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting 1 missing tasks from ResultStage 7 (file:/zeppelin/data/test.txt MapPartitionsRDD[19] at textFile at <console>:78) (first 15 tasks are for partitions Vector(0))
 INFO [2020-02-19 07:46:11,661] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Adding task set 7.0 with 1 tasks
 INFO [2020-02-19 07:46:11,662] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Added task set TaskSet_7.0 tasks to pool default
 INFO [2020-02-19 07:46:11,663] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Starting task 0.0 in stage 7.0 (TID 28, 172.27.0.7, executor 1, partition 0, PROCESS_LOCAL, 7892 bytes)
 INFO [2020-02-19 07:46:11,680] ({dispatcher-event-loop-2} Logging.scala[logInfo]:54) - Added broadcast_17_piece0 in memory on 172.27.0.7:42597 (size: 2.0 KB, free: 912.1 MB)
 INFO [2020-02-19 07:46:11,694] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Added broadcast_16_piece0 in memory on 172.27.0.7:42597 (size: 22.9 KB, free: 912.1 MB)
 WARN [2020-02-19 07:46:11,718] ({task-result-getter-0} Logging.scala[logWarning]:66) - Lost task 0.0 in stage 7.0 (TID 28, 172.27.0.7, executor 1): java.io.FileNotFoundException: File file:/zeppelin/data/test.txt does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:611)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:824)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:601)
	at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:421)
	at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:142)
	at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:346)
	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:769)
	at org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:109)
	at org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.liftedTree1$1(HadoopRDD.scala:267)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:266)
	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:224)
	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:95)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

 INFO [2020-02-19 07:46:11,720] ({dispatcher-event-loop-2} Logging.scala[logInfo]:54) - Starting task 0.1 in stage 7.0 (TID 29, 172.27.0.6, executor 0, partition 0, PROCESS_LOCAL, 7892 bytes)
 INFO [2020-02-19 07:46:11,741] ({dispatcher-event-loop-3} Logging.scala[logInfo]:54) - Added broadcast_17_piece0 in memory on 172.27.0.6:36245 (size: 2.0 KB, free: 912.2 MB)
 INFO [2020-02-19 07:46:11,769] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Added broadcast_16_piece0 in memory on 172.27.0.6:36245 (size: 22.9 KB, free: 912.1 MB)
 INFO [2020-02-19 07:46:11,820] ({task-result-getter-1} Logging.scala[logInfo]:54) - Lost task 0.1 in stage 7.0 (TID 29) on 172.27.0.6, executor 0: java.io.FileNotFoundException (File file:/zeppelin/data/test.txt does not exist) [duplicate 1]
 INFO [2020-02-19 07:46:11,821] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Starting task 0.2 in stage 7.0 (TID 30, 172.27.0.7, executor 1, partition 0, PROCESS_LOCAL, 7892 bytes)
 INFO [2020-02-19 07:46:11,841] ({task-result-getter-2} Logging.scala[logInfo]:54) - Lost task 0.2 in stage 7.0 (TID 30) on 172.27.0.7, executor 1: java.io.FileNotFoundException (File file:/zeppelin/data/test.txt does not exist) [duplicate 2]
 INFO [2020-02-19 07:46:11,842] ({dispatcher-event-loop-2} Logging.scala[logInfo]:54) - Starting task 0.3 in stage 7.0 (TID 31, 172.27.0.7, executor 1, partition 0, PROCESS_LOCAL, 7892 bytes)
 INFO [2020-02-19 07:46:11,863] ({task-result-getter-3} Logging.scala[logInfo]:54) - Lost task 0.3 in stage 7.0 (TID 31) on 172.27.0.7, executor 1: java.io.FileNotFoundException (File file:/zeppelin/data/test.txt does not exist) [duplicate 3]
ERROR [2020-02-19 07:46:11,864] ({task-result-getter-3} Logging.scala[logError]:70) - Task 0 in stage 7.0 failed 4 times; aborting job
 INFO [2020-02-19 07:46:11,865] ({task-result-getter-3} Logging.scala[logInfo]:54) - Removed TaskSet 7.0, whose tasks have all completed, from pool default
 INFO [2020-02-19 07:46:11,866] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Cancelling stage 7
 INFO [2020-02-19 07:46:11,866] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Killing all running tasks in stage 7: Stage cancelled
 INFO [2020-02-19 07:46:11,869] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - ResultStage 7 (first at <console>:79) failed in 0.220 s due to Job aborted due to stage failure: Task 0 in stage 7.0 failed 4 times, most recent failure: Lost task 0.3 in stage 7.0 (TID 31, 172.27.0.7, executor 1): java.io.FileNotFoundException: File file:/zeppelin/data/test.txt does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:611)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:824)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:601)
	at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:421)
	at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:142)
	at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:346)
	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:769)
	at org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:109)
	at org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.liftedTree1$1(HadoopRDD.scala:267)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:266)
	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:224)
	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:95)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
 INFO [2020-02-19 07:46:11,871] ({pool-2-thread-11} Logging.scala[logInfo]:54) - Job 7 failed: first at <console>:79, took 0.229627 s
 INFO [2020-02-19 07:46:12,070] ({pool-2-thread-11} SchedulerFactory.java[jobFinished]:120) - Job 20200219-065917_1853343063 finished by scheduler interpreter_299988080
 INFO [2020-02-19 07:46:32,050] ({pool-2-thread-3} SchedulerFactory.java[jobStarted]:114) - Job 20200219-065917_1853343063 started by scheduler interpreter_299988080
 INFO [2020-02-19 07:46:32,055] ({pool-2-thread-3} OldSparkInterpreter.java[populateSparkWebUrl]:931) - Sending metadata to Zeppelin server: {message=Spark UI enabled, url=http://zeppelin:4040}
 INFO [2020-02-19 07:46:32,649] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 196
 INFO [2020-02-19 07:46:32,650] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 194
 INFO [2020-02-19 07:46:32,650] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 199
 INFO [2020-02-19 07:46:32,657] ({dispatcher-event-loop-3} Logging.scala[logInfo]:54) - Removed broadcast_17_piece0 on 172.27.0.7:42597 in memory (size: 2.0 KB, free: 912.1 MB)
 INFO [2020-02-19 07:46:32,658] ({dispatcher-event-loop-3} Logging.scala[logInfo]:54) - Removed broadcast_17_piece0 on 172.27.0.6:36245 in memory (size: 2.0 KB, free: 912.1 MB)
 INFO [2020-02-19 07:46:32,677] ({dispatcher-event-loop-3} Logging.scala[logInfo]:54) - Removed broadcast_17_piece0 on zeppelin:44945 in memory (size: 2.0 KB, free: 366.1 MB)
 INFO [2020-02-19 07:46:32,733] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 190
 INFO [2020-02-19 07:46:32,734] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 177
 INFO [2020-02-19 07:46:32,736] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 175
 INFO [2020-02-19 07:46:32,737] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 186
 INFO [2020-02-19 07:46:32,738] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 176
 INFO [2020-02-19 07:46:32,738] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 189
 INFO [2020-02-19 07:46:32,739] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 197
 INFO [2020-02-19 07:46:32,739] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 180
 INFO [2020-02-19 07:46:32,740] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 182
 INFO [2020-02-19 07:46:32,741] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 183
 INFO [2020-02-19 07:46:32,741] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 193
 INFO [2020-02-19 07:46:32,742] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 181
 INFO [2020-02-19 07:46:32,743] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 191
 INFO [2020-02-19 07:46:32,743] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 179
 INFO [2020-02-19 07:46:32,744] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 178
 INFO [2020-02-19 07:46:32,745] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 198
 INFO [2020-02-19 07:46:32,745] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 184
 INFO [2020-02-19 07:46:32,746] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 185
 INFO [2020-02-19 07:46:32,747] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 187
 INFO [2020-02-19 07:46:32,747] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 188
 INFO [2020-02-19 07:46:32,748] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 192
 INFO [2020-02-19 07:46:32,749] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 195
 INFO [2020-02-19 07:46:33,428] ({pool-2-thread-3} Logging.scala[logInfo]:54) - Block broadcast_18 stored as values in memory (estimated size 236.7 KB, free 363.5 MB)
 INFO [2020-02-19 07:46:33,449] ({pool-2-thread-3} Logging.scala[logInfo]:54) - Block broadcast_18_piece0 stored as bytes in memory (estimated size 22.9 KB, free 363.5 MB)
 INFO [2020-02-19 07:46:33,451] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Added broadcast_18_piece0 in memory on zeppelin:44945 (size: 22.9 KB, free: 366.1 MB)
 INFO [2020-02-19 07:46:33,452] ({pool-2-thread-3} Logging.scala[logInfo]:54) - Created broadcast 18 from textFile at <console>:81
 INFO [2020-02-19 07:46:33,652] ({pool-2-thread-3} FileInputFormat.java[listStatus]:249) - Total input paths to process : 1
 INFO [2020-02-19 07:46:33,662] ({pool-2-thread-3} Logging.scala[logInfo]:54) - Starting job: first at <console>:82
 INFO [2020-02-19 07:46:33,663] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Got job 8 (first at <console>:82) with 1 output partitions
 INFO [2020-02-19 07:46:33,664] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Final stage: ResultStage 8 (first at <console>:82)
 INFO [2020-02-19 07:46:33,664] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Parents of final stage: List()
 INFO [2020-02-19 07:46:33,665] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Missing parents: List()
 INFO [2020-02-19 07:46:33,666] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting ResultStage 8 (file:///zeppelin/data/test.txt MapPartitionsRDD[21] at textFile at <console>:81), which has no missing parents
 INFO [2020-02-19 07:46:33,668] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_19 stored as values in memory (estimated size 3.5 KB, free 363.5 MB)
 INFO [2020-02-19 07:46:33,669] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_19_piece0 stored as bytes in memory (estimated size 2.0 KB, free 363.5 MB)
 INFO [2020-02-19 07:46:33,671] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Added broadcast_19_piece0 in memory on zeppelin:44945 (size: 2.0 KB, free: 366.1 MB)
 INFO [2020-02-19 07:46:33,672] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Created broadcast 19 from broadcast at DAGScheduler.scala:1161
 INFO [2020-02-19 07:46:33,674] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting 1 missing tasks from ResultStage 8 (file:///zeppelin/data/test.txt MapPartitionsRDD[21] at textFile at <console>:81) (first 15 tasks are for partitions Vector(0))
 INFO [2020-02-19 07:46:33,675] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Adding task set 8.0 with 1 tasks
 INFO [2020-02-19 07:46:33,676] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Added task set TaskSet_8.0 tasks to pool default
 INFO [2020-02-19 07:46:33,676] ({dispatcher-event-loop-3} Logging.scala[logInfo]:54) - Starting task 0.0 in stage 8.0 (TID 32, 172.27.0.7, executor 1, partition 0, PROCESS_LOCAL, 7892 bytes)
 INFO [2020-02-19 07:46:33,691] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Added broadcast_19_piece0 in memory on 172.27.0.7:42597 (size: 2.0 KB, free: 912.1 MB)
 INFO [2020-02-19 07:46:33,705] ({dispatcher-event-loop-2} Logging.scala[logInfo]:54) - Added broadcast_18_piece0 in memory on 172.27.0.7:42597 (size: 22.9 KB, free: 912.1 MB)
 WARN [2020-02-19 07:46:33,729] ({task-result-getter-0} Logging.scala[logWarning]:66) - Lost task 0.0 in stage 8.0 (TID 32, 172.27.0.7, executor 1): java.io.FileNotFoundException: File file:/zeppelin/data/test.txt does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:611)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:824)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:601)
	at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:421)
	at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:142)
	at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:346)
	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:769)
	at org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:109)
	at org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.liftedTree1$1(HadoopRDD.scala:267)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:266)
	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:224)
	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:95)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

 INFO [2020-02-19 07:46:33,731] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Starting task 0.1 in stage 8.0 (TID 33, 172.27.0.7, executor 1, partition 0, PROCESS_LOCAL, 7892 bytes)
 INFO [2020-02-19 07:46:33,743] ({task-result-getter-1} Logging.scala[logInfo]:54) - Lost task 0.1 in stage 8.0 (TID 33) on 172.27.0.7, executor 1: java.io.FileNotFoundException (File file:/zeppelin/data/test.txt does not exist) [duplicate 1]
 INFO [2020-02-19 07:46:33,748] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Starting task 0.2 in stage 8.0 (TID 34, 172.27.0.6, executor 0, partition 0, PROCESS_LOCAL, 7892 bytes)
 INFO [2020-02-19 07:46:33,767] ({dispatcher-event-loop-2} Logging.scala[logInfo]:54) - Added broadcast_19_piece0 in memory on 172.27.0.6:36245 (size: 2.0 KB, free: 912.1 MB)
 INFO [2020-02-19 07:46:33,787] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Added broadcast_18_piece0 in memory on 172.27.0.6:36245 (size: 22.9 KB, free: 912.1 MB)
 INFO [2020-02-19 07:46:33,863] ({task-result-getter-2} Logging.scala[logInfo]:54) - Lost task 0.2 in stage 8.0 (TID 34) on 172.27.0.6, executor 0: java.io.FileNotFoundException (File file:/zeppelin/data/test.txt does not exist) [duplicate 2]
 INFO [2020-02-19 07:46:33,865] ({dispatcher-event-loop-2} Logging.scala[logInfo]:54) - Starting task 0.3 in stage 8.0 (TID 35, 172.27.0.7, executor 1, partition 0, PROCESS_LOCAL, 7892 bytes)
 INFO [2020-02-19 07:46:33,878] ({task-result-getter-3} Logging.scala[logInfo]:54) - Lost task 0.3 in stage 8.0 (TID 35) on 172.27.0.7, executor 1: java.io.FileNotFoundException (File file:/zeppelin/data/test.txt does not exist) [duplicate 3]
ERROR [2020-02-19 07:46:33,880] ({task-result-getter-3} Logging.scala[logError]:70) - Task 0 in stage 8.0 failed 4 times; aborting job
 INFO [2020-02-19 07:46:33,881] ({task-result-getter-3} Logging.scala[logInfo]:54) - Removed TaskSet 8.0, whose tasks have all completed, from pool default
 INFO [2020-02-19 07:46:33,882] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Cancelling stage 8
 INFO [2020-02-19 07:46:33,883] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Killing all running tasks in stage 8: Stage cancelled
 INFO [2020-02-19 07:46:33,884] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - ResultStage 8 (first at <console>:82) failed in 0.217 s due to Job aborted due to stage failure: Task 0 in stage 8.0 failed 4 times, most recent failure: Lost task 0.3 in stage 8.0 (TID 35, 172.27.0.7, executor 1): java.io.FileNotFoundException: File file:/zeppelin/data/test.txt does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:611)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:824)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:601)
	at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:421)
	at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:142)
	at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:346)
	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:769)
	at org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:109)
	at org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.liftedTree1$1(HadoopRDD.scala:267)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:266)
	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:224)
	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:95)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
 INFO [2020-02-19 07:46:33,886] ({pool-2-thread-3} Logging.scala[logInfo]:54) - Job 8 failed: first at <console>:82, took 0.222876 s
 INFO [2020-02-19 07:46:34,232] ({pool-2-thread-3} SchedulerFactory.java[jobFinished]:120) - Job 20200219-065917_1853343063 finished by scheduler interpreter_299988080
 INFO [2020-02-19 07:48:38,838] ({pool-2-thread-12} SchedulerFactory.java[jobStarted]:114) - Job 20200219-065917_1853343063 started by scheduler interpreter_299988080
 INFO [2020-02-19 07:48:38,848] ({pool-2-thread-12} OldSparkInterpreter.java[populateSparkWebUrl]:931) - Sending metadata to Zeppelin server: {message=Spark UI enabled, url=http://zeppelin:4040}
 INFO [2020-02-19 07:48:39,236] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 212
 INFO [2020-02-19 07:48:39,237] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 216
 INFO [2020-02-19 07:48:39,237] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 215
 INFO [2020-02-19 07:48:39,238] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 221
 INFO [2020-02-19 07:48:39,239] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 203
 INFO [2020-02-19 07:48:39,240] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 214
 INFO [2020-02-19 07:48:39,243] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 201
 INFO [2020-02-19 07:48:39,244] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 222
 INFO [2020-02-19 07:48:39,246] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 205
 INFO [2020-02-19 07:48:39,247] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 202
 INFO [2020-02-19 07:48:39,248] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 219
 INFO [2020-02-19 07:48:39,249] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 213
 INFO [2020-02-19 07:48:39,251] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 207
 INFO [2020-02-19 07:48:39,257] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Removed broadcast_19_piece0 on zeppelin:44945 in memory (size: 2.0 KB, free: 366.1 MB)
 INFO [2020-02-19 07:48:39,259] ({dispatcher-event-loop-3} Logging.scala[logInfo]:54) - Removed broadcast_19_piece0 on 172.27.0.7:42597 in memory (size: 2.0 KB, free: 912.1 MB)
 INFO [2020-02-19 07:48:39,266] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Removed broadcast_19_piece0 on 172.27.0.6:36245 in memory (size: 2.0 KB, free: 912.1 MB)
 INFO [2020-02-19 07:48:39,284] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 204
 INFO [2020-02-19 07:48:39,285] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 218
 INFO [2020-02-19 07:48:39,286] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 220
 INFO [2020-02-19 07:48:39,287] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 209
 INFO [2020-02-19 07:48:39,288] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 217
 INFO [2020-02-19 07:48:39,289] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 223
 INFO [2020-02-19 07:48:39,290] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 208
 INFO [2020-02-19 07:48:39,290] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 200
 INFO [2020-02-19 07:48:39,291] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 224
 INFO [2020-02-19 07:48:39,292] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 211
 INFO [2020-02-19 07:48:39,292] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 210
 INFO [2020-02-19 07:48:39,293] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 206
 INFO [2020-02-19 07:48:40,004] ({pool-2-thread-12} SchedulerFactory.java[jobFinished]:120) - Job 20200219-065917_1853343063 finished by scheduler interpreter_299988080
 INFO [2020-02-19 07:51:01,183] ({pool-2-thread-23} SchedulerFactory.java[jobStarted]:114) - Job 20200219-065917_1853343063 started by scheduler interpreter_299988080
 INFO [2020-02-19 07:51:01,188] ({pool-2-thread-23} OldSparkInterpreter.java[populateSparkWebUrl]:931) - Sending metadata to Zeppelin server: {message=Spark UI enabled, url=http://zeppelin:4040}
 INFO [2020-02-19 07:51:03,696] ({pool-2-thread-23} SchedulerFactory.java[jobFinished]:120) - Job 20200219-065917_1853343063 finished by scheduler interpreter_299988080
 INFO [2020-02-19 07:51:25,633] ({pool-2-thread-13} SchedulerFactory.java[jobStarted]:114) - Job 20200219-065917_1853343063 started by scheduler interpreter_299988080
 INFO [2020-02-19 07:51:25,638] ({pool-2-thread-13} OldSparkInterpreter.java[populateSparkWebUrl]:931) - Sending metadata to Zeppelin server: {message=Spark UI enabled, url=http://zeppelin:4040}
 INFO [2020-02-19 07:51:31,529] ({pool-2-thread-13} Logging.scala[logInfo]:54) - Code generated in 372.360307 ms
 INFO [2020-02-19 07:51:31,552] ({pool-2-thread-13} Logging.scala[logInfo]:54) - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/zeppelin/spark-warehouse').
 INFO [2020-02-19 07:51:31,554] ({pool-2-thread-13} Logging.scala[logInfo]:54) - Warehouse path is 'file:/zeppelin/spark-warehouse'.
 INFO [2020-02-19 07:51:31,572] ({pool-2-thread-13} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@1f8a5e64{/SQL,null,AVAILABLE,@Spark}
 INFO [2020-02-19 07:51:31,574] ({pool-2-thread-13} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@3bce52f8{/SQL/json,null,AVAILABLE,@Spark}
 INFO [2020-02-19 07:51:31,576] ({pool-2-thread-13} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@3e99fe14{/SQL/execution,null,AVAILABLE,@Spark}
 INFO [2020-02-19 07:51:31,577] ({pool-2-thread-13} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@526c448b{/SQL/execution/json,null,AVAILABLE,@Spark}
 INFO [2020-02-19 07:51:31,582] ({pool-2-thread-13} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@1f5ff53d{/static/sql,null,AVAILABLE,@Spark}
 INFO [2020-02-19 07:51:32,323] ({pool-2-thread-13} Logging.scala[logInfo]:54) - Registered StateStoreCoordinator endpoint
 INFO [2020-02-19 07:51:32,783] ({pool-2-thread-13} SchedulerFactory.java[jobFinished]:120) - Job 20200219-065917_1853343063 finished by scheduler interpreter_299988080
 INFO [2020-02-19 07:52:09,059] ({pool-2-thread-4} SchedulerFactory.java[jobStarted]:114) - Job 20200219-065917_1853343063 started by scheduler interpreter_299988080
 INFO [2020-02-19 07:52:09,064] ({pool-2-thread-4} OldSparkInterpreter.java[populateSparkWebUrl]:931) - Sending metadata to Zeppelin server: {message=Spark UI enabled, url=http://zeppelin:4040}
 INFO [2020-02-19 07:52:13,593] ({pool-2-thread-4} SchedulerFactory.java[jobFinished]:120) - Job 20200219-065917_1853343063 finished by scheduler interpreter_299988080
 INFO [2020-02-19 07:54:22,106] ({pool-2-thread-14} SchedulerFactory.java[jobStarted]:114) - Job 20200219-065917_1853343063 started by scheduler interpreter_299988080
 INFO [2020-02-19 07:54:22,113] ({pool-2-thread-14} OldSparkInterpreter.java[populateSparkWebUrl]:931) - Sending metadata to Zeppelin server: {message=Spark UI enabled, url=http://zeppelin:4040}
 INFO [2020-02-19 07:54:27,634] ({pool-2-thread-14} Logging.scala[logInfo]:54) - Code generated in 28.600651 ms
 INFO [2020-02-19 07:54:27,747] ({pool-2-thread-14} Configuration.java[warnOnceIfDeprecated]:1173) - mapred.output.dir is deprecated. Instead, use mapreduce.output.fileoutputformat.outputdir
 INFO [2020-02-19 07:54:27,758] ({pool-2-thread-14} Logging.scala[logInfo]:54) - Using output committer class org.apache.hadoop.mapred.FileOutputCommitter
 INFO [2020-02-19 07:54:27,761] ({pool-2-thread-14} FileOutputCommitter.java[<init>]:108) - File Output Committer Algorithm version is 1
 INFO [2020-02-19 07:54:27,811] ({pool-2-thread-14} Logging.scala[logInfo]:54) - Starting job: runJob at SparkHadoopWriter.scala:78
 INFO [2020-02-19 07:54:27,817] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Got job 9 (runJob at SparkHadoopWriter.scala:78) with 3 output partitions
 INFO [2020-02-19 07:54:27,820] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Final stage: ResultStage 9 (runJob at SparkHadoopWriter.scala:78)
 INFO [2020-02-19 07:54:27,821] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Parents of final stage: List()
 INFO [2020-02-19 07:54:27,822] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Missing parents: List()
 INFO [2020-02-19 07:54:27,825] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting ResultStage 9 (MapPartitionsRDD[26] at saveAsTextFile at <console>:106), which has no missing parents
 INFO [2020-02-19 07:54:27,909] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_20 stored as values in memory (estimated size 74.8 KB, free 363.4 MB)
 INFO [2020-02-19 07:54:27,913] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_20_piece0 stored as bytes in memory (estimated size 27.4 KB, free 363.4 MB)
 INFO [2020-02-19 07:54:27,916] ({dispatcher-event-loop-2} Logging.scala[logInfo]:54) - Added broadcast_20_piece0 in memory on zeppelin:44945 (size: 27.4 KB, free: 366.0 MB)
 INFO [2020-02-19 07:54:27,926] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Created broadcast 20 from broadcast at DAGScheduler.scala:1161
 INFO [2020-02-19 07:54:27,933] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting 3 missing tasks from ResultStage 9 (MapPartitionsRDD[26] at saveAsTextFile at <console>:106) (first 15 tasks are for partitions Vector(0, 1, 2))
 INFO [2020-02-19 07:54:27,934] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Adding task set 9.0 with 3 tasks
 INFO [2020-02-19 07:54:27,936] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Added task set TaskSet_9.0 tasks to pool default
 INFO [2020-02-19 07:54:27,947] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Starting task 0.0 in stage 9.0 (TID 36, 172.27.0.6, executor 0, partition 0, PROCESS_LOCAL, 8064 bytes)
 INFO [2020-02-19 07:54:27,952] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Starting task 1.0 in stage 9.0 (TID 37, 172.27.0.7, executor 1, partition 1, PROCESS_LOCAL, 8064 bytes)
 INFO [2020-02-19 07:54:27,953] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Starting task 2.0 in stage 9.0 (TID 38, 172.27.0.6, executor 0, partition 2, PROCESS_LOCAL, 8064 bytes)
 INFO [2020-02-19 07:54:28,208] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Added broadcast_20_piece0 in memory on 172.27.0.7:42597 (size: 27.4 KB, free: 912.1 MB)
 INFO [2020-02-19 07:54:28,212] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Added broadcast_20_piece0 in memory on 172.27.0.6:36245 (size: 27.4 KB, free: 912.1 MB)
 INFO [2020-02-19 07:54:33,989] ({task-result-getter-1} Logging.scala[logInfo]:54) - Finished task 2.0 in stage 9.0 (TID 38) in 6030 ms on 172.27.0.6 (executor 0) (1/3)
 INFO [2020-02-19 07:54:33,994] ({task-result-getter-2} Logging.scala[logInfo]:54) - Finished task 0.0 in stage 9.0 (TID 36) in 6057 ms on 172.27.0.6 (executor 0) (2/3)
 INFO [2020-02-19 07:54:33,995] ({task-result-getter-0} Logging.scala[logInfo]:54) - Finished task 1.0 in stage 9.0 (TID 37) in 6044 ms on 172.27.0.7 (executor 1) (3/3)
 INFO [2020-02-19 07:54:33,996] ({task-result-getter-0} Logging.scala[logInfo]:54) - Removed TaskSet 9.0, whose tasks have all completed, from pool default
 INFO [2020-02-19 07:54:34,000] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - ResultStage 9 (runJob at SparkHadoopWriter.scala:78) finished in 6.167 s
 INFO [2020-02-19 07:54:34,002] ({pool-2-thread-14} Logging.scala[logInfo]:54) - Job 9 finished: runJob at SparkHadoopWriter.scala:78, took 6.187552 s
 INFO [2020-02-19 07:54:34,165] ({pool-2-thread-14} Logging.scala[logInfo]:54) - Job job_20200219075427_0026 committed.
 INFO [2020-02-19 07:54:34,169] ({pool-2-thread-14} SchedulerFactory.java[jobFinished]:120) - Job 20200219-065917_1853343063 finished by scheduler interpreter_299988080
 INFO [2020-02-19 07:55:01,049] ({pool-2-thread-8} SchedulerFactory.java[jobStarted]:114) - Job 20200219-065917_1853343063 started by scheduler interpreter_299988080
 INFO [2020-02-19 07:55:01,053] ({pool-2-thread-8} OldSparkInterpreter.java[populateSparkWebUrl]:931) - Sending metadata to Zeppelin server: {message=Spark UI enabled, url=http://zeppelin:4040}
 INFO [2020-02-19 07:55:02,113] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 236
 INFO [2020-02-19 07:55:02,114] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 241
 INFO [2020-02-19 07:55:02,115] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 248
 INFO [2020-02-19 07:55:02,115] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 240
 INFO [2020-02-19 07:55:02,116] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 242
 INFO [2020-02-19 07:55:02,117] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 249
 INFO [2020-02-19 07:55:02,117] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 229
 INFO [2020-02-19 07:55:02,118] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 230
 INFO [2020-02-19 07:55:02,119] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 237
 INFO [2020-02-19 07:55:02,120] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 235
 INFO [2020-02-19 07:55:02,120] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 227
 INFO [2020-02-19 07:55:02,121] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 226
 INFO [2020-02-19 07:55:02,123] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 239
 INFO [2020-02-19 07:55:02,125] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 245
 INFO [2020-02-19 07:55:02,129] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 234
 INFO [2020-02-19 07:55:02,130] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 250
 INFO [2020-02-19 07:55:02,131] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 243
 INFO [2020-02-19 07:55:02,132] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 233
 INFO [2020-02-19 07:55:02,133] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 246
 INFO [2020-02-19 07:55:02,134] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 232
 INFO [2020-02-19 07:55:02,147] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Removed broadcast_20_piece0 on 172.27.0.6:36245 in memory (size: 27.4 KB, free: 912.1 MB)
 INFO [2020-02-19 07:55:02,149] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Removed broadcast_20_piece0 on zeppelin:44945 in memory (size: 27.4 KB, free: 366.1 MB)
 INFO [2020-02-19 07:55:02,151] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Removed broadcast_20_piece0 on 172.27.0.7:42597 in memory (size: 27.4 KB, free: 912.1 MB)
 INFO [2020-02-19 07:55:02,181] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 231
 INFO [2020-02-19 07:55:02,182] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 244
 INFO [2020-02-19 07:55:02,183] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 228
 INFO [2020-02-19 07:55:02,184] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 247
 INFO [2020-02-19 07:55:02,185] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 238
 INFO [2020-02-19 07:55:06,929] ({pool-2-thread-8} SchedulerFactory.java[jobFinished]:120) - Job 20200219-065917_1853343063 finished by scheduler interpreter_299988080
 INFO [2020-02-19 07:55:45,184] ({pool-2-thread-15} SchedulerFactory.java[jobStarted]:114) - Job 20200219-065917_1853343063 started by scheduler interpreter_299988080
 INFO [2020-02-19 07:55:45,187] ({pool-2-thread-15} OldSparkInterpreter.java[populateSparkWebUrl]:931) - Sending metadata to Zeppelin server: {message=Spark UI enabled, url=http://zeppelin:4040}
 INFO [2020-02-19 07:55:48,786] ({pool-2-thread-15} Logging.scala[logInfo]:54) - Using output committer class org.apache.hadoop.mapred.FileOutputCommitter
 INFO [2020-02-19 07:55:48,788] ({pool-2-thread-15} FileOutputCommitter.java[<init>]:108) - File Output Committer Algorithm version is 1
 INFO [2020-02-19 07:55:48,803] ({pool-2-thread-15} Logging.scala[logInfo]:54) - Starting job: runJob at SparkHadoopWriter.scala:78
 INFO [2020-02-19 07:55:48,805] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Got job 10 (runJob at SparkHadoopWriter.scala:78) with 3 output partitions
 INFO [2020-02-19 07:55:48,806] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Final stage: ResultStage 10 (runJob at SparkHadoopWriter.scala:78)
 INFO [2020-02-19 07:55:48,806] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Parents of final stage: List()
 INFO [2020-02-19 07:55:48,807] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Missing parents: List()
 INFO [2020-02-19 07:55:48,809] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting ResultStage 10 (MapPartitionsRDD[36] at saveAsTextFile at <console>:116), which has no missing parents
 INFO [2020-02-19 07:55:48,822] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_21 stored as values in memory (estimated size 74.8 KB, free 363.4 MB)
 INFO [2020-02-19 07:55:48,834] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_21_piece0 stored as bytes in memory (estimated size 27.4 KB, free 363.4 MB)
 INFO [2020-02-19 07:55:48,835] ({dispatcher-event-loop-3} Logging.scala[logInfo]:54) - Added broadcast_21_piece0 in memory on zeppelin:44945 (size: 27.4 KB, free: 366.0 MB)
 INFO [2020-02-19 07:55:48,837] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Created broadcast 21 from broadcast at DAGScheduler.scala:1161
 INFO [2020-02-19 07:55:48,838] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting 3 missing tasks from ResultStage 10 (MapPartitionsRDD[36] at saveAsTextFile at <console>:116) (first 15 tasks are for partitions Vector(0, 1, 2))
 INFO [2020-02-19 07:55:48,839] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Adding task set 10.0 with 3 tasks
 INFO [2020-02-19 07:55:48,840] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Added task set TaskSet_10.0 tasks to pool default
 INFO [2020-02-19 07:55:48,842] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Starting task 0.0 in stage 10.0 (TID 39, 172.27.0.7, executor 1, partition 0, PROCESS_LOCAL, 8064 bytes)
 INFO [2020-02-19 07:55:48,845] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Starting task 1.0 in stage 10.0 (TID 40, 172.27.0.6, executor 0, partition 1, PROCESS_LOCAL, 8064 bytes)
 INFO [2020-02-19 07:55:48,846] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Starting task 2.0 in stage 10.0 (TID 41, 172.27.0.7, executor 1, partition 2, PROCESS_LOCAL, 8064 bytes)
 INFO [2020-02-19 07:55:48,891] ({dispatcher-event-loop-2} Logging.scala[logInfo]:54) - Added broadcast_21_piece0 in memory on 172.27.0.6:36245 (size: 27.4 KB, free: 912.1 MB)
 INFO [2020-02-19 07:55:48,907] ({dispatcher-event-loop-3} Logging.scala[logInfo]:54) - Added broadcast_21_piece0 in memory on 172.27.0.7:42597 (size: 27.4 KB, free: 912.1 MB)
 INFO [2020-02-19 07:55:49,011] ({task-result-getter-3} Logging.scala[logInfo]:54) - Finished task 1.0 in stage 10.0 (TID 40) in 167 ms on 172.27.0.6 (executor 0) (1/3)
 INFO [2020-02-19 07:55:49,139] ({task-result-getter-1} Logging.scala[logInfo]:54) - Finished task 0.0 in stage 10.0 (TID 39) in 298 ms on 172.27.0.7 (executor 1) (2/3)
 INFO [2020-02-19 07:55:49,155] ({task-result-getter-2} Logging.scala[logInfo]:54) - Finished task 2.0 in stage 10.0 (TID 41) in 310 ms on 172.27.0.7 (executor 1) (3/3)
 INFO [2020-02-19 07:55:49,160] ({task-result-getter-2} Logging.scala[logInfo]:54) - Removed TaskSet 10.0, whose tasks have all completed, from pool default
 INFO [2020-02-19 07:55:49,162] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - ResultStage 10 (runJob at SparkHadoopWriter.scala:78) finished in 0.351 s
 INFO [2020-02-19 07:55:49,170] ({pool-2-thread-15} Logging.scala[logInfo]:54) - Job 10 finished: runJob at SparkHadoopWriter.scala:78, took 0.365995 s
 INFO [2020-02-19 07:55:49,228] ({pool-2-thread-15} Logging.scala[logInfo]:54) - Job job_20200219075548_0036 committed.
 INFO [2020-02-19 07:55:49,823] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 269
 INFO [2020-02-19 07:55:49,825] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 254
 INFO [2020-02-19 07:55:49,848] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 262
 INFO [2020-02-19 07:55:49,854] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 263
 INFO [2020-02-19 07:55:49,862] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 258
 INFO [2020-02-19 07:55:49,884] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 266
 INFO [2020-02-19 07:55:49,889] ({pool-2-thread-15} SchedulerFactory.java[jobFinished]:120) - Job 20200219-065917_1853343063 finished by scheduler interpreter_299988080
 INFO [2020-02-19 07:55:49,892] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 261
 INFO [2020-02-19 07:55:49,903] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 273
 INFO [2020-02-19 07:55:49,904] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 270
 INFO [2020-02-19 07:55:49,905] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 268
 INFO [2020-02-19 07:55:49,907] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 256
 INFO [2020-02-19 07:55:49,916] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Removed broadcast_21_piece0 on zeppelin:44945 in memory (size: 27.4 KB, free: 366.1 MB)
 INFO [2020-02-19 07:55:49,929] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Removed broadcast_21_piece0 on 172.27.0.6:36245 in memory (size: 27.4 KB, free: 912.1 MB)
 INFO [2020-02-19 07:55:49,935] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Removed broadcast_21_piece0 on 172.27.0.7:42597 in memory (size: 27.4 KB, free: 912.1 MB)
 INFO [2020-02-19 07:55:49,954] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 267
 INFO [2020-02-19 07:55:49,955] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 271
 INFO [2020-02-19 07:55:49,956] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 277
 INFO [2020-02-19 07:55:49,957] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 259
 INFO [2020-02-19 07:55:49,957] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 274
 INFO [2020-02-19 07:55:49,958] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 260
 INFO [2020-02-19 07:55:49,959] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 255
 INFO [2020-02-19 07:55:49,963] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 272
 INFO [2020-02-19 07:55:49,964] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 275
 INFO [2020-02-19 07:55:49,978] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 257
 INFO [2020-02-19 07:55:49,985] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 253
 INFO [2020-02-19 07:55:49,988] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 276
 INFO [2020-02-19 07:55:49,993] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 265
 INFO [2020-02-19 07:55:49,995] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 264
 INFO [2020-02-19 07:56:24,419] ({pool-2-thread-5} SchedulerFactory.java[jobStarted]:114) - Job 20200219-065917_1853343063 started by scheduler interpreter_299988080
 INFO [2020-02-19 07:56:24,423] ({pool-2-thread-5} OldSparkInterpreter.java[populateSparkWebUrl]:931) - Sending metadata to Zeppelin server: {message=Spark UI enabled, url=http://zeppelin:4040}
 INFO [2020-02-19 07:56:28,068] ({pool-2-thread-5} Logging.scala[logInfo]:54) - Using output committer class org.apache.hadoop.mapred.FileOutputCommitter
 INFO [2020-02-19 07:56:28,069] ({pool-2-thread-5} FileOutputCommitter.java[<init>]:108) - File Output Committer Algorithm version is 1
 INFO [2020-02-19 07:56:28,078] ({pool-2-thread-5} Logging.scala[logInfo]:54) - Starting job: runJob at SparkHadoopWriter.scala:78
 INFO [2020-02-19 07:56:28,080] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Got job 11 (runJob at SparkHadoopWriter.scala:78) with 3 output partitions
 INFO [2020-02-19 07:56:28,081] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Final stage: ResultStage 11 (runJob at SparkHadoopWriter.scala:78)
 INFO [2020-02-19 07:56:28,082] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Parents of final stage: List()
 INFO [2020-02-19 07:56:28,082] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Missing parents: List()
 INFO [2020-02-19 07:56:28,083] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting ResultStage 11 (MapPartitionsRDD[41] at saveAsTextFile at <console>:121), which has no missing parents
 INFO [2020-02-19 07:56:28,096] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_22 stored as values in memory (estimated size 74.8 KB, free 363.4 MB)
 INFO [2020-02-19 07:56:28,099] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_22_piece0 stored as bytes in memory (estimated size 27.4 KB, free 363.4 MB)
 INFO [2020-02-19 07:56:28,101] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Added broadcast_22_piece0 in memory on zeppelin:44945 (size: 27.4 KB, free: 366.0 MB)
 INFO [2020-02-19 07:56:28,102] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Created broadcast 22 from broadcast at DAGScheduler.scala:1161
 INFO [2020-02-19 07:56:28,104] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting 3 missing tasks from ResultStage 11 (MapPartitionsRDD[41] at saveAsTextFile at <console>:121) (first 15 tasks are for partitions Vector(0, 1, 2))
 INFO [2020-02-19 07:56:28,105] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Adding task set 11.0 with 3 tasks
 INFO [2020-02-19 07:56:28,106] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Added task set TaskSet_11.0 tasks to pool default
 INFO [2020-02-19 07:56:28,107] ({dispatcher-event-loop-3} Logging.scala[logInfo]:54) - Starting task 0.0 in stage 11.0 (TID 42, 172.27.0.6, executor 0, partition 0, PROCESS_LOCAL, 8064 bytes)
 INFO [2020-02-19 07:56:28,109] ({dispatcher-event-loop-3} Logging.scala[logInfo]:54) - Starting task 1.0 in stage 11.0 (TID 43, 172.27.0.7, executor 1, partition 1, PROCESS_LOCAL, 8064 bytes)
 INFO [2020-02-19 07:56:28,110] ({dispatcher-event-loop-3} Logging.scala[logInfo]:54) - Starting task 2.0 in stage 11.0 (TID 44, 172.27.0.6, executor 0, partition 2, PROCESS_LOCAL, 8064 bytes)
 INFO [2020-02-19 07:56:28,128] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Added broadcast_22_piece0 in memory on 172.27.0.7:42597 (size: 27.4 KB, free: 912.1 MB)
 INFO [2020-02-19 07:56:28,139] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Added broadcast_22_piece0 in memory on 172.27.0.6:36245 (size: 27.4 KB, free: 912.1 MB)
 INFO [2020-02-19 07:56:28,204] ({task-result-getter-0} Logging.scala[logInfo]:54) - Finished task 1.0 in stage 11.0 (TID 43) in 96 ms on 172.27.0.7 (executor 1) (1/3)
 INFO [2020-02-19 07:56:28,247] ({task-result-getter-3} Logging.scala[logInfo]:54) - Finished task 2.0 in stage 11.0 (TID 44) in 138 ms on 172.27.0.6 (executor 0) (2/3)
 INFO [2020-02-19 07:56:28,259] ({task-result-getter-1} Logging.scala[logInfo]:54) - Finished task 0.0 in stage 11.0 (TID 42) in 152 ms on 172.27.0.6 (executor 0) (3/3)
 INFO [2020-02-19 07:56:28,260] ({task-result-getter-1} Logging.scala[logInfo]:54) - Removed TaskSet 11.0, whose tasks have all completed, from pool default
 INFO [2020-02-19 07:56:28,262] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - ResultStage 11 (runJob at SparkHadoopWriter.scala:78) finished in 0.176 s
 INFO [2020-02-19 07:56:28,263] ({pool-2-thread-5} Logging.scala[logInfo]:54) - Job 11 finished: runJob at SparkHadoopWriter.scala:78, took 0.183641 s
 INFO [2020-02-19 07:56:28,312] ({pool-2-thread-5} Logging.scala[logInfo]:54) - Job job_20200219075628_0041 committed.
 INFO [2020-02-19 07:56:28,926] ({pool-2-thread-5} Logging.scala[logInfo]:54) - Block broadcast_23 stored as values in memory (estimated size 238.8 KB, free 363.2 MB)
 INFO [2020-02-19 07:56:28,936] ({pool-2-thread-5} Logging.scala[logInfo]:54) - Block broadcast_23_piece0 stored as bytes in memory (estimated size 23.0 KB, free 363.2 MB)
 INFO [2020-02-19 07:56:28,938] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Added broadcast_23_piece0 in memory on zeppelin:44945 (size: 23.0 KB, free: 366.0 MB)
 INFO [2020-02-19 07:56:28,940] ({pool-2-thread-5} Logging.scala[logInfo]:54) - Created broadcast 23 from textFile at <console>:120
 INFO [2020-02-19 07:56:29,377] ({pool-2-thread-5} FileInputFormat.java[listStatus]:249) - Total input paths to process : 0
 INFO [2020-02-19 07:56:29,582] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Removed broadcast_22_piece0 on 172.27.0.7:42597 in memory (size: 27.4 KB, free: 912.1 MB)
 INFO [2020-02-19 07:56:29,589] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Removed broadcast_22_piece0 on 172.27.0.6:36245 in memory (size: 27.4 KB, free: 912.1 MB)
 INFO [2020-02-19 07:56:29,597] ({dispatcher-event-loop-2} Logging.scala[logInfo]:54) - Removed broadcast_22_piece0 on zeppelin:44945 in memory (size: 27.4 KB, free: 366.0 MB)
 INFO [2020-02-19 07:56:29,610] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 285
 INFO [2020-02-19 07:56:29,611] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 288
 INFO [2020-02-19 07:56:29,612] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 293
 INFO [2020-02-19 07:56:29,613] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 297
 INFO [2020-02-19 07:56:29,613] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 286
 INFO [2020-02-19 07:56:29,614] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 299
 INFO [2020-02-19 07:56:29,615] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 290
 INFO [2020-02-19 07:56:29,616] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 280
 INFO [2020-02-19 07:56:29,617] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 295
 INFO [2020-02-19 07:56:29,618] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 303
 INFO [2020-02-19 07:56:29,618] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 291
 INFO [2020-02-19 07:56:29,619] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 302
 INFO [2020-02-19 07:56:29,620] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 289
 INFO [2020-02-19 07:56:29,621] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 298
 INFO [2020-02-19 07:56:29,622] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 282
 INFO [2020-02-19 07:56:29,622] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 294
 INFO [2020-02-19 07:56:29,623] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 300
 INFO [2020-02-19 07:56:29,624] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 292
 INFO [2020-02-19 07:56:29,624] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 301
 INFO [2020-02-19 07:56:29,625] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 281
 INFO [2020-02-19 07:56:29,625] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 283
 INFO [2020-02-19 07:56:29,626] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 296
 INFO [2020-02-19 07:56:29,627] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 279
 INFO [2020-02-19 07:56:29,627] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 287
 INFO [2020-02-19 07:56:29,628] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 284
 INFO [2020-02-19 07:56:29,688] ({pool-2-thread-5} SchedulerFactory.java[jobFinished]:120) - Job 20200219-065917_1853343063 finished by scheduler interpreter_299988080
 INFO [2020-02-19 07:57:38,898] ({pool-2-thread-16} SchedulerFactory.java[jobStarted]:114) - Job 20200219-065917_1853343063 started by scheduler interpreter_299988080
 INFO [2020-02-19 07:57:38,904] ({pool-2-thread-16} OldSparkInterpreter.java[populateSparkWebUrl]:931) - Sending metadata to Zeppelin server: {message=Spark UI enabled, url=http://zeppelin:4040}
 INFO [2020-02-19 07:57:42,163] ({pool-2-thread-16} SchedulerFactory.java[jobFinished]:120) - Job 20200219-065917_1853343063 finished by scheduler interpreter_299988080
 INFO [2020-02-19 07:58:13,624] ({pool-2-thread-9} SchedulerFactory.java[jobStarted]:114) - Job 20200219-065917_1853343063 started by scheduler interpreter_299988080
 INFO [2020-02-19 07:58:13,627] ({pool-2-thread-9} OldSparkInterpreter.java[populateSparkWebUrl]:931) - Sending metadata to Zeppelin server: {message=Spark UI enabled, url=http://zeppelin:4040}
 INFO [2020-02-19 07:58:16,780] ({pool-2-thread-9} Logging.scala[logInfo]:54) - Block broadcast_24 stored as values in memory (estimated size 238.8 KB, free 363.0 MB)
 INFO [2020-02-19 07:58:16,787] ({pool-2-thread-9} Logging.scala[logInfo]:54) - Block broadcast_24_piece0 stored as bytes in memory (estimated size 23.0 KB, free 363.0 MB)
 INFO [2020-02-19 07:58:16,789] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Added broadcast_24_piece0 in memory on zeppelin:44945 (size: 23.0 KB, free: 366.0 MB)
 INFO [2020-02-19 07:58:16,790] ({pool-2-thread-9} Logging.scala[logInfo]:54) - Created broadcast 24 from textFile at <console>:130
 INFO [2020-02-19 07:58:17,063] ({pool-2-thread-9} FileInputFormat.java[listStatus]:249) - Total input paths to process : 1
 INFO [2020-02-19 07:58:17,080] ({pool-2-thread-9} Logging.scala[logInfo]:54) - Starting job: first at <console>:131
 INFO [2020-02-19 07:58:17,081] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Got job 12 (first at <console>:131) with 1 output partitions
 INFO [2020-02-19 07:58:17,082] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Final stage: ResultStage 12 (first at <console>:131)
 INFO [2020-02-19 07:58:17,083] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Parents of final stage: List()
 INFO [2020-02-19 07:58:17,083] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Missing parents: List()
 INFO [2020-02-19 07:58:17,084] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting ResultStage 12 (file:///zeppelin/data/test.txt MapPartitionsRDD[50] at textFile at <console>:130), which has no missing parents
 INFO [2020-02-19 07:58:17,086] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_25 stored as values in memory (estimated size 3.5 KB, free 363.0 MB)
 INFO [2020-02-19 07:58:17,088] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_25_piece0 stored as bytes in memory (estimated size 2.0 KB, free 363.0 MB)
 INFO [2020-02-19 07:58:17,090] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Added broadcast_25_piece0 in memory on zeppelin:44945 (size: 2.0 KB, free: 366.0 MB)
 INFO [2020-02-19 07:58:17,090] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Created broadcast 25 from broadcast at DAGScheduler.scala:1161
 INFO [2020-02-19 07:58:17,092] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting 1 missing tasks from ResultStage 12 (file:///zeppelin/data/test.txt MapPartitionsRDD[50] at textFile at <console>:130) (first 15 tasks are for partitions Vector(0))
 INFO [2020-02-19 07:58:17,093] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Adding task set 12.0 with 1 tasks
 INFO [2020-02-19 07:58:17,094] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Added task set TaskSet_12.0 tasks to pool default
 INFO [2020-02-19 07:58:17,095] ({dispatcher-event-loop-2} Logging.scala[logInfo]:54) - Starting task 0.0 in stage 12.0 (TID 45, 172.27.0.7, executor 1, partition 0, PROCESS_LOCAL, 7892 bytes)
 INFO [2020-02-19 07:58:17,111] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Added broadcast_25_piece0 in memory on 172.27.0.7:42597 (size: 2.0 KB, free: 912.1 MB)
 INFO [2020-02-19 07:58:17,127] ({dispatcher-event-loop-3} Logging.scala[logInfo]:54) - Added broadcast_24_piece0 in memory on 172.27.0.7:42597 (size: 23.0 KB, free: 912.1 MB)
 WARN [2020-02-19 07:58:17,147] ({task-result-getter-2} Logging.scala[logWarning]:66) - Lost task 0.0 in stage 12.0 (TID 45, 172.27.0.7, executor 1): java.io.FileNotFoundException: File file:/zeppelin/data/test.txt does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:611)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:824)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:601)
	at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:421)
	at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:142)
	at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:346)
	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:769)
	at org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:109)
	at org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.liftedTree1$1(HadoopRDD.scala:267)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:266)
	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:224)
	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:95)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

 INFO [2020-02-19 07:58:17,149] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Starting task 0.1 in stage 12.0 (TID 46, 172.27.0.6, executor 0, partition 0, PROCESS_LOCAL, 7892 bytes)
 INFO [2020-02-19 07:58:17,177] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Added broadcast_25_piece0 in memory on 172.27.0.6:36245 (size: 2.0 KB, free: 912.1 MB)
 INFO [2020-02-19 07:58:17,215] ({dispatcher-event-loop-2} Logging.scala[logInfo]:54) - Added broadcast_24_piece0 in memory on 172.27.0.6:36245 (size: 23.0 KB, free: 912.1 MB)
 INFO [2020-02-19 07:58:17,247] ({task-result-getter-0} Logging.scala[logInfo]:54) - Lost task 0.1 in stage 12.0 (TID 46) on 172.27.0.6, executor 0: java.io.FileNotFoundException (File file:/zeppelin/data/test.txt does not exist) [duplicate 1]
 INFO [2020-02-19 07:58:17,249] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Starting task 0.2 in stage 12.0 (TID 47, 172.27.0.7, executor 1, partition 0, PROCESS_LOCAL, 7892 bytes)
 INFO [2020-02-19 07:58:17,265] ({task-result-getter-3} Logging.scala[logInfo]:54) - Lost task 0.2 in stage 12.0 (TID 47) on 172.27.0.7, executor 1: java.io.FileNotFoundException (File file:/zeppelin/data/test.txt does not exist) [duplicate 2]
 INFO [2020-02-19 07:58:17,269] ({dispatcher-event-loop-3} Logging.scala[logInfo]:54) - Starting task 0.3 in stage 12.0 (TID 48, 172.27.0.7, executor 1, partition 0, PROCESS_LOCAL, 7892 bytes)
 INFO [2020-02-19 07:58:17,291] ({task-result-getter-1} Logging.scala[logInfo]:54) - Lost task 0.3 in stage 12.0 (TID 48) on 172.27.0.7, executor 1: java.io.FileNotFoundException (File file:/zeppelin/data/test.txt does not exist) [duplicate 3]
ERROR [2020-02-19 07:58:17,292] ({task-result-getter-1} Logging.scala[logError]:70) - Task 0 in stage 12.0 failed 4 times; aborting job
 INFO [2020-02-19 07:58:17,294] ({task-result-getter-1} Logging.scala[logInfo]:54) - Removed TaskSet 12.0, whose tasks have all completed, from pool default
 INFO [2020-02-19 07:58:17,296] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Cancelling stage 12
 INFO [2020-02-19 07:58:17,298] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Killing all running tasks in stage 12: Stage cancelled
 INFO [2020-02-19 07:58:17,299] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - ResultStage 12 (first at <console>:131) failed in 0.214 s due to Job aborted due to stage failure: Task 0 in stage 12.0 failed 4 times, most recent failure: Lost task 0.3 in stage 12.0 (TID 48, 172.27.0.7, executor 1): java.io.FileNotFoundException: File file:/zeppelin/data/test.txt does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:611)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:824)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:601)
	at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:421)
	at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:142)
	at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:346)
	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:769)
	at org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:109)
	at org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.liftedTree1$1(HadoopRDD.scala:267)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:266)
	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:224)
	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:95)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
 INFO [2020-02-19 07:58:17,303] ({pool-2-thread-9} Logging.scala[logInfo]:54) - Job 12 failed: first at <console>:131, took 0.221936 s
 INFO [2020-02-19 07:58:18,015] ({pool-2-thread-9} SchedulerFactory.java[jobFinished]:120) - Job 20200219-065917_1853343063 finished by scheduler interpreter_299988080
 INFO [2020-02-19 07:59:17,499] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 311
 INFO [2020-02-19 07:59:17,501] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 326
 INFO [2020-02-19 07:59:17,501] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 316
 INFO [2020-02-19 07:59:17,502] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 322
 INFO [2020-02-19 07:59:17,503] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 315
 INFO [2020-02-19 07:59:17,503] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 328
 INFO [2020-02-19 07:59:17,505] ({dispatcher-event-loop-2} Logging.scala[logInfo]:54) - Removed broadcast_25_piece0 on zeppelin:44945 in memory (size: 2.0 KB, free: 366.0 MB)
 INFO [2020-02-19 07:59:17,509] ({dispatcher-event-loop-3} Logging.scala[logInfo]:54) - Removed broadcast_25_piece0 on 172.27.0.6:36245 in memory (size: 2.0 KB, free: 912.1 MB)
 INFO [2020-02-19 07:59:17,510] ({dispatcher-event-loop-3} Logging.scala[logInfo]:54) - Removed broadcast_25_piece0 on 172.27.0.7:42597 in memory (size: 2.0 KB, free: 912.1 MB)
 INFO [2020-02-19 07:59:17,515] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 313
 INFO [2020-02-19 07:59:17,516] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 318
 INFO [2020-02-19 07:59:17,516] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 306
 INFO [2020-02-19 07:59:17,517] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 327
 INFO [2020-02-19 07:59:17,517] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 319
 INFO [2020-02-19 07:59:17,517] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 324
 INFO [2020-02-19 07:59:17,518] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 317
 INFO [2020-02-19 07:59:17,519] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 308
 INFO [2020-02-19 07:59:17,519] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 305
 INFO [2020-02-19 07:59:17,520] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 307
 INFO [2020-02-19 07:59:17,521] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 314
 INFO [2020-02-19 07:59:17,521] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 309
 INFO [2020-02-19 07:59:17,522] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 310
 INFO [2020-02-19 07:59:17,523] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 320
 INFO [2020-02-19 07:59:17,523] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 312
 INFO [2020-02-19 07:59:17,524] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 325
 INFO [2020-02-19 07:59:17,524] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 323
 INFO [2020-02-19 07:59:17,525] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 329
 INFO [2020-02-19 07:59:17,525] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 321
 INFO [2020-02-19 07:59:53,265] ({pool-2-thread-17} SchedulerFactory.java[jobStarted]:114) - Job 20200219-065917_1853343063 started by scheduler interpreter_299988080
 INFO [2020-02-19 07:59:53,268] ({pool-2-thread-17} OldSparkInterpreter.java[populateSparkWebUrl]:931) - Sending metadata to Zeppelin server: {message=Spark UI enabled, url=http://zeppelin:4040}
 INFO [2020-02-19 07:59:56,140] ({pool-2-thread-17} Logging.scala[logInfo]:54) - Block broadcast_26 stored as values in memory (estimated size 238.8 KB, free 362.8 MB)
 INFO [2020-02-19 07:59:56,147] ({pool-2-thread-17} Logging.scala[logInfo]:54) - Block broadcast_26_piece0 stored as bytes in memory (estimated size 23.0 KB, free 362.7 MB)
 INFO [2020-02-19 07:59:56,149] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Added broadcast_26_piece0 in memory on zeppelin:44945 (size: 23.0 KB, free: 366.0 MB)
 INFO [2020-02-19 07:59:56,150] ({pool-2-thread-17} Logging.scala[logInfo]:54) - Created broadcast 26 from textFile at <console>:135
 INFO [2020-02-19 07:59:56,371] ({pool-2-thread-17} FileInputFormat.java[listStatus]:249) - Total input paths to process : 1
 INFO [2020-02-19 07:59:56,376] ({pool-2-thread-17} Logging.scala[logInfo]:54) - Starting job: first at <console>:136
 INFO [2020-02-19 07:59:56,377] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Got job 13 (first at <console>:136) with 1 output partitions
 INFO [2020-02-19 07:59:56,378] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Final stage: ResultStage 13 (first at <console>:136)
 INFO [2020-02-19 07:59:56,379] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Parents of final stage: List()
 INFO [2020-02-19 07:59:56,379] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Missing parents: List()
 INFO [2020-02-19 07:59:56,380] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting ResultStage 13 (file:///zeppelin/data/test.txt MapPartitionsRDD[52] at textFile at <console>:135), which has no missing parents
 INFO [2020-02-19 07:59:56,382] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_27 stored as values in memory (estimated size 3.5 KB, free 362.7 MB)
 INFO [2020-02-19 07:59:56,385] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_27_piece0 stored as bytes in memory (estimated size 2.0 KB, free 362.7 MB)
 INFO [2020-02-19 07:59:56,386] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Added broadcast_27_piece0 in memory on zeppelin:44945 (size: 2.0 KB, free: 366.0 MB)
 INFO [2020-02-19 07:59:56,387] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Created broadcast 27 from broadcast at DAGScheduler.scala:1161
 INFO [2020-02-19 07:59:56,388] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting 1 missing tasks from ResultStage 13 (file:///zeppelin/data/test.txt MapPartitionsRDD[52] at textFile at <console>:135) (first 15 tasks are for partitions Vector(0))
 INFO [2020-02-19 07:59:56,389] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Adding task set 13.0 with 1 tasks
 INFO [2020-02-19 07:59:56,390] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Added task set TaskSet_13.0 tasks to pool default
 INFO [2020-02-19 07:59:56,392] ({dispatcher-event-loop-2} Logging.scala[logInfo]:54) - Starting task 0.0 in stage 13.0 (TID 49, 172.27.0.6, executor 0, partition 0, PROCESS_LOCAL, 7892 bytes)
 INFO [2020-02-19 07:59:56,409] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Added broadcast_27_piece0 in memory on 172.27.0.6:36245 (size: 2.0 KB, free: 912.1 MB)
 INFO [2020-02-19 07:59:56,422] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Added broadcast_26_piece0 in memory on 172.27.0.6:36245 (size: 23.0 KB, free: 912.1 MB)
 WARN [2020-02-19 07:59:56,442] ({task-result-getter-2} Logging.scala[logWarning]:66) - Lost task 0.0 in stage 13.0 (TID 49, 172.27.0.6, executor 0): java.io.FileNotFoundException: File file:/zeppelin/data/test.txt does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:611)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:824)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:601)
	at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:421)
	at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:142)
	at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:346)
	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:769)
	at org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:109)
	at org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.liftedTree1$1(HadoopRDD.scala:267)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:266)
	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:224)
	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:95)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

 INFO [2020-02-19 07:59:56,443] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Starting task 0.1 in stage 13.0 (TID 50, 172.27.0.6, executor 0, partition 0, PROCESS_LOCAL, 7892 bytes)
 INFO [2020-02-19 07:59:56,452] ({task-result-getter-0} Logging.scala[logInfo]:54) - Lost task 0.1 in stage 13.0 (TID 50) on 172.27.0.6, executor 0: java.io.FileNotFoundException (File file:/zeppelin/data/test.txt does not exist) [duplicate 1]
 INFO [2020-02-19 07:59:56,454] ({dispatcher-event-loop-3} Logging.scala[logInfo]:54) - Starting task 0.2 in stage 13.0 (TID 51, 172.27.0.6, executor 0, partition 0, PROCESS_LOCAL, 7892 bytes)
 INFO [2020-02-19 07:59:56,464] ({task-result-getter-3} Logging.scala[logInfo]:54) - Lost task 0.2 in stage 13.0 (TID 51) on 172.27.0.6, executor 0: java.io.FileNotFoundException (File file:/zeppelin/data/test.txt does not exist) [duplicate 2]
 INFO [2020-02-19 07:59:56,465] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Starting task 0.3 in stage 13.0 (TID 52, 172.27.0.6, executor 0, partition 0, PROCESS_LOCAL, 7892 bytes)
 INFO [2020-02-19 07:59:56,475] ({task-result-getter-1} Logging.scala[logInfo]:54) - Lost task 0.3 in stage 13.0 (TID 52) on 172.27.0.6, executor 0: java.io.FileNotFoundException (File file:/zeppelin/data/test.txt does not exist) [duplicate 3]
ERROR [2020-02-19 07:59:56,476] ({task-result-getter-1} Logging.scala[logError]:70) - Task 0 in stage 13.0 failed 4 times; aborting job
 INFO [2020-02-19 07:59:56,477] ({task-result-getter-1} Logging.scala[logInfo]:54) - Removed TaskSet 13.0, whose tasks have all completed, from pool default
 INFO [2020-02-19 07:59:56,478] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Cancelling stage 13
 INFO [2020-02-19 07:59:56,479] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Killing all running tasks in stage 13: Stage cancelled
 INFO [2020-02-19 07:59:56,480] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - ResultStage 13 (first at <console>:136) failed in 0.099 s due to Job aborted due to stage failure: Task 0 in stage 13.0 failed 4 times, most recent failure: Lost task 0.3 in stage 13.0 (TID 52, 172.27.0.6, executor 0): java.io.FileNotFoundException: File file:/zeppelin/data/test.txt does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:611)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:824)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:601)
	at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:421)
	at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:142)
	at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:346)
	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:769)
	at org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:109)
	at org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.liftedTree1$1(HadoopRDD.scala:267)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:266)
	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:224)
	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:95)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
 INFO [2020-02-19 07:59:56,482] ({pool-2-thread-17} Logging.scala[logInfo]:54) - Job 13 failed: first at <console>:136, took 0.104698 s
 INFO [2020-02-19 07:59:56,793] ({pool-2-thread-17} SchedulerFactory.java[jobFinished]:120) - Job 20200219-065917_1853343063 finished by scheduler interpreter_299988080
ERROR [2020-02-19 08:01:31,229] ({dispatcher-event-loop-2} Logging.scala[logError]:70) - Lost executor 1 on 172.27.0.7: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
ERROR [2020-02-19 08:01:31,270] ({dispatcher-event-loop-2} Logging.scala[logError]:70) - Lost executor 0 on 172.27.0.6: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
 INFO [2020-02-19 08:01:31,283] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Executor lost: 1 (epoch 0)
 INFO [2020-02-19 08:01:31,290] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Trying to remove executor 1 from BlockManagerMaster.
 INFO [2020-02-19 08:01:31,296] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Removing block manager BlockManagerId(1, 172.27.0.7, 42597, None)
 INFO [2020-02-19 08:01:31,298] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Removed 1 successfully in removeExecutor
 INFO [2020-02-19 08:01:31,301] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Shuffle files lost for executor: 1 (epoch 0)
 INFO [2020-02-19 08:01:31,307] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Executor lost: 0 (epoch 1)
 INFO [2020-02-19 08:01:31,308] ({dispatcher-event-loop-3} Logging.scala[logInfo]:54) - Trying to remove executor 0 from BlockManagerMaster.
 INFO [2020-02-19 08:01:31,310] ({dispatcher-event-loop-3} Logging.scala[logInfo]:54) - Removing block manager BlockManagerId(0, 172.27.0.6, 36245, None)
 INFO [2020-02-19 08:01:31,310] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Removed 0 successfully in removeExecutor
 INFO [2020-02-19 08:01:31,311] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Shuffle files lost for executor: 0 (epoch 1)
 INFO [2020-02-19 08:01:32,027] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Executor updated: app-20200219072915-0000/0 is now LOST (worker lost)
 INFO [2020-02-19 08:01:32,051] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Executor app-20200219072915-0000/0 removed: worker lost
 INFO [2020-02-19 08:01:32,069] ({dispatcher-event-loop-2} Logging.scala[logInfo]:54) - Trying to remove executor 0 from BlockManagerMaster.
 INFO [2020-02-19 08:01:32,074] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Removal of executor 0 requested
 INFO [2020-02-19 08:01:32,077] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Asked to remove non-existent executor 0
 INFO [2020-02-19 08:01:32,146] ({dispatcher-event-loop-3} Logging.scala[logInfo]:54) - Master removed worker worker-20200219072825-172.27.0.6-44371: 172.27.0.6:44371 got disassociated
 INFO [2020-02-19 08:01:32,149] ({dispatcher-event-loop-3} Logging.scala[logInfo]:54) - Worker worker-20200219072825-172.27.0.6-44371 removed: 172.27.0.6:44371 got disassociated
 INFO [2020-02-19 08:01:32,153] ({dispatcher-event-loop-3} Logging.scala[logInfo]:54) - Executor updated: app-20200219072915-0000/1 is now LOST (worker lost)
 INFO [2020-02-19 08:01:32,156] ({dispatcher-event-loop-3} Logging.scala[logInfo]:54) - Executor app-20200219072915-0000/1 removed: worker lost
 INFO [2020-02-19 08:01:32,158] ({dispatcher-event-loop-3} Logging.scala[logInfo]:54) - Master removed worker worker-20200219072825-172.27.0.7-41643: 172.27.0.7:41643 got disassociated
 INFO [2020-02-19 08:01:32,159] ({dispatcher-event-loop-3} Logging.scala[logInfo]:54) - Worker worker-20200219072825-172.27.0.7-41643 removed: 172.27.0.7:41643 got disassociated
 INFO [2020-02-19 08:01:32,158] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Handle removed worker worker-20200219072825-172.27.0.6-44371: 172.27.0.6:44371 got disassociated
 INFO [2020-02-19 08:01:32,162] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Removal of executor 1 requested
 INFO [2020-02-19 08:01:32,164] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Trying to remove executor 1 from BlockManagerMaster.
 INFO [2020-02-19 08:01:32,168] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Shuffle files lost for worker worker-20200219072825-172.27.0.6-44371 on host 172.27.0.6
 INFO [2020-02-19 08:01:32,167] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Asked to remove non-existent executor 1
 INFO [2020-02-19 08:01:32,177] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Handle removed worker worker-20200219072825-172.27.0.7-41643: 172.27.0.7:41643 got disassociated
 INFO [2020-02-19 08:01:32,178] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Shuffle files lost for worker worker-20200219072825-172.27.0.7-41643 on host 172.27.0.7
 WARN [2020-02-19 08:01:42,249] ({dispatcher-event-loop-1} Logging.scala[logWarning]:66) - Connection to spark-master:7077 failed; waiting for master to reconnect...
 WARN [2020-02-19 08:01:42,251] ({dispatcher-event-loop-1} Logging.scala[logWarning]:66) - Disconnected from Spark cluster! Waiting for reconnection...
 WARN [2020-02-19 08:01:42,252] ({dispatcher-event-loop-1} Logging.scala[logWarning]:66) - Connection to spark-master:7077 failed; waiting for master to reconnect...
 INFO [2020-02-19 08:01:47,243] ({pool-1-thread-1} OldSparkInterpreter.java[close]:1243) - Close interpreter
 INFO [2020-02-19 08:01:47,261] ({pool-1-thread-1} AbstractConnector.java[doStop]:318) - Stopped Spark@536a4302{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
 INFO [2020-02-19 08:01:47,266] ({pool-1-thread-1} Logging.scala[logInfo]:54) - Stopped Spark web UI at http://zeppelin:4040
 INFO [2020-02-19 08:01:47,277] ({pool-1-thread-1} Logging.scala[logInfo]:54) - Shutting down all executors
 INFO [2020-02-19 08:01:47,278] ({dispatcher-event-loop-3} Logging.scala[logInfo]:54) - Asking each executor to shut down
 INFO [2020-02-19 08:01:47,308] ({dispatcher-event-loop-2} Logging.scala[logInfo]:54) - MapOutputTrackerMasterEndpoint stopped!
 INFO [2020-02-19 08:01:47,437] ({pool-1-thread-1} Logging.scala[logInfo]:54) - MemoryStore cleared
 INFO [2020-02-19 08:01:47,438] ({pool-1-thread-1} Logging.scala[logInfo]:54) - BlockManager stopped
 INFO [2020-02-19 08:01:47,441] ({pool-1-thread-1} Logging.scala[logInfo]:54) - BlockManagerMaster stopped
 INFO [2020-02-19 08:01:47,456] ({dispatcher-event-loop-2} Logging.scala[logInfo]:54) - OutputCommitCoordinator stopped!
 INFO [2020-02-19 08:01:47,488] ({pool-1-thread-1} Logging.scala[logInfo]:54) - Successfully stopped SparkContext
 INFO [2020-02-19 08:01:47,522] ({pool-1-thread-1} RemoteInterpreterServer.java[shutdown]:209) - Shutting down...
 INFO [2020-02-19 08:01:49,677] ({Thread-1} Logging.scala[logInfo]:54) - Shutdown hook called
 INFO [2020-02-19 08:01:49,724] ({Thread-1} Logging.scala[logInfo]:54) - Deleting directory /tmp/spark-78f000ec-6ac3-41b2-8234-df50f5c6eaec
 INFO [2020-02-19 08:01:49,735] ({Thread-1} Logging.scala[logInfo]:54) - Deleting directory /tmp/spark-2aa612a3-e498-4d77-bd0d-70aa66c5918e
 INFO [2020-02-19 08:01:50,518] ({Thread-1} Logging.scala[logInfo]:54) - Deleting directory /tmp/spark-6073434b-d537-438a-92ff-63818af2d188
 WARN [2020-02-19 08:08:50,689] ({main} NativeCodeLoader.java[<clinit>]:62) - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
 INFO [2020-02-19 08:08:51,230] ({main} RemoteInterpreterServer.java[main]:261) - URL:jar:file:/zeppelin/interpreter/spark/spark-interpreter-0.8.2.jar!/org/apache/zeppelin/interpreter/remote/RemoteInterpreterServer.class
 INFO [2020-02-19 08:08:51,287] ({main} RemoteInterpreterServer.java[<init>]:162) - Launching ThriftServer at 172.30.0.3:43065
 INFO [2020-02-19 08:08:51,294] ({main} RemoteInterpreterServer.java[<init>]:166) - Starting remote interpreter server on port 43065
 INFO [2020-02-19 08:08:51,296] ({Thread-3} RemoteInterpreterServer.java[run]:203) - Starting remote interpreter server on port 43065
 INFO [2020-02-19 08:08:52,329] ({Thread-4} RemoteInterpreterUtils.java[registerInterpreter]:165) - callbackHost: 172.30.0.3, callbackPort: 43571, callbackInfo: CallbackInfo(host:172.30.0.3, port:43065)
 INFO [2020-02-19 08:08:52,930] ({pool-1-thread-1} RemoteInterpreterServer.java[createInterpreter]:311) - Instantiate interpreter org.apache.zeppelin.spark.SparkInterpreter
 INFO [2020-02-19 08:08:52,936] ({pool-1-thread-1} RemoteInterpreterServer.java[createInterpreter]:311) - Instantiate interpreter org.apache.zeppelin.spark.SparkSqlInterpreter
 INFO [2020-02-19 08:08:52,942] ({pool-1-thread-1} RemoteInterpreterServer.java[createInterpreter]:311) - Instantiate interpreter org.apache.zeppelin.spark.DepInterpreter
 INFO [2020-02-19 08:08:52,953] ({pool-1-thread-1} RemoteInterpreterServer.java[createInterpreter]:311) - Instantiate interpreter org.apache.zeppelin.spark.PySparkInterpreter
 INFO [2020-02-19 08:08:52,965] ({pool-1-thread-1} RemoteInterpreterServer.java[createInterpreter]:311) - Instantiate interpreter org.apache.zeppelin.spark.IPySparkInterpreter
 INFO [2020-02-19 08:08:52,969] ({pool-1-thread-1} RemoteInterpreterServer.java[createInterpreter]:311) - Instantiate interpreter org.apache.zeppelin.spark.SparkRInterpreter
 WARN [2020-02-19 08:08:53,152] ({pool-1-thread-1} ZeppelinConfiguration.java[create]:117) - Failed to load configuration, proceeding with a default
 INFO [2020-02-19 08:08:53,185] ({pool-1-thread-1} ZeppelinConfiguration.java[create]:129) - Server Host: 0.0.0.0
 INFO [2020-02-19 08:08:53,187] ({pool-1-thread-1} ZeppelinConfiguration.java[create]:131) - Server Port: 8080
 INFO [2020-02-19 08:08:53,188] ({pool-1-thread-1} ZeppelinConfiguration.java[create]:135) - Context Path: /
 INFO [2020-02-19 08:08:53,194] ({pool-1-thread-1} ZeppelinConfiguration.java[create]:136) - Zeppelin Version: 0.8.2
 INFO [2020-02-19 08:08:53,195] ({pool-1-thread-1} SchedulerFactory.java[<init>]:59) - Scheduler Thread Pool Size: 100
 INFO [2020-02-19 08:08:53,201] ({pool-2-thread-2} SchedulerFactory.java[jobStarted]:114) - Job 20200219-065917_1853343063 started by scheduler interpreter_1821642557
 INFO [2020-02-19 08:08:58,237] ({pool-2-thread-2} OldSparkInterpreter.java[createSparkSession]:311) - ------ Create new SparkSession spark://spark-master:7077 -------
 INFO [2020-02-19 08:08:58,584] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Running Spark version 2.4.0
 INFO [2020-02-19 08:08:58,622] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Submitted application: Zeppelin
 INFO [2020-02-19 08:08:58,729] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Changing view acls to: root
 INFO [2020-02-19 08:08:58,730] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Changing modify acls to: root
 INFO [2020-02-19 08:08:58,731] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Changing view acls groups to: 
 INFO [2020-02-19 08:08:58,732] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Changing modify acls groups to: 
 INFO [2020-02-19 08:08:58,733] ({pool-2-thread-2} Logging.scala[logInfo]:54) - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
 INFO [2020-02-19 08:08:59,141] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Successfully started service 'sparkDriver' on port 40477.
 INFO [2020-02-19 08:08:59,190] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Registering MapOutputTracker
 INFO [2020-02-19 08:08:59,234] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Registering BlockManagerMaster
 INFO [2020-02-19 08:08:59,242] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
 INFO [2020-02-19 08:08:59,244] ({pool-2-thread-2} Logging.scala[logInfo]:54) - BlockManagerMasterEndpoint up
 INFO [2020-02-19 08:08:59,264] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Created local directory at /tmp/blockmgr-9b11dee4-5e42-42ff-980a-d90d2c5ba382
 INFO [2020-02-19 08:08:59,288] ({pool-2-thread-2} Logging.scala[logInfo]:54) - MemoryStore started with capacity 366.3 MB
 INFO [2020-02-19 08:08:59,318] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Registering OutputCommitCoordinator
 INFO [2020-02-19 08:08:59,485] ({pool-2-thread-2} Log.java[initialized]:192) - Logging initialized @10998ms
 INFO [2020-02-19 08:08:59,607] ({pool-2-thread-2} Server.java[doStart]:351) - jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
 INFO [2020-02-19 08:08:59,632] ({pool-2-thread-2} Server.java[doStart]:419) - Started @11145ms
 INFO [2020-02-19 08:08:59,660] ({pool-2-thread-2} AbstractConnector.java[doStart]:278) - Started ServerConnector@5d21bf57{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
 INFO [2020-02-19 08:08:59,661] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Successfully started service 'SparkUI' on port 4040.
 INFO [2020-02-19 08:08:59,693] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@117c25fe{/jobs,null,AVAILABLE,@Spark}
 INFO [2020-02-19 08:08:59,697] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@6319ef70{/jobs/json,null,AVAILABLE,@Spark}
 INFO [2020-02-19 08:08:59,699] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@70d40a39{/jobs/job,null,AVAILABLE,@Spark}
 INFO [2020-02-19 08:08:59,702] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@1862d272{/jobs/job/json,null,AVAILABLE,@Spark}
 INFO [2020-02-19 08:08:59,704] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@342ccc61{/stages,null,AVAILABLE,@Spark}
 INFO [2020-02-19 08:08:59,705] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@64f99d1f{/stages/json,null,AVAILABLE,@Spark}
 INFO [2020-02-19 08:08:59,707] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@362da8d5{/stages/stage,null,AVAILABLE,@Spark}
 INFO [2020-02-19 08:08:59,709] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@42f6e8ac{/stages/stage/json,null,AVAILABLE,@Spark}
 INFO [2020-02-19 08:08:59,713] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@19469b27{/stages/pool,null,AVAILABLE,@Spark}
 INFO [2020-02-19 08:08:59,716] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@31d5ae22{/stages/pool/json,null,AVAILABLE,@Spark}
 INFO [2020-02-19 08:08:59,719] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@2780826e{/storage,null,AVAILABLE,@Spark}
 INFO [2020-02-19 08:08:59,721] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@5246bdba{/storage/json,null,AVAILABLE,@Spark}
 INFO [2020-02-19 08:08:59,722] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@7ff48b76{/storage/rdd,null,AVAILABLE,@Spark}
 INFO [2020-02-19 08:08:59,724] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@634c9e14{/storage/rdd/json,null,AVAILABLE,@Spark}
 INFO [2020-02-19 08:08:59,726] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@40fcf3e1{/environment,null,AVAILABLE,@Spark}
 INFO [2020-02-19 08:08:59,729] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@27303d29{/environment/json,null,AVAILABLE,@Spark}
 INFO [2020-02-19 08:08:59,731] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@7c44ad82{/executors,null,AVAILABLE,@Spark}
 INFO [2020-02-19 08:08:59,733] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@7bff3020{/executors/json,null,AVAILABLE,@Spark}
 INFO [2020-02-19 08:08:59,734] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@5c3bb745{/executors/threadDump,null,AVAILABLE,@Spark}
 INFO [2020-02-19 08:08:59,736] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@7a44109e{/executors/threadDump/json,null,AVAILABLE,@Spark}
 INFO [2020-02-19 08:08:59,742] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@23b59ae9{/static,null,AVAILABLE,@Spark}
 INFO [2020-02-19 08:08:59,744] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@1c86e498{/,null,AVAILABLE,@Spark}
 INFO [2020-02-19 08:08:59,748] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@52eb6ec5{/api,null,AVAILABLE,@Spark}
 INFO [2020-02-19 08:08:59,750] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@7533a0fe{/jobs/job/kill,null,AVAILABLE,@Spark}
 INFO [2020-02-19 08:08:59,751] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@4517d93e{/stages/stage/kill,null,AVAILABLE,@Spark}
 INFO [2020-02-19 08:08:59,755] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Bound SparkUI to 0.0.0.0, and started at http://zeppelin:4040
 INFO [2020-02-19 08:08:59,789] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Added JAR file:/zeppelin/interpreter/spark/spark-interpreter-0.8.2.jar at spark://zeppelin:40477/jars/spark-interpreter-0.8.2.jar with timestamp 1582099739789
 WARN [2020-02-19 08:08:59,884] ({pool-2-thread-2} Logging.scala[logWarning]:66) - Fair Scheduler configuration file not found so jobs will be scheduled in FIFO order. To use fair scheduling, configure pools in fairscheduler.xml or set spark.scheduler.allocation.file to a file that contains the configuration.
 INFO [2020-02-19 08:08:59,898] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Created default pool: default, schedulingMode: FIFO, minShare: 0, weight: 1
 INFO [2020-02-19 08:08:59,973] ({appclient-register-master-threadpool-0} Logging.scala[logInfo]:54) - Connecting to master spark://spark-master:7077...
 INFO [2020-02-19 08:09:00,029] ({netty-rpc-connection-0} TransportClientFactory.java[createClient]:267) - Successfully created connection to spark-master/172.30.0.5:7077 after 33 ms (0 ms spent in bootstraps)
 INFO [2020-02-19 08:09:00,180] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Connected to Spark cluster with app ID app-20200219080900-0000
 INFO [2020-02-19 08:09:00,191] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44103.
 INFO [2020-02-19 08:09:00,193] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Server created on zeppelin:44103
 INFO [2020-02-19 08:09:00,196] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
 INFO [2020-02-19 08:09:00,231] ({dispatcher-event-loop-2} Logging.scala[logInfo]:54) - Executor added: app-20200219080900-0000/0 on worker-20200219080753-172.30.0.6-34229 (172.30.0.6:34229) with 4 core(s)
 INFO [2020-02-19 08:09:00,241] ({dispatcher-event-loop-2} Logging.scala[logInfo]:54) - Granted executor ID app-20200219080900-0000/0 on hostPort 172.30.0.6:34229 with 4 core(s), 2.0 GB RAM
 INFO [2020-02-19 08:09:00,246] ({dispatcher-event-loop-2} Logging.scala[logInfo]:54) - Executor added: app-20200219080900-0000/1 on worker-20200219080753-172.30.0.7-37349 (172.30.0.7:37349) with 4 core(s)
 INFO [2020-02-19 08:09:00,247] ({dispatcher-event-loop-2} Logging.scala[logInfo]:54) - Granted executor ID app-20200219080900-0000/1 on hostPort 172.30.0.7:37349 with 4 core(s), 2.0 GB RAM
 INFO [2020-02-19 08:09:00,319] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Registering BlockManager BlockManagerId(driver, zeppelin, 44103, None)
 INFO [2020-02-19 08:09:00,331] ({dispatcher-event-loop-3} Logging.scala[logInfo]:54) - Registering block manager zeppelin:44103 with 366.3 MB RAM, BlockManagerId(driver, zeppelin, 44103, None)
 INFO [2020-02-19 08:09:00,351] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Registered BlockManager BlockManagerId(driver, zeppelin, 44103, None)
 INFO [2020-02-19 08:09:00,353] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Initialized BlockManager: BlockManagerId(driver, zeppelin, 44103, None)
 INFO [2020-02-19 08:09:00,495] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Executor updated: app-20200219080900-0000/1 is now RUNNING
 INFO [2020-02-19 08:09:00,505] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Executor updated: app-20200219080900-0000/0 is now RUNNING
 INFO [2020-02-19 08:09:00,862] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@2b936e77{/metrics/json,null,AVAILABLE,@Spark}
 INFO [2020-02-19 08:09:00,918] ({pool-2-thread-2} Logging.scala[logInfo]:54) - SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
 INFO [2020-02-19 08:09:01,002] ({pool-2-thread-2} OldSparkInterpreter.java[createSparkSession]:347) - Created Spark session with Hive support
 INFO [2020-02-19 08:09:06,483] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.30.0.7:54178) with ID 1
 INFO [2020-02-19 08:09:06,524] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.30.0.6:43518) with ID 0
 INFO [2020-02-19 08:09:07,142] ({dispatcher-event-loop-3} Logging.scala[logInfo]:54) - Registering block manager 172.30.0.6:41599 with 912.3 MB RAM, BlockManagerId(0, 172.30.0.6, 41599, None)
 INFO [2020-02-19 08:09:07,162] ({dispatcher-event-loop-3} Logging.scala[logInfo]:54) - Registering block manager 172.30.0.7:37775 with 912.3 MB RAM, BlockManagerId(1, 172.30.0.7, 37775, None)
 INFO [2020-02-19 08:09:13,364] ({pool-2-thread-2} SparkShims.java[loadShims]:62) - Initializing shims for Spark 2.x
 INFO [2020-02-19 08:09:13,371] ({pool-2-thread-2} OldSparkInterpreter.java[populateSparkWebUrl]:931) - Sending metadata to Zeppelin server: {message=Spark UI enabled, url=http://zeppelin:4040}
 INFO [2020-02-19 08:09:19,190] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/zeppelin/spark-warehouse').
 INFO [2020-02-19 08:09:19,208] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Warehouse path is 'file:/zeppelin/spark-warehouse'.
 INFO [2020-02-19 08:09:19,356] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@407906eb{/SQL,null,AVAILABLE,@Spark}
 INFO [2020-02-19 08:09:19,372] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@7e4dc93d{/SQL/json,null,AVAILABLE,@Spark}
 INFO [2020-02-19 08:09:19,388] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@4cfacfca{/SQL/execution,null,AVAILABLE,@Spark}
 INFO [2020-02-19 08:09:19,405] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@4b128269{/SQL/execution/json,null,AVAILABLE,@Spark}
 INFO [2020-02-19 08:09:19,412] ({pool-2-thread-2} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@781fa7c5{/static/sql,null,AVAILABLE,@Spark}
 INFO [2020-02-19 08:09:20,548] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Registered StateStoreCoordinator endpoint
 INFO [2020-02-19 08:09:21,086] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Code generated in 457.773123 ms
 INFO [2020-02-19 08:09:23,175] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Block broadcast_0 stored as values in memory (estimated size 238.8 KB, free 366.1 MB)
 INFO [2020-02-19 08:09:23,301] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Block broadcast_0_piece0 stored as bytes in memory (estimated size 23.0 KB, free 366.0 MB)
 INFO [2020-02-19 08:09:23,305] ({dispatcher-event-loop-2} Logging.scala[logInfo]:54) - Added broadcast_0_piece0 in memory on zeppelin:44103 (size: 23.0 KB, free: 366.3 MB)
 INFO [2020-02-19 08:09:23,315] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Created broadcast 0 from textFile at <console>:30
 INFO [2020-02-19 08:09:23,877] ({pool-2-thread-2} FileInputFormat.java[listStatus]:249) - Total input paths to process : 1
 INFO [2020-02-19 08:09:23,966] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Starting job: first at <console>:31
 INFO [2020-02-19 08:09:23,998] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Got job 0 (first at <console>:31) with 1 output partitions
 INFO [2020-02-19 08:09:24,000] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Final stage: ResultStage 0 (first at <console>:31)
 INFO [2020-02-19 08:09:24,002] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Parents of final stage: List()
 INFO [2020-02-19 08:09:24,008] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Missing parents: List()
 INFO [2020-02-19 08:09:24,018] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting ResultStage 0 (file:///zeppelin/data/test.txt MapPartitionsRDD[1] at textFile at <console>:30), which has no missing parents
 INFO [2020-02-19 08:09:24,051] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_1 stored as values in memory (estimated size 3.5 KB, free 366.0 MB)
 INFO [2020-02-19 08:09:24,056] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.1 KB, free 366.0 MB)
 INFO [2020-02-19 08:09:24,058] ({dispatcher-event-loop-3} Logging.scala[logInfo]:54) - Added broadcast_1_piece0 in memory on zeppelin:44103 (size: 2.1 KB, free: 366.3 MB)
 INFO [2020-02-19 08:09:24,061] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Created broadcast 1 from broadcast at DAGScheduler.scala:1161
 INFO [2020-02-19 08:09:24,093] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting 1 missing tasks from ResultStage 0 (file:///zeppelin/data/test.txt MapPartitionsRDD[1] at textFile at <console>:30) (first 15 tasks are for partitions Vector(0))
 INFO [2020-02-19 08:09:24,097] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Adding task set 0.0 with 1 tasks
 INFO [2020-02-19 08:09:24,130] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Added task set TaskSet_0.0 tasks to pool default
 INFO [2020-02-19 08:09:24,165] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Starting task 0.0 in stage 0.0 (TID 0, 172.30.0.6, executor 0, partition 0, PROCESS_LOCAL, 7892 bytes)
 INFO [2020-02-19 08:09:25,215] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Added broadcast_1_piece0 in memory on 172.30.0.6:41599 (size: 2.1 KB, free: 912.3 MB)
 INFO [2020-02-19 08:09:25,422] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Added broadcast_0_piece0 in memory on 172.30.0.6:41599 (size: 23.0 KB, free: 912.3 MB)
 WARN [2020-02-19 08:09:25,939] ({task-result-getter-0} Logging.scala[logWarning]:66) - Lost task 0.0 in stage 0.0 (TID 0, 172.30.0.6, executor 0): java.io.FileNotFoundException: File file:/zeppelin/data/test.txt does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:611)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:824)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:601)
	at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:421)
	at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:142)
	at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:346)
	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:769)
	at org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:109)
	at org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.liftedTree1$1(HadoopRDD.scala:267)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:266)
	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:224)
	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:95)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

 INFO [2020-02-19 08:09:25,944] ({dispatcher-event-loop-2} Logging.scala[logInfo]:54) - Starting task 0.1 in stage 0.0 (TID 1, 172.30.0.6, executor 0, partition 0, PROCESS_LOCAL, 7892 bytes)
 INFO [2020-02-19 08:09:26,004] ({task-result-getter-1} Logging.scala[logInfo]:54) - Lost task 0.1 in stage 0.0 (TID 1) on 172.30.0.6, executor 0: java.io.FileNotFoundException (File file:/zeppelin/data/test.txt does not exist) [duplicate 1]
 INFO [2020-02-19 08:09:26,006] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Starting task 0.2 in stage 0.0 (TID 2, 172.30.0.6, executor 0, partition 0, PROCESS_LOCAL, 7892 bytes)
 INFO [2020-02-19 08:09:26,035] ({task-result-getter-2} Logging.scala[logInfo]:54) - Lost task 0.2 in stage 0.0 (TID 2) on 172.30.0.6, executor 0: java.io.FileNotFoundException (File file:/zeppelin/data/test.txt does not exist) [duplicate 2]
 INFO [2020-02-19 08:09:26,037] ({dispatcher-event-loop-3} Logging.scala[logInfo]:54) - Starting task 0.3 in stage 0.0 (TID 3, 172.30.0.7, executor 1, partition 0, PROCESS_LOCAL, 7892 bytes)
 INFO [2020-02-19 08:09:26,717] ({dispatcher-event-loop-2} Logging.scala[logInfo]:54) - Added broadcast_1_piece0 in memory on 172.30.0.7:37775 (size: 2.1 KB, free: 912.3 MB)
 INFO [2020-02-19 08:09:26,914] ({dispatcher-event-loop-3} Logging.scala[logInfo]:54) - Added broadcast_0_piece0 in memory on 172.30.0.7:37775 (size: 23.0 KB, free: 912.3 MB)
 INFO [2020-02-19 08:09:27,529] ({task-result-getter-3} Logging.scala[logInfo]:54) - Lost task 0.3 in stage 0.0 (TID 3) on 172.30.0.7, executor 1: java.io.FileNotFoundException (File file:/zeppelin/data/test.txt does not exist) [duplicate 3]
ERROR [2020-02-19 08:09:27,532] ({task-result-getter-3} Logging.scala[logError]:70) - Task 0 in stage 0.0 failed 4 times; aborting job
 INFO [2020-02-19 08:09:27,537] ({task-result-getter-3} Logging.scala[logInfo]:54) - Removed TaskSet 0.0, whose tasks have all completed, from pool default
 INFO [2020-02-19 08:09:27,540] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Cancelling stage 0
 INFO [2020-02-19 08:09:27,542] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Killing all running tasks in stage 0: Stage cancelled
 INFO [2020-02-19 08:09:27,550] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - ResultStage 0 (first at <console>:31) failed in 3.500 s due to Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 3, 172.30.0.7, executor 1): java.io.FileNotFoundException: File file:/zeppelin/data/test.txt does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:611)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:824)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:601)
	at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:421)
	at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:142)
	at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:346)
	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:769)
	at org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:109)
	at org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.liftedTree1$1(HadoopRDD.scala:267)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:266)
	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:224)
	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:95)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
 INFO [2020-02-19 08:09:27,556] ({pool-2-thread-2} Logging.scala[logInfo]:54) - Job 0 failed: first at <console>:31, took 3.588951 s
 INFO [2020-02-19 08:09:28,013] ({pool-2-thread-2} SchedulerFactory.java[jobFinished]:120) - Job 20200219-065917_1853343063 finished by scheduler interpreter_1821642557
 INFO [2020-02-19 08:10:33,506] ({pool-2-thread-3} SchedulerFactory.java[jobStarted]:114) - Job 20200219-065917_1853343063 started by scheduler interpreter_1821642557
 INFO [2020-02-19 08:10:33,517] ({pool-2-thread-3} OldSparkInterpreter.java[populateSparkWebUrl]:931) - Sending metadata to Zeppelin server: {message=Spark UI enabled, url=http://zeppelin:4040}
 INFO [2020-02-19 08:10:34,009] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 4
 INFO [2020-02-19 08:10:34,010] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 18
 INFO [2020-02-19 08:10:34,011] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 21
 INFO [2020-02-19 08:10:34,012] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 1
 INFO [2020-02-19 08:10:34,014] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 16
 INFO [2020-02-19 08:10:34,015] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 14
 INFO [2020-02-19 08:10:34,016] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 13
 INFO [2020-02-19 08:10:34,017] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 6
 INFO [2020-02-19 08:10:34,024] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 3
 INFO [2020-02-19 08:10:34,024] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 22
 INFO [2020-02-19 08:10:34,025] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 24
 INFO [2020-02-19 08:10:34,025] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 12
 INFO [2020-02-19 08:10:34,026] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 17
 INFO [2020-02-19 08:10:34,027] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 19
 INFO [2020-02-19 08:10:34,027] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 8
 INFO [2020-02-19 08:10:34,028] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 10
 INFO [2020-02-19 08:10:34,028] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 23
 INFO [2020-02-19 08:10:34,029] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 15
 INFO [2020-02-19 08:10:34,030] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 2
 INFO [2020-02-19 08:10:34,030] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 5
 INFO [2020-02-19 08:10:34,160] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Removed broadcast_1_piece0 on 172.30.0.7:37775 in memory (size: 2.1 KB, free: 912.3 MB)
 INFO [2020-02-19 08:10:34,193] ({dispatcher-event-loop-3} Logging.scala[logInfo]:54) - Removed broadcast_1_piece0 on zeppelin:44103 in memory (size: 2.1 KB, free: 366.3 MB)
 INFO [2020-02-19 08:10:34,340] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Removed broadcast_1_piece0 on 172.30.0.6:41599 in memory (size: 2.1 KB, free: 912.3 MB)
 INFO [2020-02-19 08:10:34,376] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 11
 INFO [2020-02-19 08:10:34,377] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 20
 INFO [2020-02-19 08:10:34,378] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 9
 INFO [2020-02-19 08:10:34,382] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 0
 INFO [2020-02-19 08:10:34,383] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 7
 INFO [2020-02-19 08:10:37,803] ({pool-2-thread-3} Logging.scala[logInfo]:54) - Block broadcast_2 stored as values in memory (estimated size 238.8 KB, free 365.8 MB)
 INFO [2020-02-19 08:10:37,825] ({pool-2-thread-3} Logging.scala[logInfo]:54) - Block broadcast_2_piece0 stored as bytes in memory (estimated size 23.0 KB, free 365.8 MB)
 INFO [2020-02-19 08:10:37,832] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Added broadcast_2_piece0 in memory on zeppelin:44103 (size: 23.0 KB, free: 366.3 MB)
 INFO [2020-02-19 08:10:37,834] ({pool-2-thread-3} Logging.scala[logInfo]:54) - Created broadcast 2 from textFile at <console>:35
 INFO [2020-02-19 08:10:38,110] ({pool-2-thread-3} FileInputFormat.java[listStatus]:249) - Total input paths to process : 1
 INFO [2020-02-19 08:10:38,125] ({pool-2-thread-3} Logging.scala[logInfo]:54) - Starting job: first at <console>:36
 INFO [2020-02-19 08:10:38,127] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Got job 1 (first at <console>:36) with 1 output partitions
 INFO [2020-02-19 08:10:38,128] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Final stage: ResultStage 1 (first at <console>:36)
 INFO [2020-02-19 08:10:38,129] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Parents of final stage: List()
 INFO [2020-02-19 08:10:38,130] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Missing parents: List()
 INFO [2020-02-19 08:10:38,131] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting ResultStage 1 (file:///zeppelin/data/test.txt MapPartitionsRDD[3] at textFile at <console>:35), which has no missing parents
 INFO [2020-02-19 08:10:38,134] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_3 stored as values in memory (estimated size 3.5 KB, free 365.8 MB)
 INFO [2020-02-19 08:10:38,138] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.0 KB, free 365.8 MB)
 INFO [2020-02-19 08:10:38,140] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Added broadcast_3_piece0 in memory on zeppelin:44103 (size: 2.0 KB, free: 366.3 MB)
 INFO [2020-02-19 08:10:38,141] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Created broadcast 3 from broadcast at DAGScheduler.scala:1161
 INFO [2020-02-19 08:10:38,143] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting 1 missing tasks from ResultStage 1 (file:///zeppelin/data/test.txt MapPartitionsRDD[3] at textFile at <console>:35) (first 15 tasks are for partitions Vector(0))
 INFO [2020-02-19 08:10:38,144] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Adding task set 1.0 with 1 tasks
 INFO [2020-02-19 08:10:38,145] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Added task set TaskSet_1.0 tasks to pool default
 INFO [2020-02-19 08:10:38,146] ({dispatcher-event-loop-2} Logging.scala[logInfo]:54) - Starting task 0.0 in stage 1.0 (TID 4, 172.30.0.7, executor 1, partition 0, PROCESS_LOCAL, 7892 bytes)
 INFO [2020-02-19 08:10:38,218] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Added broadcast_3_piece0 in memory on 172.30.0.7:37775 (size: 2.0 KB, free: 912.3 MB)
 INFO [2020-02-19 08:10:38,239] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Added broadcast_2_piece0 in memory on 172.30.0.7:37775 (size: 23.0 KB, free: 912.3 MB)
 WARN [2020-02-19 08:10:38,284] ({task-result-getter-0} Logging.scala[logWarning]:66) - Lost task 0.0 in stage 1.0 (TID 4, 172.30.0.7, executor 1): java.io.FileNotFoundException: File file:/zeppelin/data/test.txt does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:611)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:824)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:601)
	at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:421)
	at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:142)
	at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:346)
	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:769)
	at org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:109)
	at org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.liftedTree1$1(HadoopRDD.scala:267)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:266)
	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:224)
	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:95)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

 INFO [2020-02-19 08:10:38,285] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Starting task 0.1 in stage 1.0 (TID 5, 172.30.0.7, executor 1, partition 0, PROCESS_LOCAL, 7892 bytes)
 INFO [2020-02-19 08:10:38,518] ({task-result-getter-1} Logging.scala[logInfo]:54) - Lost task 0.1 in stage 1.0 (TID 5) on 172.30.0.7, executor 1: java.io.FileNotFoundException (File file:/zeppelin/data/test.txt does not exist) [duplicate 1]
 INFO [2020-02-19 08:10:38,523] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Starting task 0.2 in stage 1.0 (TID 6, 172.30.0.7, executor 1, partition 0, PROCESS_LOCAL, 7892 bytes)
 INFO [2020-02-19 08:10:38,555] ({task-result-getter-2} Logging.scala[logInfo]:54) - Lost task 0.2 in stage 1.0 (TID 6) on 172.30.0.7, executor 1: java.io.FileNotFoundException (File file:/zeppelin/data/test.txt does not exist) [duplicate 2]
 INFO [2020-02-19 08:10:38,558] ({dispatcher-event-loop-3} Logging.scala[logInfo]:54) - Starting task 0.3 in stage 1.0 (TID 7, 172.30.0.7, executor 1, partition 0, PROCESS_LOCAL, 7892 bytes)
 INFO [2020-02-19 08:10:38,579] ({task-result-getter-3} Logging.scala[logInfo]:54) - Lost task 0.3 in stage 1.0 (TID 7) on 172.30.0.7, executor 1: java.io.FileNotFoundException (File file:/zeppelin/data/test.txt does not exist) [duplicate 3]
ERROR [2020-02-19 08:10:38,580] ({task-result-getter-3} Logging.scala[logError]:70) - Task 0 in stage 1.0 failed 4 times; aborting job
 INFO [2020-02-19 08:10:38,582] ({task-result-getter-3} Logging.scala[logInfo]:54) - Removed TaskSet 1.0, whose tasks have all completed, from pool default
 INFO [2020-02-19 08:10:38,583] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Cancelling stage 1
 INFO [2020-02-19 08:10:38,586] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Killing all running tasks in stage 1: Stage cancelled
 INFO [2020-02-19 08:10:38,587] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - ResultStage 1 (first at <console>:36) failed in 0.455 s due to Job aborted due to stage failure: Task 0 in stage 1.0 failed 4 times, most recent failure: Lost task 0.3 in stage 1.0 (TID 7, 172.30.0.7, executor 1): java.io.FileNotFoundException: File file:/zeppelin/data/test.txt does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:611)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:824)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:601)
	at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:421)
	at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:142)
	at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:346)
	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:769)
	at org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:109)
	at org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.liftedTree1$1(HadoopRDD.scala:267)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:266)
	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:224)
	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:95)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
 INFO [2020-02-19 08:10:38,589] ({pool-2-thread-3} Logging.scala[logInfo]:54) - Job 1 failed: first at <console>:36, took 0.462366 s
 INFO [2020-02-19 08:10:38,840] ({pool-2-thread-3} SchedulerFactory.java[jobFinished]:120) - Job 20200219-065917_1853343063 finished by scheduler interpreter_1821642557
 INFO [2020-02-19 08:39:01,646] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 43
 INFO [2020-02-19 08:39:01,649] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 41
 INFO [2020-02-19 08:39:01,650] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 34
 INFO [2020-02-19 08:39:01,650] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 42
 INFO [2020-02-19 08:39:01,651] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 47
 INFO [2020-02-19 08:39:01,662] ({dispatcher-event-loop-3} Logging.scala[logInfo]:54) - Removed broadcast_3_piece0 on zeppelin:44103 in memory (size: 2.0 KB, free: 366.3 MB)
 INFO [2020-02-19 08:39:01,665] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Removed broadcast_3_piece0 on 172.30.0.7:37775 in memory (size: 2.0 KB, free: 912.3 MB)
 INFO [2020-02-19 08:39:01,675] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 44
 INFO [2020-02-19 08:39:01,676] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 46
 INFO [2020-02-19 08:39:01,677] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 33
 INFO [2020-02-19 08:39:01,678] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 35
 INFO [2020-02-19 08:39:01,678] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 26
 INFO [2020-02-19 08:39:01,679] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 40
 INFO [2020-02-19 08:39:01,680] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 39
 INFO [2020-02-19 08:39:01,680] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 31
 INFO [2020-02-19 08:39:01,681] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 45
 INFO [2020-02-19 08:39:01,682] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 38
 INFO [2020-02-19 08:39:01,682] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 29
 INFO [2020-02-19 08:39:01,683] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 30
 INFO [2020-02-19 08:39:01,684] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 32
 INFO [2020-02-19 08:39:01,684] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 49
 INFO [2020-02-19 08:39:01,685] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 36
 INFO [2020-02-19 08:39:01,686] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 28
 INFO [2020-02-19 08:39:01,687] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 25
 INFO [2020-02-19 08:39:01,687] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 37
 INFO [2020-02-19 08:39:01,688] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 27
 INFO [2020-02-19 08:39:01,689] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 48
 INFO [2020-02-19 16:17:30,005] ({pool-2-thread-2} SchedulerFactory.java[jobStarted]:114) - Job 20200219-070050_1333640979 started by scheduler interpreter_1821642557
 INFO [2020-02-19 16:17:30,013] ({pool-2-thread-2} OldSparkInterpreter.java[populateSparkWebUrl]:931) - Sending metadata to Zeppelin server: {message=Spark UI enabled, url=http://zeppelin:4040}
 INFO [2020-02-19 16:17:31,386] ({pool-2-thread-2} SchedulerFactory.java[jobFinished]:120) - Job 20200219-070050_1333640979 finished by scheduler interpreter_1821642557
 INFO [2020-02-19 16:17:51,610] ({pool-2-thread-3} SchedulerFactory.java[jobStarted]:114) - Job 20200219-070050_1333640979 started by scheduler interpreter_1821642557
 INFO [2020-02-19 16:17:51,623] ({pool-2-thread-3} OldSparkInterpreter.java[populateSparkWebUrl]:931) - Sending metadata to Zeppelin server: {message=Spark UI enabled, url=http://zeppelin:4040}
 INFO [2020-02-19 16:17:52,798] ({pool-2-thread-3} SchedulerFactory.java[jobFinished]:120) - Job 20200219-070050_1333640979 finished by scheduler interpreter_1821642557
ERROR [2020-02-19 17:03:50,360] ({dispatcher-event-loop-2} Logging.scala[logError]:70) - Lost executor 0 on 172.30.0.6: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
ERROR [2020-02-19 17:03:50,393] ({dispatcher-event-loop-2} Logging.scala[logError]:70) - Lost executor 1 on 172.30.0.7: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
 INFO [2020-02-19 17:03:50,410] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Executor lost: 0 (epoch 0)
 INFO [2020-02-19 17:03:50,418] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Trying to remove executor 0 from BlockManagerMaster.
 INFO [2020-02-19 17:03:50,422] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Removing block manager BlockManagerId(0, 172.30.0.6, 41599, None)
 INFO [2020-02-19 17:03:50,425] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Removed 0 successfully in removeExecutor
 INFO [2020-02-19 17:03:50,428] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Shuffle files lost for executor: 0 (epoch 0)
 INFO [2020-02-19 17:03:50,433] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Executor lost: 1 (epoch 1)
 INFO [2020-02-19 17:03:50,436] ({dispatcher-event-loop-2} Logging.scala[logInfo]:54) - Trying to remove executor 1 from BlockManagerMaster.
 INFO [2020-02-19 17:03:50,437] ({dispatcher-event-loop-2} Logging.scala[logInfo]:54) - Removing block manager BlockManagerId(1, 172.30.0.7, 37775, None)
 INFO [2020-02-19 17:03:50,438] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Removed 1 successfully in removeExecutor
 INFO [2020-02-19 17:03:50,439] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Shuffle files lost for executor: 1 (epoch 1)
 INFO [2020-02-19 17:03:50,870] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Executor updated: app-20200219080900-0000/0 is now LOST (worker lost)
 INFO [2020-02-19 17:03:50,901] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Executor app-20200219080900-0000/0 removed: worker lost
 INFO [2020-02-19 17:03:50,914] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Trying to remove executor 0 from BlockManagerMaster.
 INFO [2020-02-19 17:03:50,919] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Removal of executor 0 requested
 INFO [2020-02-19 17:03:50,928] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Asked to remove non-existent executor 0
 INFO [2020-02-19 17:03:50,969] ({dispatcher-event-loop-3} Logging.scala[logInfo]:54) - Master removed worker worker-20200219080753-172.30.0.6-34229: 172.30.0.6:34229 got disassociated
 INFO [2020-02-19 17:03:50,993] ({dispatcher-event-loop-3} Logging.scala[logInfo]:54) - Worker worker-20200219080753-172.30.0.6-34229 removed: 172.30.0.6:34229 got disassociated
 INFO [2020-02-19 17:03:51,001] ({dispatcher-event-loop-3} Logging.scala[logInfo]:54) - Executor updated: app-20200219080900-0000/1 is now LOST (worker lost)
 INFO [2020-02-19 17:03:51,002] ({dispatcher-event-loop-3} Logging.scala[logInfo]:54) - Executor app-20200219080900-0000/1 removed: worker lost
 INFO [2020-02-19 17:03:51,004] ({dispatcher-event-loop-3} Logging.scala[logInfo]:54) - Master removed worker worker-20200219080753-172.30.0.7-37349: 172.30.0.7:37349 got disassociated
 INFO [2020-02-19 17:03:51,005] ({dispatcher-event-loop-3} Logging.scala[logInfo]:54) - Worker worker-20200219080753-172.30.0.7-37349 removed: 172.30.0.7:37349 got disassociated
 INFO [2020-02-19 17:03:51,016] ({dispatcher-event-loop-2} Logging.scala[logInfo]:54) - Handle removed worker worker-20200219080753-172.30.0.6-34229: 172.30.0.6:34229 got disassociated
 INFO [2020-02-19 17:03:51,018] ({dispatcher-event-loop-2} Logging.scala[logInfo]:54) - Removal of executor 1 requested
 INFO [2020-02-19 17:03:51,018] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Trying to remove executor 1 from BlockManagerMaster.
 INFO [2020-02-19 17:03:51,019] ({dispatcher-event-loop-2} Logging.scala[logInfo]:54) - Asked to remove non-existent executor 1
 INFO [2020-02-19 17:03:51,020] ({dispatcher-event-loop-2} Logging.scala[logInfo]:54) - Handle removed worker worker-20200219080753-172.30.0.7-37349: 172.30.0.7:37349 got disassociated
 INFO [2020-02-19 17:03:51,022] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Shuffle files lost for worker worker-20200219080753-172.30.0.6-34229 on host 172.30.0.6
 INFO [2020-02-19 17:03:51,027] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Shuffle files lost for worker worker-20200219080753-172.30.0.7-37349 on host 172.30.0.7
 WARN [2020-02-19 17:04:01,664] ({dispatcher-event-loop-2} Logging.scala[logWarning]:66) - Connection to spark-master:7077 failed; waiting for master to reconnect...
 WARN [2020-02-19 17:04:01,666] ({dispatcher-event-loop-2} Logging.scala[logWarning]:66) - Disconnected from Spark cluster! Waiting for reconnection...
 WARN [2020-02-19 17:04:01,673] ({dispatcher-event-loop-2} Logging.scala[logWarning]:66) - Connection to spark-master:7077 failed; waiting for master to reconnect...
 INFO [2020-02-19 17:04:04,463] ({pool-1-thread-2} OldSparkInterpreter.java[close]:1243) - Close interpreter
 INFO [2020-02-19 17:04:04,481] ({pool-1-thread-2} AbstractConnector.java[doStop]:318) - Stopped Spark@5d21bf57{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
 INFO [2020-02-19 17:04:04,487] ({pool-1-thread-2} Logging.scala[logInfo]:54) - Stopped Spark web UI at http://zeppelin:4040
 INFO [2020-02-19 17:04:04,508] ({pool-1-thread-2} Logging.scala[logInfo]:54) - Shutting down all executors
 INFO [2020-02-19 17:04:04,510] ({dispatcher-event-loop-2} Logging.scala[logInfo]:54) - Asking each executor to shut down
 INFO [2020-02-19 17:04:04,542] ({dispatcher-event-loop-3} Logging.scala[logInfo]:54) - MapOutputTrackerMasterEndpoint stopped!
 INFO [2020-02-19 17:04:04,604] ({pool-1-thread-2} Logging.scala[logInfo]:54) - MemoryStore cleared
 INFO [2020-02-19 17:04:04,606] ({pool-1-thread-2} Logging.scala[logInfo]:54) - BlockManager stopped
 INFO [2020-02-19 17:04:04,608] ({pool-1-thread-2} Logging.scala[logInfo]:54) - BlockManagerMaster stopped
 INFO [2020-02-19 17:04:04,618] ({dispatcher-event-loop-2} Logging.scala[logInfo]:54) - OutputCommitCoordinator stopped!
 INFO [2020-02-19 17:04:04,641] ({pool-1-thread-2} Logging.scala[logInfo]:54) - Successfully stopped SparkContext
 INFO [2020-02-19 17:04:04,692] ({pool-1-thread-2} RemoteInterpreterServer.java[shutdown]:209) - Shutting down...
 INFO [2020-02-19 17:04:06,877] ({Thread-1} Logging.scala[logInfo]:54) - Shutdown hook called
 INFO [2020-02-19 17:04:06,886] ({Thread-1} Logging.scala[logInfo]:54) - Deleting directory /tmp/spark-fd3fdfb7-22b6-4bdd-aacf-597c29b0a296
 INFO [2020-02-19 17:04:06,892] ({Thread-1} Logging.scala[logInfo]:54) - Deleting directory /tmp/spark-98ff7fac-6e98-4a0b-ad84-7b2df7b688ae
 INFO [2020-02-19 17:04:06,901] ({Thread-1} Logging.scala[logInfo]:54) - Deleting directory /tmp/spark-6afcdb5c-c50a-4858-bbbe-05d7322c83a0
